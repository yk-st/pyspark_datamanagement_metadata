21/11/23 11:27:23 @WARN @Utils@ Your hostname, yukisaitos-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.30 instead (on interface en0)
21/11/23 11:27:23 @WARN @Utils@ Set SPARK_LOCAL_IP if you need to bind to another address
21/11/23 11:27:24 @INFO @HiveConf@ Found configuration file file:/opt/homebrew/Cellar/apache-spark/3.2.0/libexec/conf/hive-site.xml
21/11/23 11:27:24 @INFO @SparkContext@ Running Spark version 3.2.0
21/11/23 11:27:24 @WARN @NativeCodeLoader@ Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/11/23 11:27:24 @INFO @ResourceUtils@ ==============================================================
21/11/23 11:27:24 @INFO @ResourceUtils@ No custom resources configured for spark.driver.
21/11/23 11:27:24 @INFO @ResourceUtils@ ==============================================================
21/11/23 11:27:24 @INFO @SparkContext@ Submitted application: PySparkShell
21/11/23 11:27:24 @INFO @ResourceProfile@ Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/11/23 11:27:24 @INFO @ResourceProfile@ Limiting resource is cpu
21/11/23 11:27:24 @INFO @ResourceProfileManager@ Added ResourceProfile id: 0
21/11/23 11:27:24 @INFO @SecurityManager@ Changing view acls to: saitouyuuki
21/11/23 11:27:24 @INFO @SecurityManager@ Changing modify acls to: saitouyuuki
21/11/23 11:27:24 @INFO @SecurityManager@ Changing view acls groups to: 
21/11/23 11:27:24 @INFO @SecurityManager@ Changing modify acls groups to: 
21/11/23 11:27:24 @INFO @SecurityManager@ SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(saitouyuuki); groups with view permissions: Set(); users  with modify permissions: Set(saitouyuuki); groups with modify permissions: Set()
21/11/23 11:27:24 @INFO @Utils@ Successfully started service 'sparkDriver' on port 64401.
21/11/23 11:27:24 @INFO @SparkEnv@ Registering MapOutputTracker
21/11/23 11:27:24 @INFO @SparkEnv@ Registering BlockManagerMaster
21/11/23 11:27:24 @INFO @BlockManagerMasterEndpoint@ Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/11/23 11:27:24 @INFO @BlockManagerMasterEndpoint@ BlockManagerMasterEndpoint up
21/11/23 11:27:24 @INFO @SparkEnv@ Registering BlockManagerMasterHeartbeat
21/11/23 11:27:24 @INFO @DiskBlockManager@ Created local directory at /private/var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/blockmgr-2847fab5-57a9-402a-83e1-5b52e771ab49
21/11/23 11:27:24 @INFO @MemoryStore@ MemoryStore started with capacity 434.4 MiB
21/11/23 11:27:24 @INFO @SparkEnv@ Registering OutputCommitCoordinator
21/11/23 11:27:24 @WARN @Utils@ Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/11/23 11:27:24 @INFO @Utils@ Successfully started service 'SparkUI' on port 4041.
21/11/23 11:27:24 @INFO @SparkUI@ Bound SparkUI to 0.0.0.0, and started at http://192.168.0.30:4041
21/11/23 11:27:25 @INFO @Executor@ Starting executor ID driver on host 192.168.0.30
21/11/23 11:27:25 @INFO @Utils@ Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64402.
21/11/23 11:27:25 @INFO @NettyBlockTransferService@ Server created on 192.168.0.30:64402
21/11/23 11:27:25 @INFO @BlockManager@ Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/11/23 11:27:25 @INFO @BlockManagerMaster@ Registering BlockManager BlockManagerId(driver, 192.168.0.30, 64402, None)
21/11/23 11:27:25 @INFO @BlockManagerMasterEndpoint@ Registering block manager 192.168.0.30:64402 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.30, 64402, None)
21/11/23 11:27:25 @INFO @BlockManagerMaster@ Registered BlockManager BlockManagerId(driver, 192.168.0.30, 64402, None)
21/11/23 11:27:25 @INFO @BlockManager@ Initialized BlockManager: BlockManagerId(driver, 192.168.0.30, 64402, None)
21/11/23 11:27:25 @INFO @SharedState@ Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
21/11/23 11:27:25 @INFO @SharedState@ Warehouse path is 'file:/Users/saitouyuuki/Desktop/src/crypto_placer/spark-warehouse'.
21/11/23 11:27:44 @WARN @Utils@ Your hostname, yukisaitos-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.30 instead (on interface en0)
21/11/23 11:27:44 @WARN @Utils@ Set SPARK_LOCAL_IP if you need to bind to another address
21/11/23 11:27:44 @INFO @SparkContext@ Running Spark version 3.2.0
21/11/23 11:27:44 @WARN @NativeCodeLoader@ Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/11/23 11:27:44 @INFO @ResourceUtils@ ==============================================================
21/11/23 11:27:44 @INFO @ResourceUtils@ No custom resources configured for spark.driver.
21/11/23 11:27:44 @INFO @ResourceUtils@ ==============================================================
21/11/23 11:27:44 @INFO @SparkContext@ Submitted application: chapter1
21/11/23 11:27:44 @INFO @ResourceProfile@ Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/11/23 11:27:44 @INFO @ResourceProfile@ Limiting resource is cpu
21/11/23 11:27:44 @INFO @ResourceProfileManager@ Added ResourceProfile id: 0
21/11/23 11:27:44 @INFO @SecurityManager@ Changing view acls to: saitouyuuki
21/11/23 11:27:44 @INFO @SecurityManager@ Changing modify acls to: saitouyuuki
21/11/23 11:27:44 @INFO @SecurityManager@ Changing view acls groups to: 
21/11/23 11:27:44 @INFO @SecurityManager@ Changing modify acls groups to: 
21/11/23 11:27:44 @INFO @SecurityManager@ SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(saitouyuuki); groups with view permissions: Set(); users  with modify permissions: Set(saitouyuuki); groups with modify permissions: Set()
21/11/23 11:27:44 @INFO @Utils@ Successfully started service 'sparkDriver' on port 64454.
21/11/23 11:27:44 @INFO @SparkEnv@ Registering MapOutputTracker
21/11/23 11:27:44 @INFO @SparkEnv@ Registering BlockManagerMaster
21/11/23 11:27:44 @INFO @BlockManagerMasterEndpoint@ Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/11/23 11:27:44 @INFO @BlockManagerMasterEndpoint@ BlockManagerMasterEndpoint up
21/11/23 11:27:44 @INFO @SparkEnv@ Registering BlockManagerMasterHeartbeat
21/11/23 11:27:44 @INFO @DiskBlockManager@ Created local directory at /private/var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/blockmgr-fc9832c6-dbf9-4da8-b478-ff441bbb8b38
21/11/23 11:27:44 @INFO @MemoryStore@ MemoryStore started with capacity 434.4 MiB
21/11/23 11:27:44 @INFO @SparkEnv@ Registering OutputCommitCoordinator
21/11/23 11:27:44 @WARN @Utils@ Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/11/23 11:27:44 @WARN @Utils@ Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
21/11/23 11:27:44 @WARN @Utils@ Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
21/11/23 11:27:44 @INFO @Utils@ Successfully started service 'SparkUI' on port 4043.
21/11/23 11:27:44 @INFO @SparkUI@ Bound SparkUI to 0.0.0.0, and started at http://192.168.0.30:4043
21/11/23 11:27:45 @INFO @Executor@ Starting executor ID driver on host 192.168.0.30
21/11/23 11:27:45 @INFO @Utils@ Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64455.
21/11/23 11:27:45 @INFO @NettyBlockTransferService@ Server created on 192.168.0.30:64455
21/11/23 11:27:45 @INFO @BlockManager@ Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/11/23 11:27:45 @INFO @BlockManagerMaster@ Registering BlockManager BlockManagerId(driver, 192.168.0.30, 64455, None)
21/11/23 11:27:45 @INFO @BlockManagerMasterEndpoint@ Registering block manager 192.168.0.30:64455 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.30, 64455, None)
21/11/23 11:27:45 @INFO @BlockManagerMaster@ Registered BlockManager BlockManagerId(driver, 192.168.0.30, 64455, None)
21/11/23 11:27:45 @INFO @BlockManager@ Initialized BlockManager: BlockManagerId(driver, 192.168.0.30, 64455, None)
21/11/23 11:27:45 @INFO @SingleEventLogFileWriter@ Logging events to file:/private/tmp/spark-events/local-1637634465009.inprogress
21/11/23 11:27:45 @INFO @SharedState@ Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
21/11/23 11:27:45 @INFO @SharedState@ Warehouse path is 'file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse'.
21/11/23 11:27:47 @INFO @InMemoryFileIndex@ It took 21 ms to list leaf files for 1 paths.
21/11/23 11:27:48 @INFO @FileSourceStrategy@ Pushed Filters: IsNotNull(code),IsNotNull(gengo),Not(EqualTo(code,都道府県コード)),EqualTo(gengo,平成)
21/11/23 11:27:48 @INFO @FileSourceStrategy@ Post-Scan Filters: isnotnull(code#0),isnotnull(gengo#2),NOT (code#0 = 都道府県コード),(gengo#2 = 平成)
21/11/23 11:27:48 @INFO @FileSourceStrategy@ Output Data Schema: struct<code: string, kenmei: string, gengo: string, wareki: string, seireki: string ... 7 more fields>
21/11/23 11:27:48 @INFO @CodeGenerator@ Code generated in 99.366917 ms
21/11/23 11:27:48 @INFO @MemoryStore@ Block broadcast_0 stored as values in memory (estimated size 192.5 KiB, free 434.2 MiB)
21/11/23 11:27:48 @INFO @MemoryStore@ Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 434.2 MiB)
21/11/23 11:27:48 @INFO @BlockManagerInfo@ Added broadcast_0_piece0 in memory on 192.168.0.30:64455 (size: 33.6 KiB, free: 434.4 MiB)
21/11/23 11:27:48 @INFO @SparkContext@ Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0
21/11/23 11:27:48 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:27:48 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:27:48 @INFO @DAGScheduler@ Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:27:48 @INFO @DAGScheduler@ Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:27:48 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:27:48 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:27:48 @INFO @DAGScheduler@ Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:27:48 @INFO @MemoryStore@ Block broadcast_1 stored as values in memory (estimated size 16.9 KiB, free 434.2 MiB)
21/11/23 11:27:48 @INFO @MemoryStore@ Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.2 MiB)
21/11/23 11:27:48 @INFO @BlockManagerInfo@ Added broadcast_1_piece0 in memory on 192.168.0.30:64455 (size: 7.8 KiB, free: 434.4 MiB)
21/11/23 11:27:48 @INFO @SparkContext@ Created broadcast 1 from broadcast at DAGScheduler.scala:1427
21/11/23 11:27:48 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:27:48 @INFO @TaskSchedulerImpl@ Adding task set 0.0 with 1 tasks resource profile 0
21/11/23 11:27:48 @INFO @TaskSetManager@ Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4914 bytes) taskResourceAssignments Map()
21/11/23 11:27:48 @INFO @Executor@ Running task 0.0 in stage 0.0 (TID 0)
21/11/23 11:27:49 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/jinko.csv, range: 0-55038, partition values: [empty row]
21/11/23 11:27:49 @INFO @CodeGenerator@ Code generated in 10.49975 ms
21/11/23 11:27:49 @INFO @CodeGenerator@ Code generated in 6.735708 ms
21/11/23 11:27:49 @INFO @CodeGenerator@ Code generated in 3.69975 ms
21/11/23 11:27:49 @INFO @Executor@ Finished task 0.0 in stage 0.0 (TID 0). 2771 bytes result sent to driver
21/11/23 11:27:49 @INFO @TaskSetManager@ Finished task 0.0 in stage 0.0 (TID 0) in 199 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:27:49 @INFO @TaskSchedulerImpl@ Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/11/23 11:27:49 @INFO @DAGScheduler@ ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 0.271 s
21/11/23 11:27:49 @INFO @DAGScheduler@ Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:27:49 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 0: Stage finished
21/11/23 11:27:49 @INFO @DAGScheduler@ Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 0.295567 s
21/11/23 11:27:49 @INFO @CodeGenerator@ Code generated in 9.932042 ms
21/11/23 11:27:51 @INFO @HiveConf@ Found configuration file file:/opt/homebrew/Cellar/apache-spark/3.2.0/libexec/conf/hive-site.xml
21/11/23 11:27:51 @INFO @HiveUtils@ Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
21/11/23 11:27:51 @INFO @HiveClientImpl@ Warehouse location for Hive client (version 2.3.9) is file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse
21/11/23 11:27:51 @WARN @HiveConf@ HiveConf of name hive.stats.jdbc.timeout does not exist
21/11/23 11:27:51 @WARN @HiveConf@ HiveConf of name hive.stats.retries.wait does not exist
21/11/23 11:27:51 @INFO @HiveMetaStore@ 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/11/23 11:27:51 @INFO @ObjectStore@ ObjectStore, initialize called
21/11/23 11:27:52 @INFO @Persistence@ Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/11/23 11:27:52 @INFO @Persistence@ Property datanucleus.cache.level2 unknown - will be ignored
21/11/23 11:27:52 @INFO @ObjectStore@ Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/11/23 11:27:52 @INFO @BlockManagerInfo@ Removed broadcast_0_piece0 on 192.168.0.30:64455 in memory (size: 33.6 KiB, free: 434.4 MiB)
21/11/23 11:27:52 @INFO @BlockManagerInfo@ Removed broadcast_1_piece0 on 192.168.0.30:64455 in memory (size: 7.8 KiB, free: 434.4 MiB)
21/11/23 11:27:52 @INFO @MetaStoreDirectSql@ Using direct SQL, underlying DB is MYSQL
21/11/23 11:27:52 @INFO @ObjectStore@ Initialized ObjectStore
21/11/23 11:27:52 @INFO @HiveMetaStore@ Added admin role in metastore
21/11/23 11:27:52 @INFO @HiveMetaStore@ Added public role in metastore
21/11/23 11:27:52 @INFO @HiveMetaStore@ No user is added in admin role, since config is empty
21/11/23 11:27:53 @INFO @HiveMetaStore@ 0: get_database: default
21/11/23 11:27:53 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: default	
21/11/23 11:27:53 @INFO @HiveMetaStore@ 0: get_database: global_temp
21/11/23 11:27:53 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: global_temp	
21/11/23 11:27:53 @WARN @ObjectStore@ Failed to get database global_temp, returning NoSuchObjectException
21/11/23 11:27:53 @INFO @HiveMetaStore@ 0: create_database: Database(name:data_management_crush_course, description:, locationUri:file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse/data_management_crush_course.db, parameters:{}, ownerName:saitouyuuki)
21/11/23 11:27:53 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=create_database: Database(name:data_management_crush_course, description:, locationUri:file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse/data_management_crush_course.db, parameters:{}, ownerName:saitouyuuki)	
21/11/23 11:27:55 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:27:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:27:55 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:27:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:27:55 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:27:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:27:55 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:27:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:27:55 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:27:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:27:55 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:27:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:27:55 @INFO @SQLStdHiveAccessController@ Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=325ed9be-df07-4e1e-ab6a-d3a368d66523, clientType=HIVECLI]
21/11/23 11:27:55 @WARN @SessionState@ METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
21/11/23 11:27:55 @INFO @metastore@ Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
21/11/23 11:27:55 @INFO @HiveMetaStore@ 0: Cleaning up thread local RawStore...
21/11/23 11:27:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
21/11/23 11:27:55 @INFO @HiveMetaStore@ 0: Done cleaning up thread local RawStore
21/11/23 11:27:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
21/11/23 11:27:55 @INFO @HiveMetaStore@ 0: create_table: Table(tableName:jinko_table, dbName:data_management_crush_course, owner:saitouyuuki, createTime:1637634475, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:code, type:string, comment:null), FieldSchema(name:gengo, type:string, comment:null), FieldSchema(name:wareki, type:string, comment:null), FieldSchema(name:seireki, type:string, comment:null), FieldSchema(name:chu, type:string, comment:null), FieldSchema(name:sokei, type:string, comment:null), FieldSchema(name:jinko_male, type:string, comment:null), FieldSchema(name:jinko_femail, type:string, comment:null)], location:file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:kenmei, type:string, comment:null)], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema.numPartCols=1, spark.sql.sources.schema.partCol.0=kenmei, spark.sql.sources.schema={"type":"struct","fields":[{"name":"code","type":"string","nullable":true,"metadata":{}},{"name":"gengo","type":"string","nullable":true,"metadata":{}},{"name":"wareki","type":"string","nullable":true,"metadata":{}},{"name":"seireki","type":"string","nullable":true,"metadata":{}},{"name":"chu","type":"string","nullable":true,"metadata":{}},{"name":"sokei","type":"string","nullable":true,"metadata":{}},{"name":"jinko_male","type":"string","nullable":true,"metadata":{}},{"name":"jinko_femail","type":"string","nullable":true,"metadata":{}},{"name":"kenmei","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{saitouyuuki=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
21/11/23 11:27:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=create_table: Table(tableName:jinko_table, dbName:data_management_crush_course, owner:saitouyuuki, createTime:1637634475, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:code, type:string, comment:null), FieldSchema(name:gengo, type:string, comment:null), FieldSchema(name:wareki, type:string, comment:null), FieldSchema(name:seireki, type:string, comment:null), FieldSchema(name:chu, type:string, comment:null), FieldSchema(name:sokei, type:string, comment:null), FieldSchema(name:jinko_male, type:string, comment:null), FieldSchema(name:jinko_femail, type:string, comment:null)], location:file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[FieldSchema(name:kenmei, type:string, comment:null)], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema.numPartCols=1, spark.sql.sources.schema.partCol.0=kenmei, spark.sql.sources.schema={"type":"struct","fields":[{"name":"code","type":"string","nullable":true,"metadata":{}},{"name":"gengo","type":"string","nullable":true,"metadata":{}},{"name":"wareki","type":"string","nullable":true,"metadata":{}},{"name":"seireki","type":"string","nullable":true,"metadata":{}},{"name":"chu","type":"string","nullable":true,"metadata":{}},{"name":"sokei","type":"string","nullable":true,"metadata":{}},{"name":"jinko_male","type":"string","nullable":true,"metadata":{}},{"name":"jinko_femail","type":"string","nullable":true,"metadata":{}},{"name":"kenmei","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{saitouyuuki=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))	
21/11/23 11:27:55 @WARN @HiveConf@ HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
21/11/23 11:27:55 @WARN @HiveConf@ HiveConf of name hive.stats.jdbc.timeout does not exist
21/11/23 11:27:55 @WARN @HiveConf@ HiveConf of name hive.stats.retries.wait does not exist
21/11/23 11:27:55 @INFO @HiveMetaStore@ 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/11/23 11:27:55 @INFO @ObjectStore@ ObjectStore, initialize called
21/11/23 11:27:55 @INFO @MetaStoreDirectSql@ Using direct SQL, underlying DB is MYSQL
21/11/23 11:27:55 @INFO @ObjectStore@ Initialized ObjectStore
21/11/23 11:27:58 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:27:58 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:27:58 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:27:58 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:27:58 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:27:58 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:27:58 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:27:58 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:27:58 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:27:58 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:27:58 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:27:58 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:27:58 @INFO @HiveMetaStore@ 0: create_table: Table(tableName:jinko_code, dbName:data_management_crush_course, owner:saitouyuuki, createTime:1637634478, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:code, type:string, comment:null), FieldSchema(name:kenmei, type:string, comment:null)], location:file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{line.delim=
, field.delim=,, serialization.format=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema={"type":"struct","fields":[{"name":"code","type":"string","nullable":true,"metadata":{}},{"name":"kenmei","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{saitouyuuki=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
21/11/23 11:27:58 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=create_table: Table(tableName:jinko_code, dbName:data_management_crush_course, owner:saitouyuuki, createTime:1637634478, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:code, type:string, comment:null), FieldSchema(name:kenmei, type:string, comment:null)], location:file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{line.delim=
, field.delim=,, serialization.format=,}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema={"type":"struct","fields":[{"name":"code","type":"string","nullable":true,"metadata":{}},{"name":"kenmei","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{saitouyuuki=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))	
21/11/23 11:28:00 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:00 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:00 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:28:00 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:28:00 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:28:00 @INFO @Persistence@ Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
21/11/23 11:28:00 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:00 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:00 @INFO @HiveMetaStore@ 0: get_tables: db=data_management_crush_course pat=*
21/11/23 11:28:00 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_tables: db=data_management_crush_course pat=*	
21/11/23 11:28:00 @INFO @CodeGenerator@ Code generated in 6.807958 ms
21/11/23 11:28:00 @INFO @CodeGenerator@ Code generated in 5.50975 ms
21/11/23 11:28:00 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:28:00 @INFO @DAGScheduler@ Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:28:00 @INFO @DAGScheduler@ Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:28:00 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:28:00 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:28:00 @INFO @DAGScheduler@ Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:28:00 @INFO @MemoryStore@ Block broadcast_2 stored as values in memory (estimated size 7.5 KiB, free 434.4 MiB)
21/11/23 11:28:00 @INFO @MemoryStore@ Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.4 MiB)
21/11/23 11:28:00 @INFO @BlockManagerInfo@ Added broadcast_2_piece0 in memory on 192.168.0.30:64455 (size: 3.8 KiB, free: 434.4 MiB)
21/11/23 11:28:00 @INFO @SparkContext@ Created broadcast 2 from broadcast at DAGScheduler.scala:1427
21/11/23 11:28:00 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:28:00 @INFO @TaskSchedulerImpl@ Adding task set 1.0 with 1 tasks resource profile 0
21/11/23 11:28:00 @INFO @TaskSetManager@ Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4689 bytes) taskResourceAssignments Map()
21/11/23 11:28:00 @INFO @Executor@ Running task 0.0 in stage 1.0 (TID 1)
21/11/23 11:28:00 @INFO @Executor@ Finished task 0.0 in stage 1.0 (TID 1). 1437 bytes result sent to driver
21/11/23 11:28:00 @INFO @TaskSetManager@ Finished task 0.0 in stage 1.0 (TID 1) in 18 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:28:00 @INFO @TaskSchedulerImpl@ Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/11/23 11:28:00 @INFO @DAGScheduler@ ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.025 s
21/11/23 11:28:00 @INFO @DAGScheduler@ Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:28:00 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 1: Stage finished
21/11/23 11:28:00 @INFO @DAGScheduler@ Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.028434 s
21/11/23 11:28:00 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:28:00 @INFO @DAGScheduler@ Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:28:00 @INFO @DAGScheduler@ Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:28:00 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:28:00 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:28:00 @INFO @DAGScheduler@ Submitting ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:28:00 @INFO @MemoryStore@ Block broadcast_3 stored as values in memory (estimated size 7.5 KiB, free 434.4 MiB)
21/11/23 11:28:00 @INFO @MemoryStore@ Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.4 MiB)
21/11/23 11:28:00 @INFO @BlockManagerInfo@ Added broadcast_3_piece0 in memory on 192.168.0.30:64455 (size: 3.8 KiB, free: 434.4 MiB)
21/11/23 11:28:00 @INFO @SparkContext@ Created broadcast 3 from broadcast at DAGScheduler.scala:1427
21/11/23 11:28:00 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))
21/11/23 11:28:00 @INFO @TaskSchedulerImpl@ Adding task set 2.0 with 1 tasks resource profile 0
21/11/23 11:28:00 @INFO @TaskSetManager@ Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 4689 bytes) taskResourceAssignments Map()
21/11/23 11:28:00 @INFO @Executor@ Running task 0.0 in stage 2.0 (TID 2)
21/11/23 11:28:00 @INFO @Executor@ Finished task 0.0 in stage 2.0 (TID 2). 1394 bytes result sent to driver
21/11/23 11:28:00 @INFO @TaskSetManager@ Finished task 0.0 in stage 2.0 (TID 2) in 5 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:28:00 @INFO @TaskSchedulerImpl@ Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/11/23 11:28:00 @INFO @DAGScheduler@ ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.010 s
21/11/23 11:28:00 @INFO @DAGScheduler@ Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:28:00 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 2: Stage finished
21/11/23 11:28:00 @INFO @DAGScheduler@ Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.011150 s
21/11/23 11:28:00 @INFO @CodeGenerator@ Code generated in 7.464333 ms
21/11/23 11:28:04 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:04 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:04 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:04 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:04 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:04 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:04 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:04 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:04 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:04 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:04 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:04 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:04 @INFO @CodeGenerator@ Code generated in 4.256416 ms
21/11/23 11:28:04 @INFO @CodeGenerator@ Code generated in 3.506208 ms
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:07 @INFO @FileSourceStrategy@ Pushed Filters: IsNotNull(code),IsNotNull(gengo),Not(EqualTo(code,都道府県コード)),EqualTo(gengo,平成)
21/11/23 11:28:07 @INFO @FileSourceStrategy@ Post-Scan Filters: isnotnull(code#0),isnotnull(gengo#2),NOT (code#0 = 都道府県コード),(gengo#2 = 平成)
21/11/23 11:28:07 @INFO @FileSourceStrategy@ Output Data Schema: struct<code: string, kenmei: string, gengo: string, wareki: string, seireki: string ... 7 more fields>
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:07 @INFO @HiveMetaStore@ 0: get_partitions_ps_with_auth : db=data_management_crush_course tbl=jinko_table[]
21/11/23 11:28:07 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions_ps_with_auth : db=data_management_crush_course tbl=jinko_table[]	
21/11/23 11:28:07 @INFO @ParquetFileFormat@ Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:28:07 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:28:07 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:28:07 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:28:07 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:28:07 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:28:07 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:28:07 @INFO @CodeGenerator@ Code generated in 9.060375 ms
21/11/23 11:28:07 @INFO @MemoryStore@ Block broadcast_4 stored as values in memory (estimated size 192.5 KiB, free 434.2 MiB)
21/11/23 11:28:07 @INFO @MemoryStore@ Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 434.2 MiB)
21/11/23 11:28:07 @INFO @BlockManagerInfo@ Added broadcast_4_piece0 in memory on 192.168.0.30:64455 (size: 33.6 KiB, free: 434.4 MiB)
21/11/23 11:28:07 @INFO @SparkContext@ Created broadcast 4 from sql at NativeMethodAccessorImpl.java:0
21/11/23 11:28:07 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:28:07 @INFO @SparkContext@ Starting job: sql at NativeMethodAccessorImpl.java:0
21/11/23 11:28:07 @INFO @DAGScheduler@ Got job 3 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:28:07 @INFO @DAGScheduler@ Final stage: ResultStage 3 (sql at NativeMethodAccessorImpl.java:0)
21/11/23 11:28:07 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:28:07 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:28:07 @INFO @DAGScheduler@ Submitting ResultStage 3 (MapPartitionsRDD[11] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:28:07 @INFO @MemoryStore@ Block broadcast_5 stored as values in memory (estimated size 228.1 KiB, free 433.9 MiB)
21/11/23 11:28:07 @INFO @MemoryStore@ Block broadcast_5_piece0 stored as bytes in memory (estimated size 83.0 KiB, free 433.9 MiB)
21/11/23 11:28:07 @INFO @BlockManagerInfo@ Added broadcast_5_piece0 in memory on 192.168.0.30:64455 (size: 83.0 KiB, free: 434.3 MiB)
21/11/23 11:28:07 @INFO @SparkContext@ Created broadcast 5 from broadcast at DAGScheduler.scala:1427
21/11/23 11:28:07 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:28:07 @INFO @TaskSchedulerImpl@ Adding task set 3.0 with 1 tasks resource profile 0
21/11/23 11:28:07 @INFO @TaskSetManager@ Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4914 bytes) taskResourceAssignments Map()
21/11/23 11:28:07 @INFO @Executor@ Running task 0.0 in stage 3.0 (TID 3)
21/11/23 11:28:07 @INFO @CodeGenerator@ Code generated in 6.509875 ms
21/11/23 11:28:07 @INFO @CodeGenerator@ Code generated in 9.266292 ms
21/11/23 11:28:07 @INFO @CodeGenerator@ Code generated in 6.26325 ms
21/11/23 11:28:07 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/jinko.csv, range: 0-55038, partition values: [empty row]
21/11/23 11:28:07 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:28:07 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:28:07 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:28:07 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:28:07 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:28:07 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:28:07 @INFO @CodeGenerator@ Code generated in 5.005791 ms
21/11/23 11:28:07 @INFO @CodeGenerator@ Code generated in 4.172375 ms
21/11/23 11:28:07 @INFO @CodeGenerator@ Code generated in 12.677083 ms
21/11/23 11:28:07 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:07 @INFO @CodecPool@ Got brand-new compressor [.snappy]
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:08 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @BlockManagerInfo@ Removed broadcast_3_piece0 on 192.168.0.30:64455 in memory (size: 3.8 KiB, free: 434.3 MiB)
21/11/23 11:28:09 @INFO @BlockManagerInfo@ Removed broadcast_2_piece0 on 192.168.0.30:64455 in memory (size: 3.8 KiB, free: 434.3 MiB)
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "gengo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wareki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "seireki",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "chu",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sokei",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_male",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "jinko_femail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary code (STRING);
  optional binary gengo (STRING);
  optional binary wareki (STRING);
  optional binary seireki (STRING);
  optional binary chu (STRING);
  optional binary sokei (STRING);
  optional binary jinko_male (STRING);
  optional binary jinko_femail (STRING);
}

       
21/11/23 11:28:09 @INFO @FileOutputCommitter@ Saved output of task 'attempt_202111231128077773883032634277808_0003_m_000000_3' to file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/.spark-staging-0ebed64a-9505-4db5-af0b-d7d809c19859/_temporary/0/task_202111231128077773883032634277808_0003_m_000000
21/11/23 11:28:09 @INFO @SparkHadoopMapRedUtil@ attempt_202111231128077773883032634277808_0003_m_000000_3: Committed
21/11/23 11:28:09 @INFO @Executor@ Finished task 0.0 in stage 3.0 (TID 3). 7018 bytes result sent to driver
21/11/23 11:28:09 @INFO @TaskSetManager@ Finished task 0.0 in stage 3.0 (TID 3) in 1663 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:28:09 @INFO @TaskSchedulerImpl@ Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/11/23 11:28:09 @INFO @DAGScheduler@ ResultStage 3 (sql at NativeMethodAccessorImpl.java:0) finished in 1.742 s
21/11/23 11:28:09 @INFO @DAGScheduler@ Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:28:09 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 3: Stage finished
21/11/23 11:28:09 @INFO @DAGScheduler@ Job 3 finished: sql at NativeMethodAccessorImpl.java:0, took 1.744174 s
21/11/23 11:28:09 @INFO @FileFormatWriter@ Start to commit write Job dacce355-f84a-4c9d-a931-e58a76a08a2b.
21/11/23 11:28:09 @INFO @FileFormatWriter@ Write Job dacce355-f84a-4c9d-a931-e58a76a08a2b committed. Elapsed time: 64 ms.
21/11/23 11:28:09 @INFO @FileFormatWriter@ Finished processing stats for write job dacce355-f84a-4c9d-a931-e58a76a08a2b.
21/11/23 11:28:10 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:10 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:10 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:10 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:28:10 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:28:10 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:28:10 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:28:10 @INFO @CodeGenerator@ Code generated in 24.758959 ms
21/11/23 11:28:10 @INFO @MemoryStore@ Block broadcast_6 stored as values in memory (estimated size 193.4 KiB, free 433.7 MiB)
21/11/23 11:28:10 @INFO @MemoryStore@ Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.7 MiB)
21/11/23 11:28:10 @INFO @BlockManagerInfo@ Added broadcast_6_piece0 in memory on 192.168.0.30:64455 (size: 34.0 KiB, free: 434.3 MiB)
21/11/23 11:28:10 @INFO @SparkContext@ Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0
21/11/23 11:28:10 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:10 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:10 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:10 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:28:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:28:10 @INFO @HadoopFSUtils@ Listing leaf files and directories in parallel under 50 paths. The first several paths are: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県.
21/11/23 11:28:10 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:28:10 @INFO @DAGScheduler@ Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 50 output partitions
21/11/23 11:28:10 @INFO @DAGScheduler@ Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:28:10 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:28:10 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:28:10 @INFO @DAGScheduler@ Submitting ResultStage 4 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:28:10 @INFO @MemoryStore@ Block broadcast_7 stored as values in memory (estimated size 101.0 KiB, free 433.6 MiB)
21/11/23 11:28:10 @INFO @MemoryStore@ Block broadcast_7_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)
21/11/23 11:28:10 @INFO @BlockManagerInfo@ Added broadcast_7_piece0 in memory on 192.168.0.30:64455 (size: 36.2 KiB, free: 434.2 MiB)
21/11/23 11:28:10 @INFO @SparkContext@ Created broadcast 7 from broadcast at DAGScheduler.scala:1427
21/11/23 11:28:10 @INFO @DAGScheduler@ Submitting 50 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/11/23 11:28:10 @INFO @TaskSchedulerImpl@ Adding task set 4.0 with 50 tasks resource profile 0
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 0.0 in stage 4.0 (TID 4) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 1.0 in stage 4.0 (TID 5) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 2.0 in stage 4.0 (TID 6) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 4599 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 3.0 in stage 4.0 (TID 7) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 4614 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 4.0 in stage 4.0 (TID 8) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 5.0 in stage 4.0 (TID 9) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 4587 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 6.0 in stage 4.0 (TID 10) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 7.0 in stage 4.0 (TID 11) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 8.0 in stage 4.0 (TID 12) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 9.0 in stage 4.0 (TID 13) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 0.0 in stage 4.0 (TID 4)
21/11/23 11:28:10 @INFO @Executor@ Running task 1.0 in stage 4.0 (TID 5)
21/11/23 11:28:10 @INFO @Executor@ Running task 2.0 in stage 4.0 (TID 6)
21/11/23 11:28:10 @INFO @Executor@ Running task 4.0 in stage 4.0 (TID 8)
21/11/23 11:28:10 @INFO @Executor@ Running task 3.0 in stage 4.0 (TID 7)
21/11/23 11:28:10 @INFO @Executor@ Running task 6.0 in stage 4.0 (TID 10)
21/11/23 11:28:10 @INFO @Executor@ Running task 8.0 in stage 4.0 (TID 12)
21/11/23 11:28:10 @INFO @Executor@ Running task 7.0 in stage 4.0 (TID 11)
21/11/23 11:28:10 @INFO @Executor@ Running task 5.0 in stage 4.0 (TID 9)
21/11/23 11:28:10 @INFO @Executor@ Running task 9.0 in stage 4.0 (TID 13)
21/11/23 11:28:10 @INFO @Executor@ Finished task 8.0 in stage 4.0 (TID 12). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 7.0 in stage 4.0 (TID 11). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 10.0 in stage 4.0 (TID 14) (192.168.0.30, executor driver, partition 10, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Finished task 0.0 in stage 4.0 (TID 4). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 3.0 in stage 4.0 (TID 7). 2078 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 11.0 in stage 4.0 (TID 15) (192.168.0.30, executor driver, partition 11, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Finished task 4.0 in stage 4.0 (TID 8). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 2.0 in stage 4.0 (TID 6). 2045 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 5.0 in stage 4.0 (TID 9). 2021 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 12.0 in stage 4.0 (TID 16) (192.168.0.30, executor driver, partition 12, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 10.0 in stage 4.0 (TID 14)
21/11/23 11:28:10 @INFO @Executor@ Running task 11.0 in stage 4.0 (TID 15)
21/11/23 11:28:10 @INFO @Executor@ Finished task 9.0 in stage 4.0 (TID 13). 2033 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Running task 12.0 in stage 4.0 (TID 16)
21/11/23 11:28:10 @INFO @Executor@ Finished task 6.0 in stage 4.0 (TID 10). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 13.0 in stage 4.0 (TID 17) (192.168.0.30, executor driver, partition 13, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 8.0 in stage 4.0 (TID 12) in 42 ms on 192.168.0.30 (executor driver) (1/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 1.0 in stage 4.0 (TID 5). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Running task 13.0 in stage 4.0 (TID 17)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 0.0 in stage 4.0 (TID 4) in 43 ms on 192.168.0.30 (executor driver) (2/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 14.0 in stage 4.0 (TID 18) (192.168.0.30, executor driver, partition 14, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 15.0 in stage 4.0 (TID 19) (192.168.0.30, executor driver, partition 15, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 7.0 in stage 4.0 (TID 11) in 43 ms on 192.168.0.30 (executor driver) (3/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 14.0 in stage 4.0 (TID 18)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 2.0 in stage 4.0 (TID 6) in 44 ms on 192.168.0.30 (executor driver) (4/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 15.0 in stage 4.0 (TID 19)
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 16.0 in stage 4.0 (TID 20) (192.168.0.30, executor driver, partition 16, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 4.0 in stage 4.0 (TID 8) in 46 ms on 192.168.0.30 (executor driver) (5/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 3.0 in stage 4.0 (TID 7) in 46 ms on 192.168.0.30 (executor driver) (6/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 16.0 in stage 4.0 (TID 20)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 5.0 in stage 4.0 (TID 9) in 46 ms on 192.168.0.30 (executor driver) (7/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 17.0 in stage 4.0 (TID 21) (192.168.0.30, executor driver, partition 17, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 18.0 in stage 4.0 (TID 22) (192.168.0.30, executor driver, partition 18, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 19.0 in stage 4.0 (TID 23) (192.168.0.30, executor driver, partition 19, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 9.0 in stage 4.0 (TID 13) in 48 ms on 192.168.0.30 (executor driver) (8/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 6.0 in stage 4.0 (TID 10) in 48 ms on 192.168.0.30 (executor driver) (9/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 1.0 in stage 4.0 (TID 5) in 49 ms on 192.168.0.30 (executor driver) (10/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 17.0 in stage 4.0 (TID 21)
21/11/23 11:28:10 @INFO @Executor@ Running task 19.0 in stage 4.0 (TID 23)
21/11/23 11:28:10 @INFO @Executor@ Running task 18.0 in stage 4.0 (TID 22)
21/11/23 11:28:10 @INFO @Executor@ Finished task 11.0 in stage 4.0 (TID 15). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 12.0 in stage 4.0 (TID 16). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 20.0 in stage 4.0 (TID 24) (192.168.0.30, executor driver, partition 20, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 21.0 in stage 4.0 (TID 25) (192.168.0.30, executor driver, partition 21, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 20.0 in stage 4.0 (TID 24)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 11.0 in stage 4.0 (TID 15) in 20 ms on 192.168.0.30 (executor driver) (11/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 12.0 in stage 4.0 (TID 16) in 21 ms on 192.168.0.30 (executor driver) (12/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 21.0 in stage 4.0 (TID 25)
21/11/23 11:28:10 @INFO @Executor@ Finished task 10.0 in stage 4.0 (TID 14). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 22.0 in stage 4.0 (TID 26) (192.168.0.30, executor driver, partition 22, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 10.0 in stage 4.0 (TID 14) in 25 ms on 192.168.0.30 (executor driver) (13/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 22.0 in stage 4.0 (TID 26)
21/11/23 11:28:10 @INFO @Executor@ Finished task 13.0 in stage 4.0 (TID 17). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 15.0 in stage 4.0 (TID 19). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 23.0 in stage 4.0 (TID 27) (192.168.0.30, executor driver, partition 23, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 23.0 in stage 4.0 (TID 27)
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 24.0 in stage 4.0 (TID 28) (192.168.0.30, executor driver, partition 24, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 13.0 in stage 4.0 (TID 17) in 28 ms on 192.168.0.30 (executor driver) (14/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 24.0 in stage 4.0 (TID 28)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 15.0 in stage 4.0 (TID 19) in 28 ms on 192.168.0.30 (executor driver) (15/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 14.0 in stage 4.0 (TID 18). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 25.0 in stage 4.0 (TID 29) (192.168.0.30, executor driver, partition 25, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 14.0 in stage 4.0 (TID 18) in 30 ms on 192.168.0.30 (executor driver) (16/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 25.0 in stage 4.0 (TID 29)
21/11/23 11:28:10 @INFO @Executor@ Finished task 19.0 in stage 4.0 (TID 23). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 17.0 in stage 4.0 (TID 21). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 26.0 in stage 4.0 (TID 30) (192.168.0.30, executor driver, partition 26, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 27.0 in stage 4.0 (TID 31) (192.168.0.30, executor driver, partition 27, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 26.0 in stage 4.0 (TID 30)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 19.0 in stage 4.0 (TID 23) in 27 ms on 192.168.0.30 (executor driver) (17/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 27.0 in stage 4.0 (TID 31)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 17.0 in stage 4.0 (TID 21) in 29 ms on 192.168.0.30 (executor driver) (18/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 16.0 in stage 4.0 (TID 20). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 18.0 in stage 4.0 (TID 22). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 28.0 in stage 4.0 (TID 32) (192.168.0.30, executor driver, partition 28, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 29.0 in stage 4.0 (TID 33) (192.168.0.30, executor driver, partition 29, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 28.0 in stage 4.0 (TID 32)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 16.0 in stage 4.0 (TID 20) in 34 ms on 192.168.0.30 (executor driver) (19/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 29.0 in stage 4.0 (TID 33)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 18.0 in stage 4.0 (TID 22) in 33 ms on 192.168.0.30 (executor driver) (20/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 21.0 in stage 4.0 (TID 25). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 30.0 in stage 4.0 (TID 34) (192.168.0.30, executor driver, partition 30, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 21.0 in stage 4.0 (TID 25) in 25 ms on 192.168.0.30 (executor driver) (21/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 30.0 in stage 4.0 (TID 34)
21/11/23 11:28:10 @INFO @Executor@ Finished task 22.0 in stage 4.0 (TID 26). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 31.0 in stage 4.0 (TID 35) (192.168.0.30, executor driver, partition 31, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Finished task 23.0 in stage 4.0 (TID 27). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 20.0 in stage 4.0 (TID 24). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 32.0 in stage 4.0 (TID 36) (192.168.0.30, executor driver, partition 32, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 33.0 in stage 4.0 (TID 37) (192.168.0.30, executor driver, partition 33, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 31.0 in stage 4.0 (TID 35)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 22.0 in stage 4.0 (TID 26) in 28 ms on 192.168.0.30 (executor driver) (22/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 33.0 in stage 4.0 (TID 37)
21/11/23 11:28:10 @INFO @Executor@ Finished task 24.0 in stage 4.0 (TID 28). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Running task 32.0 in stage 4.0 (TID 36)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 20.0 in stage 4.0 (TID 24) in 32 ms on 192.168.0.30 (executor driver) (23/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 23.0 in stage 4.0 (TID 27) in 26 ms on 192.168.0.30 (executor driver) (24/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 34.0 in stage 4.0 (TID 38) (192.168.0.30, executor driver, partition 34, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 24.0 in stage 4.0 (TID 28) in 26 ms on 192.168.0.30 (executor driver) (25/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 34.0 in stage 4.0 (TID 38)
21/11/23 11:28:10 @INFO @Executor@ Finished task 27.0 in stage 4.0 (TID 31). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 35.0 in stage 4.0 (TID 39) (192.168.0.30, executor driver, partition 35, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 35.0 in stage 4.0 (TID 39)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 27.0 in stage 4.0 (TID 31) in 23 ms on 192.168.0.30 (executor driver) (26/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 25.0 in stage 4.0 (TID 29). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 36.0 in stage 4.0 (TID 40) (192.168.0.30, executor driver, partition 36, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 25.0 in stage 4.0 (TID 29) in 28 ms on 192.168.0.30 (executor driver) (27/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 29.0 in stage 4.0 (TID 33). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 26.0 in stage 4.0 (TID 30). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Running task 36.0 in stage 4.0 (TID 40)
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 37.0 in stage 4.0 (TID 41) (192.168.0.30, executor driver, partition 37, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 38.0 in stage 4.0 (TID 42) (192.168.0.30, executor driver, partition 38, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 37.0 in stage 4.0 (TID 41)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 29.0 in stage 4.0 (TID 33) in 24 ms on 192.168.0.30 (executor driver) (28/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 38.0 in stage 4.0 (TID 42)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 26.0 in stage 4.0 (TID 30) in 30 ms on 192.168.0.30 (executor driver) (29/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 28.0 in stage 4.0 (TID 32). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 39.0 in stage 4.0 (TID 43) (192.168.0.30, executor driver, partition 39, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 28.0 in stage 4.0 (TID 32) in 34 ms on 192.168.0.30 (executor driver) (30/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 39.0 in stage 4.0 (TID 43)
21/11/23 11:28:10 @INFO @Executor@ Finished task 30.0 in stage 4.0 (TID 34). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 40.0 in stage 4.0 (TID 44) (192.168.0.30, executor driver, partition 40, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 30.0 in stage 4.0 (TID 34) in 27 ms on 192.168.0.30 (executor driver) (31/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 40.0 in stage 4.0 (TID 44)
21/11/23 11:28:10 @INFO @Executor@ Finished task 31.0 in stage 4.0 (TID 35). 2027 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 41.0 in stage 4.0 (TID 45) (192.168.0.30, executor driver, partition 41, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Finished task 32.0 in stage 4.0 (TID 36). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Running task 41.0 in stage 4.0 (TID 45)
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 42.0 in stage 4.0 (TID 46) (192.168.0.30, executor driver, partition 42, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 31.0 in stage 4.0 (TID 35) in 26 ms on 192.168.0.30 (executor driver) (32/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 42.0 in stage 4.0 (TID 46)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 32.0 in stage 4.0 (TID 36) in 25 ms on 192.168.0.30 (executor driver) (33/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 34.0 in stage 4.0 (TID 38). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 43.0 in stage 4.0 (TID 47) (192.168.0.30, executor driver, partition 43, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 43.0 in stage 4.0 (TID 47)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 34.0 in stage 4.0 (TID 38) in 23 ms on 192.168.0.30 (executor driver) (34/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 33.0 in stage 4.0 (TID 37). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 44.0 in stage 4.0 (TID 48) (192.168.0.30, executor driver, partition 44, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 33.0 in stage 4.0 (TID 37) in 30 ms on 192.168.0.30 (executor driver) (35/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 44.0 in stage 4.0 (TID 48)
21/11/23 11:28:10 @INFO @Executor@ Finished task 35.0 in stage 4.0 (TID 39). 1990 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 45.0 in stage 4.0 (TID 49) (192.168.0.30, executor driver, partition 45, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 35.0 in stage 4.0 (TID 39) in 28 ms on 192.168.0.30 (executor driver) (36/50)
21/11/23 11:28:10 @INFO @Executor@ Running task 45.0 in stage 4.0 (TID 49)
21/11/23 11:28:10 @INFO @Executor@ Finished task 38.0 in stage 4.0 (TID 42). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 46.0 in stage 4.0 (TID 50) (192.168.0.30, executor driver, partition 46, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 46.0 in stage 4.0 (TID 50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 38.0 in stage 4.0 (TID 42) in 27 ms on 192.168.0.30 (executor driver) (37/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 37.0 in stage 4.0 (TID 41). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 36.0 in stage 4.0 (TID 40). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 47.0 in stage 4.0 (TID 51) (192.168.0.30, executor driver, partition 47, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 48.0 in stage 4.0 (TID 52) (192.168.0.30, executor driver, partition 48, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 47.0 in stage 4.0 (TID 51)
21/11/23 11:28:10 @INFO @Executor@ Running task 48.0 in stage 4.0 (TID 52)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 37.0 in stage 4.0 (TID 41) in 29 ms on 192.168.0.30 (executor driver) (38/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 36.0 in stage 4.0 (TID 40) in 30 ms on 192.168.0.30 (executor driver) (39/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 39.0 in stage 4.0 (TID 43). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 49.0 in stage 4.0 (TID 53) (192.168.0.30, executor driver, partition 49, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 49.0 in stage 4.0 (TID 53)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 39.0 in stage 4.0 (TID 43) in 24 ms on 192.168.0.30 (executor driver) (40/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 40.0 in stage 4.0 (TID 44). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 40.0 in stage 4.0 (TID 44) in 25 ms on 192.168.0.30 (executor driver) (41/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 43.0 in stage 4.0 (TID 47). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 43.0 in stage 4.0 (TID 47) in 21 ms on 192.168.0.30 (executor driver) (42/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 41.0 in stage 4.0 (TID 45). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 41.0 in stage 4.0 (TID 45) in 24 ms on 192.168.0.30 (executor driver) (43/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 44.0 in stage 4.0 (TID 48). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 44.0 in stage 4.0 (TID 48) in 21 ms on 192.168.0.30 (executor driver) (44/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 42.0 in stage 4.0 (TID 46). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 42.0 in stage 4.0 (TID 46) in 28 ms on 192.168.0.30 (executor driver) (45/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 45.0 in stage 4.0 (TID 49). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 45.0 in stage 4.0 (TID 49) in 23 ms on 192.168.0.30 (executor driver) (46/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 47.0 in stage 4.0 (TID 51). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 46.0 in stage 4.0 (TID 50). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 48.0 in stage 4.0 (TID 52). 1984 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 47.0 in stage 4.0 (TID 51) in 19 ms on 192.168.0.30 (executor driver) (47/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 46.0 in stage 4.0 (TID 50) in 21 ms on 192.168.0.30 (executor driver) (48/50)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 48.0 in stage 4.0 (TID 52) in 18 ms on 192.168.0.30 (executor driver) (49/50)
21/11/23 11:28:10 @INFO @Executor@ Finished task 49.0 in stage 4.0 (TID 53). 1990 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 49.0 in stage 4.0 (TID 53) in 14 ms on 192.168.0.30 (executor driver) (50/50)
21/11/23 11:28:10 @INFO @TaskSchedulerImpl@ Removed TaskSet 4.0, whose tasks have all completed, from pool 
21/11/23 11:28:10 @INFO @DAGScheduler@ ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.171 s
21/11/23 11:28:10 @INFO @DAGScheduler@ Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:28:10 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 4: Stage finished
21/11/23 11:28:10 @INFO @DAGScheduler@ Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.172445 s
21/11/23 11:28:10 @INFO @InMemoryFileIndex@ It took 247 ms to list leaf files for 50 paths.
21/11/23 11:28:10 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:28:10 @INFO @DAGScheduler@ Registering RDD 18 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0
21/11/23 11:28:10 @INFO @DAGScheduler@ Got map stage job 5 (showString at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:28:10 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 5 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:28:10 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:28:10 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:28:10 @INFO @DAGScheduler@ Submitting ShuffleMapStage 5 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:28:10 @INFO @MemoryStore@ Block broadcast_8 stored as values in memory (estimated size 35.2 KiB, free 433.5 MiB)
21/11/23 11:28:10 @INFO @MemoryStore@ Block broadcast_8_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 433.5 MiB)
21/11/23 11:28:10 @INFO @BlockManagerInfo@ Added broadcast_8_piece0 in memory on 192.168.0.30:64455 (size: 16.2 KiB, free: 434.2 MiB)
21/11/23 11:28:10 @INFO @SparkContext@ Created broadcast 8 from broadcast at DAGScheduler.scala:1427
21/11/23 11:28:10 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:28:10 @INFO @TaskSchedulerImpl@ Adding task set 5.0 with 10 tasks resource profile 0
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 0.0 in stage 5.0 (TID 54) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 1.0 in stage 5.0 (TID 55) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 2.0 in stage 5.0 (TID 56) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 3.0 in stage 5.0 (TID 57) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 4.0 in stage 5.0 (TID 58) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 5.0 in stage 5.0 (TID 59) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 6.0 in stage 5.0 (TID 60) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 7.0 in stage 5.0 (TID 61) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 8.0 in stage 5.0 (TID 62) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 9.0 in stage 5.0 (TID 63) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 5.0 in stage 5.0 (TID 59)
21/11/23 11:28:10 @INFO @Executor@ Running task 0.0 in stage 5.0 (TID 54)
21/11/23 11:28:10 @INFO @Executor@ Running task 4.0 in stage 5.0 (TID 58)
21/11/23 11:28:10 @INFO @Executor@ Running task 1.0 in stage 5.0 (TID 55)
21/11/23 11:28:10 @INFO @Executor@ Running task 7.0 in stage 5.0 (TID 61)
21/11/23 11:28:10 @INFO @Executor@ Running task 6.0 in stage 5.0 (TID 60)
21/11/23 11:28:10 @INFO @Executor@ Running task 2.0 in stage 5.0 (TID 56)
21/11/23 11:28:10 @INFO @Executor@ Running task 3.0 in stage 5.0 (TID 57)
21/11/23 11:28:10 @INFO @Executor@ Running task 8.0 in stage 5.0 (TID 62)
21/11/23 11:28:10 @INFO @Executor@ Running task 9.0 in stage 5.0 (TID 63)
21/11/23 11:28:10 @INFO @CodeGenerator@ Code generated in 4.153583 ms
21/11/23 11:28:10 @INFO @CodeGenerator@ Code generated in 2.846875 ms
21/11/23 11:28:10 @INFO @CodeGenerator@ Code generated in 2.576375 ms
21/11/23 11:28:10 @INFO @CodeGenerator@ Code generated in 2.72625 ms
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:28:10 @INFO @BlockManagerInfo@ Removed broadcast_5_piece0 on 192.168.0.30:64455 in memory (size: 83.0 KiB, free: 434.3 MiB)
21/11/23 11:28:10 @INFO @BlockManagerInfo@ Removed broadcast_7_piece0 on 192.168.0.30:64455 in memory (size: 36.2 KiB, free: 434.3 MiB)
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:28:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:28:10 @INFO @Executor@ Finished task 8.0 in stage 5.0 (TID 62). 3145 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 2.0 in stage 5.0 (TID 56). 3145 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 1.0 in stage 5.0 (TID 55). 3145 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 3.0 in stage 5.0 (TID 57). 3145 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 4.0 in stage 5.0 (TID 58). 3145 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 6.0 in stage 5.0 (TID 60). 3145 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 0.0 in stage 5.0 (TID 54). 3145 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 9.0 in stage 5.0 (TID 63). 3145 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 7.0 in stage 5.0 (TID 61). 3145 bytes result sent to driver
21/11/23 11:28:10 @INFO @Executor@ Finished task 5.0 in stage 5.0 (TID 59). 3145 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 8.0 in stage 5.0 (TID 62) in 242 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 2.0 in stage 5.0 (TID 56) in 243 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 1.0 in stage 5.0 (TID 55) in 243 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 3.0 in stage 5.0 (TID 57) in 243 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 4.0 in stage 5.0 (TID 58) in 244 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 6.0 in stage 5.0 (TID 60) in 244 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 9.0 in stage 5.0 (TID 63) in 243 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 0.0 in stage 5.0 (TID 54) in 245 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 7.0 in stage 5.0 (TID 61) in 243 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 5.0 in stage 5.0 (TID 59) in 244 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:28:10 @INFO @TaskSchedulerImpl@ Removed TaskSet 5.0, whose tasks have all completed, from pool 
21/11/23 11:28:10 @INFO @DAGScheduler@ ShuffleMapStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 0.256 s
21/11/23 11:28:10 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:28:10 @INFO @DAGScheduler@ running: Set()
21/11/23 11:28:10 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:28:10 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:28:10 @INFO @ShufflePartitionsUtil@ For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
21/11/23 11:28:10 @INFO @CodeGenerator@ Code generated in 11.737959 ms
21/11/23 11:28:10 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:28:10 @INFO @DAGScheduler@ Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:28:10 @INFO @DAGScheduler@ Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:28:10 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 6)
21/11/23 11:28:10 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:28:10 @INFO @DAGScheduler@ Submitting ResultStage 7 (MapPartitionsRDD[21] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:28:10 @INFO @MemoryStore@ Block broadcast_9 stored as values in memory (estimated size 37.5 KiB, free 433.9 MiB)
21/11/23 11:28:10 @INFO @MemoryStore@ Block broadcast_9_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 433.9 MiB)
21/11/23 11:28:10 @INFO @BlockManagerInfo@ Added broadcast_9_piece0 in memory on 192.168.0.30:64455 (size: 17.9 KiB, free: 434.3 MiB)
21/11/23 11:28:10 @INFO @SparkContext@ Created broadcast 9 from broadcast at DAGScheduler.scala:1427
21/11/23 11:28:10 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[21] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:28:10 @INFO @TaskSchedulerImpl@ Adding task set 7.0 with 1 tasks resource profile 0
21/11/23 11:28:10 @INFO @TaskSetManager@ Starting task 0.0 in stage 7.0 (TID 64) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:28:10 @INFO @Executor@ Running task 0.0 in stage 7.0 (TID 64)
21/11/23 11:28:10 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (3.9 KiB) non-empty blocks including 10 (3.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:28:10 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 5 ms
21/11/23 11:28:10 @INFO @Executor@ Finished task 0.0 in stage 7.0 (TID 64). 4440 bytes result sent to driver
21/11/23 11:28:10 @INFO @TaskSetManager@ Finished task 0.0 in stage 7.0 (TID 64) in 35 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:28:10 @INFO @TaskSchedulerImpl@ Removed TaskSet 7.0, whose tasks have all completed, from pool 
21/11/23 11:28:10 @INFO @DAGScheduler@ ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 0.040 s
21/11/23 11:28:10 @INFO @DAGScheduler@ Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:28:10 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 7: Stage finished
21/11/23 11:28:10 @INFO @DAGScheduler@ Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.045663 s
21/11/23 11:28:10 @INFO @CodeGenerator@ Code generated in 3.452958 ms
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:23 @INFO @FileSourceStrategy@ Pushed Filters: IsNotNull(code),IsNotNull(gengo),IsNotNull(kenmei),Not(EqualTo(code,都道府県コード)),EqualTo(gengo,平成),Not(EqualTo(kenmei,全国))
21/11/23 11:28:23 @INFO @FileSourceStrategy@ Post-Scan Filters: isnotnull(code#0),isnotnull(gengo#2),isnotnull(kenmei#1),NOT (code#0 = 都道府県コード),(gengo#2 = 平成),NOT (kenmei#1 = 全国)
21/11/23 11:28:23 @INFO @FileSourceStrategy@ Output Data Schema: struct<code: string, kenmei: string, gengo: string ... 1 more fields>
21/11/23 11:28:23 @INFO @FileUtils@ Creating directory if it doesn't exist: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code/.hive-staging_hive_2021-11-23_11-28-23_435_1499482188435384500-1
21/11/23 11:28:23 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:28:23 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:28:23 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
21/11/23 11:28:23 @INFO @CodeGenerator@ Code generated in 4.101791 ms
21/11/23 11:28:23 @INFO @MemoryStore@ Block broadcast_10 stored as values in memory (estimated size 192.5 KiB, free 433.7 MiB)
21/11/23 11:28:23 @INFO @MemoryStore@ Block broadcast_10_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 433.6 MiB)
21/11/23 11:28:23 @INFO @BlockManagerInfo@ Added broadcast_10_piece0 in memory on 192.168.0.30:64455 (size: 33.6 KiB, free: 434.3 MiB)
21/11/23 11:28:23 @INFO @SparkContext@ Created broadcast 10 from sql at NativeMethodAccessorImpl.java:0
21/11/23 11:28:23 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:28:23 @INFO @SparkContext@ Starting job: sql at NativeMethodAccessorImpl.java:0
21/11/23 11:28:23 @INFO @DAGScheduler@ Got job 7 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:28:23 @INFO @DAGScheduler@ Final stage: ResultStage 8 (sql at NativeMethodAccessorImpl.java:0)
21/11/23 11:28:23 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:28:23 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:28:23 @INFO @DAGScheduler@ Submitting ResultStage 8 (MapPartitionsRDD[24] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:28:23 @INFO @MemoryStore@ Block broadcast_11 stored as values in memory (estimated size 312.9 KiB, free 433.3 MiB)
21/11/23 11:28:23 @INFO @MemoryStore@ Block broadcast_11_piece0 stored as bytes in memory (estimated size 112.0 KiB, free 433.2 MiB)
21/11/23 11:28:23 @INFO @BlockManagerInfo@ Added broadcast_11_piece0 in memory on 192.168.0.30:64455 (size: 112.0 KiB, free: 434.2 MiB)
21/11/23 11:28:23 @INFO @SparkContext@ Created broadcast 11 from broadcast at DAGScheduler.scala:1427
21/11/23 11:28:23 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[24] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:28:23 @INFO @TaskSchedulerImpl@ Adding task set 8.0 with 1 tasks resource profile 0
21/11/23 11:28:23 @INFO @TaskSetManager@ Starting task 0.0 in stage 8.0 (TID 65) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4914 bytes) taskResourceAssignments Map()
21/11/23 11:28:23 @INFO @Executor@ Running task 0.0 in stage 8.0 (TID 65)
21/11/23 11:28:23 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:28:23 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:28:23 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
21/11/23 11:28:23 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/jinko.csv, range: 0-55038, partition values: [empty row]
21/11/23 11:28:23 @INFO @CodeGenerator@ Code generated in 3.860458 ms
21/11/23 11:28:23 @INFO @CodeGenerator@ Code generated in 2.382292 ms
21/11/23 11:28:23 @INFO @FileOutputCommitter@ Saved output of task 'attempt_202111231128234868162017087102716_0008_m_000000_65' to file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code/.hive-staging_hive_2021-11-23_11-28-23_435_1499482188435384500-1/-ext-10000/_temporary/0/task_202111231128234868162017087102716_0008_m_000000
21/11/23 11:28:23 @INFO @SparkHadoopMapRedUtil@ attempt_202111231128234868162017087102716_0008_m_000000_65: Committed
21/11/23 11:28:23 @INFO @Executor@ Finished task 0.0 in stage 8.0 (TID 65). 2571 bytes result sent to driver
21/11/23 11:28:23 @INFO @TaskSetManager@ Finished task 0.0 in stage 8.0 (TID 65) in 62 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:28:23 @INFO @TaskSchedulerImpl@ Removed TaskSet 8.0, whose tasks have all completed, from pool 
21/11/23 11:28:23 @INFO @DAGScheduler@ ResultStage 8 (sql at NativeMethodAccessorImpl.java:0) finished in 0.144 s
21/11/23 11:28:23 @INFO @DAGScheduler@ Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:28:23 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 8: Stage finished
21/11/23 11:28:23 @INFO @DAGScheduler@ Job 7 finished: sql at NativeMethodAccessorImpl.java:0, took 0.146493 s
21/11/23 11:28:23 @INFO @FileFormatWriter@ Start to commit write Job 56a1f8ee-2d39-455c-bbf4-62ecf569b47b.
21/11/23 11:28:23 @INFO @FileFormatWriter@ Write Job 56a1f8ee-2d39-455c-bbf4-62ecf569b47b committed. Elapsed time: 6 ms.
21/11/23 11:28:23 @INFO @FileFormatWriter@ Finished processing stats for write job 56a1f8ee-2d39-455c-bbf4-62ecf569b47b.
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:23 @INFO @deprecation@ org.apache.hadoop.shaded.io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
21/11/23 11:28:23 @INFO @FileUtils@ Creating directory if it doesn't exist: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code
21/11/23 11:28:23 @INFO @SessionState@ Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
21/11/23 11:28:23 @INFO @SessionState@ Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: alter_table: db=data_management_crush_course tbl=jinko_code newtbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=alter_table: db=data_management_crush_course tbl=jinko_code newtbl=jinko_code	
21/11/23 11:28:23 @INFO @log@ Updating table stats fast for jinko_code
21/11/23 11:28:23 @INFO @log@ Updated size of table jinko_code to 4074
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:23 @INFO @HiveMetaStore@ 0: alter_table: db=data_management_crush_course tbl=jinko_code newtbl=jinko_code
21/11/23 11:28:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=alter_table: db=data_management_crush_course tbl=jinko_code newtbl=jinko_code	
21/11/23 11:28:23 @INFO @log@ Updating table stats fast for jinko_code
21/11/23 11:28:23 @INFO @log@ Updated size of table jinko_code to 4074
21/11/23 11:28:37 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:37 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:37 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:37 @INFO @FileSourceStrategy@ Pushed Filters: IsNotNull(code),IsNotNull(gengo),IsNotNull(kenmei),Not(EqualTo(code,都道府県コード)),EqualTo(gengo,平成),Not(EqualTo(kenmei,全国))
21/11/23 11:28:37 @INFO @FileSourceStrategy@ Post-Scan Filters: isnotnull(code#0),isnotnull(gengo#2),isnotnull(kenmei#1),NOT (code#0 = 都道府県コード),(gengo#2 = 平成),NOT (kenmei#1 = 全国)
21/11/23 11:28:37 @INFO @FileSourceStrategy@ Output Data Schema: struct<code: string, kenmei: string, gengo: string ... 1 more fields>
21/11/23 11:28:37 @INFO @FileUtils@ Creating directory if it doesn't exist: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code/.hive-staging_hive_2021-11-23_11-28-37_865_7742676331152628443-1
21/11/23 11:28:37 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:28:37 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:28:37 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
21/11/23 11:28:37 @INFO @MemoryStore@ Block broadcast_12 stored as values in memory (estimated size 192.5 KiB, free 433.0 MiB)
21/11/23 11:28:37 @INFO @MemoryStore@ Block broadcast_12_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 433.0 MiB)
21/11/23 11:28:37 @INFO @BlockManagerInfo@ Added broadcast_12_piece0 in memory on 192.168.0.30:64455 (size: 33.6 KiB, free: 434.1 MiB)
21/11/23 11:28:37 @INFO @SparkContext@ Created broadcast 12 from sql at NativeMethodAccessorImpl.java:0
21/11/23 11:28:37 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:28:37 @INFO @SparkContext@ Starting job: sql at NativeMethodAccessorImpl.java:0
21/11/23 11:28:37 @INFO @DAGScheduler@ Got job 8 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:28:37 @INFO @DAGScheduler@ Final stage: ResultStage 9 (sql at NativeMethodAccessorImpl.java:0)
21/11/23 11:28:37 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:28:37 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:28:37 @INFO @DAGScheduler@ Submitting ResultStage 9 (MapPartitionsRDD[27] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:28:37 @INFO @MemoryStore@ Block broadcast_13 stored as values in memory (estimated size 312.9 KiB, free 432.7 MiB)
21/11/23 11:28:38 @INFO @MemoryStore@ Block broadcast_13_piece0 stored as bytes in memory (estimated size 112.0 KiB, free 432.6 MiB)
21/11/23 11:28:38 @INFO @BlockManagerInfo@ Added broadcast_13_piece0 in memory on 192.168.0.30:64455 (size: 112.0 KiB, free: 434.0 MiB)
21/11/23 11:28:38 @INFO @SparkContext@ Created broadcast 13 from broadcast at DAGScheduler.scala:1427
21/11/23 11:28:38 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[27] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:28:38 @INFO @TaskSchedulerImpl@ Adding task set 9.0 with 1 tasks resource profile 0
21/11/23 11:28:38 @INFO @TaskSetManager@ Starting task 0.0 in stage 9.0 (TID 66) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4914 bytes) taskResourceAssignments Map()
21/11/23 11:28:38 @INFO @Executor@ Running task 0.0 in stage 9.0 (TID 66)
21/11/23 11:28:38 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:28:38 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:28:38 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
21/11/23 11:28:38 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/jinko.csv, range: 0-55038, partition values: [empty row]
21/11/23 11:28:38 @INFO @FileOutputCommitter@ Saved output of task 'attempt_20211123112837907818317422103826_0009_m_000000_66' to file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code/.hive-staging_hive_2021-11-23_11-28-37_865_7742676331152628443-1/-ext-10000/_temporary/0/task_20211123112837907818317422103826_0009_m_000000
21/11/23 11:28:38 @INFO @SparkHadoopMapRedUtil@ attempt_20211123112837907818317422103826_0009_m_000000_66: Committed
21/11/23 11:28:38 @INFO @Executor@ Finished task 0.0 in stage 9.0 (TID 66). 2571 bytes result sent to driver
21/11/23 11:28:38 @INFO @TaskSetManager@ Finished task 0.0 in stage 9.0 (TID 66) in 26 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:28:38 @INFO @TaskSchedulerImpl@ Removed TaskSet 9.0, whose tasks have all completed, from pool 
21/11/23 11:28:38 @INFO @DAGScheduler@ ResultStage 9 (sql at NativeMethodAccessorImpl.java:0) finished in 0.082 s
21/11/23 11:28:38 @INFO @DAGScheduler@ Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:28:38 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 9: Stage finished
21/11/23 11:28:38 @INFO @DAGScheduler@ Job 8 finished: sql at NativeMethodAccessorImpl.java:0, took 0.083648 s
21/11/23 11:28:38 @INFO @FileFormatWriter@ Start to commit write Job 1bd23b15-977b-4135-b777-3dda7593d469.
21/11/23 11:28:38 @INFO @FileFormatWriter@ Write Job 1bd23b15-977b-4135-b777-3dda7593d469 committed. Elapsed time: 6 ms.
21/11/23 11:28:38 @INFO @FileFormatWriter@ Finished processing stats for write job 1bd23b15-977b-4135-b777-3dda7593d469.
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:38 @INFO @FileUtils@ Creating directory if it doesn't exist: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code
21/11/23 11:28:38 @INFO @SessionState@ Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
21/11/23 11:28:38 @INFO @SessionState@ Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: alter_table: db=data_management_crush_course tbl=jinko_code newtbl=jinko_code
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=alter_table: db=data_management_crush_course tbl=jinko_code newtbl=jinko_code	
21/11/23 11:28:38 @INFO @log@ Updating table stats fast for jinko_code
21/11/23 11:28:38 @INFO @log@ Updated size of table jinko_code to 4074
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:38 @INFO @HiveMetaStore@ 0: alter_table: db=data_management_crush_course tbl=jinko_code newtbl=jinko_code
21/11/23 11:28:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=alter_table: db=data_management_crush_course tbl=jinko_code newtbl=jinko_code	
21/11/23 11:28:38 @INFO @log@ Updating table stats fast for jinko_code
21/11/23 11:28:38 @INFO @log@ Updated size of table jinko_code to 4074
21/11/23 11:28:41 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:41 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:41 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:41 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:41 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:41 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:41 @INFO @MemoryStore@ Block broadcast_14 stored as values in memory (estimated size 192.7 KiB, free 432.4 MiB)
21/11/23 11:28:41 @INFO @MemoryStore@ Block broadcast_14_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 432.4 MiB)
21/11/23 11:28:41 @INFO @BlockManagerInfo@ Added broadcast_14_piece0 in memory on 192.168.0.30:64455 (size: 33.7 KiB, free: 434.0 MiB)
21/11/23 11:28:41 @INFO @SparkContext@ Created broadcast 14 from 
21/11/23 11:28:41 @INFO @FileInputFormat@ Total input files to process : 1
21/11/23 11:28:41 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:28:41 @INFO @DAGScheduler@ Got job 9 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:28:41 @INFO @DAGScheduler@ Final stage: ResultStage 10 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:28:41 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:28:41 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:28:41 @INFO @DAGScheduler@ Submitting ResultStage 10 (MapPartitionsRDD[32] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:28:41 @INFO @MemoryStore@ Block broadcast_15 stored as values in memory (estimated size 10.8 KiB, free 432.3 MiB)
21/11/23 11:28:41 @INFO @MemoryStore@ Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 432.3 MiB)
21/11/23 11:28:41 @INFO @BlockManagerInfo@ Added broadcast_15_piece0 in memory on 192.168.0.30:64455 (size: 5.6 KiB, free: 434.0 MiB)
21/11/23 11:28:41 @INFO @SparkContext@ Created broadcast 15 from broadcast at DAGScheduler.scala:1427
21/11/23 11:28:41 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[32] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:28:41 @INFO @TaskSchedulerImpl@ Adding task set 10.0 with 1 tasks resource profile 0
21/11/23 11:28:41 @INFO @TaskSetManager@ Starting task 0.0 in stage 10.0 (TID 67) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4646 bytes) taskResourceAssignments Map()
21/11/23 11:28:41 @INFO @Executor@ Running task 0.0 in stage 10.0 (TID 67)
21/11/23 11:28:41 @INFO @HadoopRDD@ Input split: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code/part-00000-9fa5a8ad-90ba-42b3-aff4-1bfc768f81a1-c000:0+4074
21/11/23 11:28:41 @INFO @CodeGenerator@ Code generated in 2.574292 ms
21/11/23 11:28:41 @INFO @Executor@ Finished task 0.0 in stage 10.0 (TID 67). 1772 bytes result sent to driver
21/11/23 11:28:41 @INFO @TaskSetManager@ Finished task 0.0 in stage 10.0 (TID 67) in 33 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:28:41 @INFO @TaskSchedulerImpl@ Removed TaskSet 10.0, whose tasks have all completed, from pool 
21/11/23 11:28:41 @INFO @DAGScheduler@ ResultStage 10 (showString at NativeMethodAccessorImpl.java:0) finished in 0.038 s
21/11/23 11:28:41 @INFO @DAGScheduler@ Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:28:41 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 10: Stage finished
21/11/23 11:28:41 @INFO @DAGScheduler@ Job 9 finished: showString at NativeMethodAccessorImpl.java:0, took 0.040036 s
21/11/23 11:28:41 @INFO @CodeGenerator@ Code generated in 2.148458 ms
21/11/23 11:28:55 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:28:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:28:55 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:55 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:28:55 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:28:55 @INFO @MemoryStore@ Block broadcast_16 stored as values in memory (estimated size 192.7 KiB, free 432.2 MiB)
21/11/23 11:28:55 @INFO @MemoryStore@ Block broadcast_16_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 432.1 MiB)
21/11/23 11:28:55 @INFO @BlockManagerInfo@ Added broadcast_16_piece0 in memory on 192.168.0.30:64455 (size: 33.7 KiB, free: 433.9 MiB)
21/11/23 11:28:55 @INFO @SparkContext@ Created broadcast 16 from 
21/11/23 11:28:55 @INFO @FileInputFormat@ Total input files to process : 1
21/11/23 11:28:55 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:28:55 @INFO @DAGScheduler@ Got job 10 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:28:55 @INFO @DAGScheduler@ Final stage: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:28:55 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:28:55 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:28:55 @INFO @DAGScheduler@ Submitting ResultStage 11 (MapPartitionsRDD[37] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:28:55 @INFO @MemoryStore@ Block broadcast_17 stored as values in memory (estimated size 10.8 KiB, free 432.1 MiB)
21/11/23 11:28:55 @INFO @MemoryStore@ Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 432.1 MiB)
21/11/23 11:28:55 @INFO @BlockManagerInfo@ Added broadcast_17_piece0 in memory on 192.168.0.30:64455 (size: 5.6 KiB, free: 433.9 MiB)
21/11/23 11:28:55 @INFO @SparkContext@ Created broadcast 17 from broadcast at DAGScheduler.scala:1427
21/11/23 11:28:55 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[37] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:28:55 @INFO @TaskSchedulerImpl@ Adding task set 11.0 with 1 tasks resource profile 0
21/11/23 11:28:55 @INFO @TaskSetManager@ Starting task 0.0 in stage 11.0 (TID 68) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4646 bytes) taskResourceAssignments Map()
21/11/23 11:28:55 @INFO @Executor@ Running task 0.0 in stage 11.0 (TID 68)
21/11/23 11:28:55 @INFO @HadoopRDD@ Input split: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code/part-00000-9fa5a8ad-90ba-42b3-aff4-1bfc768f81a1-c000:0+4074
21/11/23 11:28:55 @INFO @Executor@ Finished task 0.0 in stage 11.0 (TID 68). 1772 bytes result sent to driver
21/11/23 11:28:55 @INFO @TaskSetManager@ Finished task 0.0 in stage 11.0 (TID 68) in 5 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:28:55 @INFO @TaskSchedulerImpl@ Removed TaskSet 11.0, whose tasks have all completed, from pool 
21/11/23 11:28:55 @INFO @DAGScheduler@ ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 0.009 s
21/11/23 11:28:55 @INFO @DAGScheduler@ Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:28:55 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 11: Stage finished
21/11/23 11:28:55 @INFO @DAGScheduler@ Job 10 finished: showString at NativeMethodAccessorImpl.java:0, took 0.010997 s
21/11/23 11:28:57 @INFO @SparkUI@ Stopped Spark web UI at http://192.168.0.30:4043
21/11/23 11:28:57 @INFO @MapOutputTrackerMasterEndpoint@ MapOutputTrackerMasterEndpoint stopped!
21/11/23 11:28:57 @INFO @MemoryStore@ MemoryStore cleared
21/11/23 11:28:57 @INFO @BlockManager@ BlockManager stopped
21/11/23 11:28:57 @INFO @BlockManagerMaster@ BlockManagerMaster stopped
21/11/23 11:28:57 @INFO @OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ OutputCommitCoordinator stopped!
21/11/23 11:28:57 @INFO @SparkContext@ Successfully stopped SparkContext
21/11/23 11:29:12 @WARN @Utils@ Your hostname, yukisaitos-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.30 instead (on interface en0)
21/11/23 11:29:12 @WARN @Utils@ Set SPARK_LOCAL_IP if you need to bind to another address
21/11/23 11:29:13 @INFO @SparkContext@ Running Spark version 3.2.0
21/11/23 11:29:13 @WARN @NativeCodeLoader@ Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/11/23 11:29:13 @INFO @ResourceUtils@ ==============================================================
21/11/23 11:29:13 @INFO @ResourceUtils@ No custom resources configured for spark.driver.
21/11/23 11:29:13 @INFO @ResourceUtils@ ==============================================================
21/11/23 11:29:13 @INFO @SparkContext@ Submitted application: chapter1
21/11/23 11:29:13 @INFO @ResourceProfile@ Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/11/23 11:29:13 @INFO @ResourceProfile@ Limiting resource is cpu
21/11/23 11:29:13 @INFO @ResourceProfileManager@ Added ResourceProfile id: 0
21/11/23 11:29:13 @INFO @SecurityManager@ Changing view acls to: saitouyuuki
21/11/23 11:29:13 @INFO @SecurityManager@ Changing modify acls to: saitouyuuki
21/11/23 11:29:13 @INFO @SecurityManager@ Changing view acls groups to: 
21/11/23 11:29:13 @INFO @SecurityManager@ Changing modify acls groups to: 
21/11/23 11:29:13 @INFO @SecurityManager@ SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(saitouyuuki); groups with view permissions: Set(); users  with modify permissions: Set(saitouyuuki); groups with modify permissions: Set()
21/11/23 11:29:13 @INFO @Utils@ Successfully started service 'sparkDriver' on port 64565.
21/11/23 11:29:13 @INFO @SparkEnv@ Registering MapOutputTracker
21/11/23 11:29:13 @INFO @SparkEnv@ Registering BlockManagerMaster
21/11/23 11:29:13 @INFO @BlockManagerMasterEndpoint@ Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/11/23 11:29:13 @INFO @BlockManagerMasterEndpoint@ BlockManagerMasterEndpoint up
21/11/23 11:29:13 @INFO @SparkEnv@ Registering BlockManagerMasterHeartbeat
21/11/23 11:29:13 @INFO @DiskBlockManager@ Created local directory at /private/var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/blockmgr-db5ca104-c702-49e8-b422-058876bc60be
21/11/23 11:29:13 @INFO @MemoryStore@ MemoryStore started with capacity 434.4 MiB
21/11/23 11:29:13 @INFO @SparkEnv@ Registering OutputCommitCoordinator
21/11/23 11:29:13 @WARN @Utils@ Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/11/23 11:29:13 @WARN @Utils@ Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
21/11/23 11:29:13 @WARN @Utils@ Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
21/11/23 11:29:13 @INFO @Utils@ Successfully started service 'SparkUI' on port 4043.
21/11/23 11:29:13 @INFO @SparkUI@ Bound SparkUI to 0.0.0.0, and started at http://192.168.0.30:4043
21/11/23 11:29:13 @INFO @Executor@ Starting executor ID driver on host 192.168.0.30
21/11/23 11:29:13 @INFO @Utils@ Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64566.
21/11/23 11:29:13 @INFO @NettyBlockTransferService@ Server created on 192.168.0.30:64566
21/11/23 11:29:13 @INFO @BlockManager@ Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/11/23 11:29:13 @INFO @BlockManagerMaster@ Registering BlockManager BlockManagerId(driver, 192.168.0.30, 64566, None)
21/11/23 11:29:13 @INFO @BlockManagerMasterEndpoint@ Registering block manager 192.168.0.30:64566 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.30, 64566, None)
21/11/23 11:29:13 @INFO @BlockManagerMaster@ Registered BlockManager BlockManagerId(driver, 192.168.0.30, 64566, None)
21/11/23 11:29:13 @INFO @BlockManager@ Initialized BlockManager: BlockManagerId(driver, 192.168.0.30, 64566, None)
21/11/23 11:29:14 @INFO @SingleEventLogFileWriter@ Logging events to file:/private/tmp/spark-events/local-1637634553920.inprogress
21/11/23 11:29:14 @INFO @SharedState@ Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
21/11/23 11:29:14 @INFO @SharedState@ Warehouse path is 'file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse'.
21/11/23 11:29:16 @INFO @HiveConf@ Found configuration file file:/opt/homebrew/Cellar/apache-spark/3.2.0/libexec/conf/hive-site.xml
21/11/23 11:29:16 @INFO @HiveUtils@ Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
21/11/23 11:29:17 @INFO @HiveClientImpl@ Warehouse location for Hive client (version 2.3.9) is file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse
21/11/23 11:29:17 @WARN @HiveConf@ HiveConf of name hive.stats.jdbc.timeout does not exist
21/11/23 11:29:17 @WARN @HiveConf@ HiveConf of name hive.stats.retries.wait does not exist
21/11/23 11:29:17 @INFO @HiveMetaStore@ 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/11/23 11:29:17 @INFO @ObjectStore@ ObjectStore, initialize called
21/11/23 11:29:17 @INFO @Persistence@ Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/11/23 11:29:17 @INFO @Persistence@ Property datanucleus.cache.level2 unknown - will be ignored
21/11/23 11:29:17 @INFO @ObjectStore@ Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/11/23 11:29:18 @INFO @MetaStoreDirectSql@ Using direct SQL, underlying DB is MYSQL
21/11/23 11:29:18 @INFO @ObjectStore@ Initialized ObjectStore
21/11/23 11:29:18 @INFO @HiveMetaStore@ Added admin role in metastore
21/11/23 11:29:18 @INFO @HiveMetaStore@ Added public role in metastore
21/11/23 11:29:18 @INFO @HiveMetaStore@ No user is added in admin role, since config is empty
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: default
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: default	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: global_temp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: global_temp	
21/11/23 11:29:18 @WARN @ObjectStore@ Failed to get database global_temp, returning NoSuchObjectException
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_tables: db=metadata_tmp pat=*
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_tables: db=metadata_tmp pat=*	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_all_tables: db=metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_all_tables: db=metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: drop_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=drop_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: drop_database: metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=drop_database: metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_all_tables: db=metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_all_tables: db=metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_functions: db=metadata_tmp pat=*
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_functions: db=metadata_tmp pat=*	
21/11/23 11:29:18 @INFO @ObjectStore@ Dropping database metadata_tmp along with all tables
21/11/23 11:29:18 @INFO @deprecation@ org.apache.hadoop.shaded.io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
21/11/23 11:29:18 @INFO @TxnHandler@ START========"HiveConf()"========
hiveDefaultUrl=null
hiveSiteURL=file:/opt/homebrew/Cellar/apache-spark/3.2.0/libexec/conf/hive-site.xml
hiveServer2SiteUrl=null
hivemetastoreSiteUrl=null
Values omitted for security reason if present: [fs.s3n.awsAccessKeyId, fs.s3a.access.key, fs.s3.awsAccessKeyId, hive.server2.keystore.password, fs.s3a.proxy.password, javax.jdo.option.ConnectionPassword, fs.s3.awsSecretAccessKey, fs.s3n.awsSecretAccessKey, fs.s3a.secret.key]
adl.feature.ownerandgroup.enableupn=false
adl.http.timeout=-1
datanucleus.autoCreateSchema=true
datanucleus.autoStartMechanismMode=ignored
datanucleus.cache.level2=false
datanucleus.cache.level2.type=none
datanucleus.connectionPool.maxPoolSize=10
datanucleus.connectionPoolingType=BONECP
datanucleus.identifierFactory=datanucleus1
datanucleus.plugin.pluginRegistryBundleCheck=LOG
datanucleus.rdbms.initializeColumnInfo=NONE
datanucleus.rdbms.useLegacyNativeValueStrategy=true
datanucleus.schema.autoCreateAll=false
datanucleus.schema.autoCreateTables=true
datanucleus.schema.validateColumns=false
datanucleus.schema.validateConstraints=false
datanucleus.schema.validateTables=false
datanucleus.storeManagerType=rdbms
datanucleus.transactionIsolation=read-committed
dfs.client.ignore.namenode.default.kms.uri=false
dfs.ha.fencing.ssh.connect-timeout=30000
file.blocksize=67108864
file.bytes-per-checksum=512
file.client-write-packet-size=65536
file.replication=1
file.stream-buffer-size=4096
fs.AbstractFileSystem.abfs.impl=org.apache.hadoop.fs.azurebfs.Abfs
fs.AbstractFileSystem.abfss.impl=org.apache.hadoop.fs.azurebfs.Abfss
fs.AbstractFileSystem.adl.impl=org.apache.hadoop.fs.adl.Adl
fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs
fs.AbstractFileSystem.ftp.impl=org.apache.hadoop.fs.ftp.FtpFs
fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs
fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs
fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A
fs.AbstractFileSystem.swebhdfs.impl=org.apache.hadoop.fs.SWebHdfs
fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs
fs.AbstractFileSystem.wasb.impl=org.apache.hadoop.fs.azure.Wasb
fs.AbstractFileSystem.wasbs.impl=org.apache.hadoop.fs.azure.Wasbs
fs.AbstractFileSystem.webhdfs.impl=org.apache.hadoop.fs.WebHdfs
fs.abfs.impl=org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
fs.abfss.impl=org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
fs.adl.impl=org.apache.hadoop.fs.adl.AdlFileSystem
fs.adl.oauth2.access.token.provider.type=ClientCredential
fs.automatic.close=true
fs.azure.authorization=false
fs.azure.authorization.caching.enable=true
fs.azure.local.sas.key.mode=false
fs.azure.sas.expiry.period=90d
fs.azure.saskey.usecontainersaskeyforallaccess=true
fs.azure.secure.mode=false
fs.azure.user.agent.prefix=unknown
fs.client.resolve.remote.symlinks=true
fs.client.resolve.topology.enabled=false
fs.defaultFS=file:///
fs.df.interval=60000
fs.du.interval=600000
fs.ftp.data.connection.mode=ACTIVE_LOCAL_DATA_CONNECTION_MODE
fs.ftp.host=0.0.0.0
fs.ftp.host.port=21
fs.ftp.impl=org.apache.hadoop.fs.ftp.FTPFileSystem
fs.ftp.timeout=0
fs.ftp.transfer.mode=BLOCK_TRANSFER_MODE
fs.getspaceused.jitterMillis=60000
fs.har.impl=org.apache.hadoop.hive.shims.HiveHarFileSystem
fs.har.impl.disable.cache=true
fs.permissions.umask-mode=022
fs.s3a.assumed.role.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
fs.s3a.assumed.role.session.duration=30m
fs.s3a.attempts.maximum=20
fs.s3a.aws.credentials.provider=
    org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,
    org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,
    com.amazonaws.auth.EnvironmentVariableCredentialsProvider,
    org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider
  
fs.s3a.block.size=32M
fs.s3a.buffer.dir=/tmp/hadoop-saitouyuuki/s3a
fs.s3a.change.detection.mode=server
fs.s3a.change.detection.source=etag
fs.s3a.change.detection.version.required=true
fs.s3a.committer.abort.pending.uploads=true
fs.s3a.committer.magic.enabled=true
fs.s3a.committer.name=file
fs.s3a.committer.staging.conflict-mode=append
fs.s3a.committer.staging.tmp.path=
    tmp/staging:
fs.s3a.committer.staging.unique-filenames=true
fs.s3a.committer.threads=8
fs.s3a.connection.establish.timeout=5000
fs.s3a.connection.maximum=48
fs.s3a.connection.request.timeout=0
fs.s3a.connection.ssl.enabled=true
fs.s3a.connection.timeout=200000
fs.s3a.delegation.tokens.enabled=false
fs.s3a.downgrade.syncable.exceptions=true
fs.s3a.endpoint=s3.amazonaws.com
fs.s3a.etag.checksum.enabled=false
fs.s3a.executor.capacity=16
fs.s3a.fast.upload.active.blocks=4
fs.s3a.fast.upload.buffer=disk
fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
fs.s3a.list.version=2
fs.s3a.max.total.tasks=32
fs.s3a.metadatastore.authoritative=false
fs.s3a.metadatastore.fail.on.write.error=true
fs.s3a.metadatastore.impl=org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore
fs.s3a.metadatastore.metadata.ttl=15m
fs.s3a.multiobjectdelete.enable=true
fs.s3a.multipart.purge=false
fs.s3a.multipart.purge.age=86400
fs.s3a.multipart.size=64M
fs.s3a.multipart.threshold=128M
fs.s3a.paging.maximum=5000
fs.s3a.path.style.access=
    false:
fs.s3a.readahead.range=64K
fs.s3a.retry.interval=500ms
fs.s3a.retry.limit=7
fs.s3a.retry.throttle.interval=100ms
fs.s3a.retry.throttle.limit=20
fs.s3a.s3guard.cli.prune.age=86400000
fs.s3a.s3guard.consistency.retry.interval=2s
fs.s3a.s3guard.consistency.retry.limit=7
fs.s3a.s3guard.ddb.background.sleep=25ms
fs.s3a.s3guard.ddb.max.retries=9
fs.s3a.s3guard.ddb.table.capacity.read=0
fs.s3a.s3guard.ddb.table.capacity.write=0
fs.s3a.s3guard.ddb.table.create=false
fs.s3a.s3guard.ddb.table.sse.enabled=false
fs.s3a.s3guard.ddb.throttle.retry.interval=100ms
fs.s3a.select.enabled=true
fs.s3a.select.errors.include.sql=false
fs.s3a.select.input.compression=none
fs.s3a.select.input.csv.comment.marker=#
fs.s3a.select.input.csv.field.delimiter=,
fs.s3a.select.input.csv.header=none
fs.s3a.select.input.csv.quote.character="
fs.s3a.select.input.csv.quote.escape.character=\\
fs.s3a.select.input.csv.record.delimiter=\n
fs.s3a.select.output.csv.field.delimiter=,
fs.s3a.select.output.csv.quote.character="
fs.s3a.select.output.csv.quote.escape.character=\\
fs.s3a.select.output.csv.quote.fields=always
fs.s3a.select.output.csv.record.delimiter=\n
fs.s3a.socket.recv.buffer=8192
fs.s3a.socket.send.buffer=8192
fs.s3a.ssl.channel.mode=default_jsse
fs.s3a.threads.keepalivetime=60
fs.s3a.threads.max=64
fs.scheme.class=dfs
fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem
fs.trash.checkpoint.interval=0
fs.trash.interval=0
fs.viewfs.overload.scheme.target.abfs.impl=org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
fs.viewfs.overload.scheme.target.abfss.impl=org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
fs.viewfs.overload.scheme.target.file.impl=org.apache.hadoop.fs.LocalFileSystem
fs.viewfs.overload.scheme.target.ftp.impl=org.apache.hadoop.fs.ftp.FTPFileSystem
fs.viewfs.overload.scheme.target.hdfs.impl=org.apache.hadoop.hdfs.DistributedFileSystem
fs.viewfs.overload.scheme.target.http.impl=org.apache.hadoop.fs.http.HttpFileSystem
fs.viewfs.overload.scheme.target.https.impl=org.apache.hadoop.fs.http.HttpsFileSystem
fs.viewfs.overload.scheme.target.o3fs.impl=org.apache.hadoop.fs.ozone.OzoneFileSystem
fs.viewfs.overload.scheme.target.ofs.impl=org.apache.hadoop.fs.ozone.RootedOzoneFileSystem
fs.viewfs.overload.scheme.target.oss.impl=org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem
fs.viewfs.overload.scheme.target.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
fs.viewfs.overload.scheme.target.swebhdfs.impl=org.apache.hadoop.hdfs.web.SWebHdfsFileSystem
fs.viewfs.overload.scheme.target.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem
fs.viewfs.overload.scheme.target.wasb.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem
fs.viewfs.overload.scheme.target.webhdfs.impl=org.apache.hadoop.hdfs.web.WebHdfsFileSystem
fs.viewfs.rename.strategy=SAME_MOUNTPOINT
fs.wasb.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem
fs.wasbs.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure
ftp.blocksize=67108864
ftp.bytes-per-checksum=512
ftp.client-write-packet-size=65536
ftp.replication=3
ftp.stream-buffer-size=4096
ha.failover-controller.active-standby-elector.zk.op.retries=3
ha.failover-controller.cli-check.rpc-timeout.ms=20000
ha.failover-controller.graceful-fence.connection.retries=1
ha.failover-controller.graceful-fence.rpc-timeout.ms=5000
ha.failover-controller.new-active.rpc-timeout.ms=60000
ha.health-monitor.check-interval.ms=1000
ha.health-monitor.connect-retry-interval.ms=1000
ha.health-monitor.rpc-timeout.ms=45000
ha.health-monitor.rpc.connect.max.retries=1
ha.health-monitor.sleep-after-disconnect.ms=1000
ha.zookeeper.acl=world:anyone:rwcda
ha.zookeeper.parent-znode=/hadoop-ha
ha.zookeeper.session-timeout.ms=10000
hadoop.bin.path=
    /usr/bin/hadoop:
hadoop.caller.context.enabled=false
hadoop.caller.context.max.size=128
hadoop.caller.context.signature.max.size=40
hadoop.common.configuration.version=3.0.0
hadoop.domainname.resolver.impl=org.apache.hadoop.net.DNSDomainNameResolver
hadoop.http.authentication.kerberos.keytab=/Users/saitouyuuki/hadoop.keytab
hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST
hadoop.http.authentication.signature.secret.file=/Users/saitouyuuki/hadoop-http-auth-signature-secret
hadoop.http.authentication.simple.anonymous.allowed=true
hadoop.http.authentication.token.validity=36000
hadoop.http.authentication.type=simple
hadoop.http.cross-origin.allowed-headers=X-Requested-With,Content-Type,Accept,Origin
hadoop.http.cross-origin.allowed-methods=GET,POST,HEAD
hadoop.http.cross-origin.allowed-origins=*
hadoop.http.cross-origin.enabled=false
hadoop.http.cross-origin.max-age=1800
hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter
hadoop.http.idle_timeout.ms=60000
hadoop.http.logs.enabled=true
hadoop.http.sni.host.check.enabled=false
hadoop.http.staticuser.user=dr.who
hadoop.jetty.logs.serve.aliases=true
hadoop.kerberos.keytab.login.autorenewal.enabled=false
hadoop.kerberos.kinit.command=kinit
hadoop.kerberos.min.seconds.before.relogin=60
hadoop.metrics.jvm.use-thread-mxbean=false
hadoop.prometheus.endpoint.enabled=false
hadoop.registry.jaas.context=Client
hadoop.registry.secure=false
hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:hdfs@
hadoop.registry.zk.connection.timeout.ms=15000
hadoop.registry.zk.quorum=localhost:2181
hadoop.registry.zk.retry.ceiling.ms=60000
hadoop.registry.zk.retry.interval.ms=1000
hadoop.registry.zk.retry.times=5
hadoop.registry.zk.root=/registry
hadoop.registry.zk.session.timeout.ms=60000
hadoop.rpc.protection=authentication
hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory
hadoop.security.auth_to_local.mechanism=hadoop
hadoop.security.authentication=simple
hadoop.security.authorization=false
hadoop.security.credential.clear-text-fallback=true
hadoop.security.crypto.buffer.size=8192
hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding
hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec
hadoop.security.dns.log-slow-lookups.enabled=false
hadoop.security.dns.log-slow-lookups.threshold.ms=1000
hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
hadoop.security.group.mapping.ldap.connection.timeout.ms=60000
hadoop.security.group.mapping.ldap.conversion.rule=none
hadoop.security.group.mapping.ldap.directory.search.timeout=10000
hadoop.security.group.mapping.ldap.num.attempts=3
hadoop.security.group.mapping.ldap.num.attempts.before.failover=3
hadoop.security.group.mapping.ldap.posix.attr.gid.name=gidNumber
hadoop.security.group.mapping.ldap.posix.attr.uid.name=uidNumber
hadoop.security.group.mapping.ldap.read.timeout.ms=60000
hadoop.security.group.mapping.ldap.search.attr.group.name=cn
hadoop.security.group.mapping.ldap.search.attr.member=member
hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group)
hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0}))
hadoop.security.group.mapping.ldap.search.group.hierarchy.levels=0
hadoop.security.group.mapping.ldap.ssl=false
hadoop.security.group.mapping.providers.combined=true
hadoop.security.groups.cache.background.reload=false
hadoop.security.groups.cache.background.reload.threads=3
hadoop.security.groups.cache.secs=300
hadoop.security.groups.cache.warn.after.ms=5000
hadoop.security.groups.negative-cache.secs=30
hadoop.security.groups.shell.command.timeout=0s
hadoop.security.instrumentation.requires.admin=false
hadoop.security.java.secure.random.algorithm=SHA1PRNG
hadoop.security.key.default.bitlength=128
hadoop.security.key.default.cipher=AES/CTR/NoPadding
hadoop.security.kms.client.authentication.retry-count=1
hadoop.security.kms.client.encrypted.key.cache.expiry=43200000
hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f
hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2
hadoop.security.kms.client.encrypted.key.cache.size=500
hadoop.security.kms.client.failover.sleep.base.millis=100
hadoop.security.kms.client.failover.sleep.max.millis=2000
hadoop.security.kms.client.timeout=60
hadoop.security.random.device.file.path=
    /dev/urandom:
hadoop.security.secure.random.impl=org.apache.hadoop.crypto.random.OpensslSecureRandom
hadoop.security.sensitive-config-keys=
      secret$
      password$
      ssl.keystore.pass$
      fs.s3a.server-side-encryption.key
      fs.s3a.*.server-side-encryption.key
      fs.s3a.secret.key
      fs.s3a.*.secret.key
      fs.s3a.session.key
      fs.s3a.*.session.key
      fs.s3a.session.token
      fs.s3a.*.session.token
      fs.azure.account.key.*
      fs.azure.oauth2.*
      fs.adl.oauth2.*
      credential$
      oauth.*secret
      oauth.*password
      oauth.*token
      hadoop.security.sensitive-config-keys
  
hadoop.security.uid.cache.secs=14400
hadoop.service.shutdown.timeout=30s
hadoop.shell.missing.defaultFs.warning=false
hadoop.shell.safely.delete.limit.num.files=100
hadoop.ssl.client.conf=ssl-client.xml
hadoop.ssl.enabled=false
hadoop.ssl.enabled.protocols=TLSv1.2
hadoop.ssl.hostname.verifier=DEFAULT
hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
hadoop.ssl.require.client.cert=false
hadoop.ssl.server.conf=ssl-server.xml
hadoop.system.tags=YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT
      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL
hadoop.tags.system=YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT
      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL
hadoop.tmp.dir=/tmp/hadoop-saitouyuuki
hadoop.user.group.static.mapping.overrides=dr.who=;
hadoop.util.hash.type=murmur
hadoop.workaround.non.threadsafe.getpwuid=true
hadoop.zk.acl=world:anyone:rwcda
hadoop.zk.num-retries=1000
hadoop.zk.retry-interval-ms=1000
hadoop.zk.timeout-ms=10000
hive.allow.udf.load.on.demand=false
hive.analyze.stmt.collect.partlevel.stats=true
hive.archive.enabled=false
hive.async.log.enabled=true
hive.ats.hook.queue.capacity=64
hive.auto.convert.join=true
hive.auto.convert.join.hashtable.max.entries=40000000
hive.auto.convert.join.noconditionaltask=true
hive.auto.convert.join.noconditionaltask.size=10000000
hive.auto.convert.join.use.nonstaged=false
hive.auto.convert.sortmerge.join=false
hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ
hive.auto.convert.sortmerge.join.reduce.side=true
hive.auto.convert.sortmerge.join.to.mapjoin=false
hive.auto.progress.timeout=0
hive.autogen.columnalias.prefix.includefuncname=false
hive.autogen.columnalias.prefix.label=_c
hive.binary.record.max.length=1000
hive.blobstore.optimizations.enabled=true
hive.blobstore.supported.schemes=s3,s3a,s3n
hive.blobstore.use.blobstore.as.scratchdir=false
hive.cache.expr.evaluation=true
hive.cbo.cnf.maxnodes=-1
hive.cbo.costmodel.cpu=0.000001
hive.cbo.costmodel.extended=false
hive.cbo.costmodel.hdfs.read=1.5
hive.cbo.costmodel.hdfs.write=10.0
hive.cbo.costmodel.local.fs.read=4.0
hive.cbo.costmodel.local.fs.write=4.0
hive.cbo.costmodel.network=150.0
hive.cbo.enable=false
hive.cbo.returnpath.hiveop=
    false:
hive.cbo.show.warnings=true
hive.cli.errors.ignore=false
hive.cli.pretty.output.num.cols=-1
hive.cli.print.current.db=false
hive.cli.print.header=false
hive.cli.prompt=hive
hive.cli.tez.session.async=true
hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.MemoryTokenStore
hive.cluster.delegation.token.store.zookeeper.znode=/hivedelegation
hive.compactor.abortedtxn.threshold=1000
hive.compactor.check.interval=300
hive.compactor.cleaner.run.interval=5000
hive.compactor.delta.num.threshold=10
hive.compactor.delta.pct.threshold=0.1
hive.compactor.history.reaper.interval=2m
hive.compactor.history.retention.attempted=2
hive.compactor.history.retention.failed=3
hive.compactor.history.retention.succeeded=3
hive.compactor.initiator.failed.compacts.threshold=2
hive.compactor.initiator.on=false
hive.compactor.max.num.delta=500
hive.compactor.worker.threads=0
hive.compactor.worker.timeout=86400
hive.compat=0.12
hive.compute.query.using.stats=true
hive.compute.splits.in.am=true
hive.conf.hidden.list=javax.jdo.option.ConnectionPassword,hive.server2.keystore.password,fs.s3.awsAccessKeyId,fs.s3.awsSecretAccessKey,fs.s3n.awsAccessKeyId,fs.s3n.awsSecretAccessKey,fs.s3a.access.key,fs.s3a.secret.key,fs.s3a.proxy.password
hive.conf.internal.variable.list=hive.added.files.path,hive.added.jars.path,hive.added.archives.path
hive.conf.restricted.list=hive.security.authenticator.manager,hive.security.authorization.manager,hive.security.metastore.authorization.manager,hive.security.metastore.authenticator.manager,hive.users.in.admin.role,hive.server2.xsrf.filter.enabled,hive.security.authorization.enabled,hive.server2.authentication.ldap.baseDN,hive.server2.authentication.ldap.url,hive.server2.authentication.ldap.Domain,hive.server2.authentication.ldap.groupDNPattern,hive.server2.authentication.ldap.groupFilter,hive.server2.authentication.ldap.userDNPattern,hive.server2.authentication.ldap.userFilter,hive.server2.authentication.ldap.groupMembershipKey,hive.server2.authentication.ldap.userMembershipKey,hive.server2.authentication.ldap.groupClassKey,hive.server2.authentication.ldap.customLDAPQuery
hive.conf.validation=true
hive.convert.join.bucket.mapjoin.tez=false
hive.count.open.txns.interval=1s
hive.counters.group.name=HIVE
hive.debug.localtask=false
hive.decode.partition.name=false
hive.default.fileformat=TextFile
hive.default.fileformat.managed=none
hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe
hive.default.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
hive.direct.sql.max.elements.in.clause=1000
hive.direct.sql.max.elements.values.clause=1000
hive.direct.sql.max.query.length=100
hive.display.partition.cols.separately=true
hive.downloaded.resources.dir=/var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/9f1320e2-c2a9-4da1-bcf2-9d28b8fbce4e_resources
hive.driver.parallel.compilation=false
hive.druid.broker.address.default=localhost:8082
hive.druid.coordinator.address.default=localhost:8081
hive.druid.http.numConnection=20
hive.druid.http.read.timeout=PT1M
hive.druid.indexer.memory.rownum.max=75000
hive.druid.indexer.partition.size.max=5000000
hive.druid.indexer.segments.granularity=DAY
hive.druid.maxTries=5
hive.druid.metadata.base=druid
hive.druid.metadata.db.type=mysql
hive.druid.passiveWaitTimeMs=30000
hive.druid.select.distribute=true
hive.druid.select.threshold=10000
hive.druid.sleep.time=PT10S
hive.druid.storage.storageDirectory=/druid/segments
hive.druid.working.directory=/tmp/workingDirectory
hive.enforce.bucketmapjoin=false
hive.enforce.sortmergebucketmapjoin=false
hive.entity.capture.transform=false
hive.entity.separator=@
hive.error.on.empty.partition=false
hive.exec.check.crossproducts=true
hive.exec.compress.intermediate=false
hive.exec.compress.output=false
hive.exec.concatenate.check.index=true
hive.exec.copyfile.maxnumfiles=1
hive.exec.copyfile.maxsize=33554432
hive.exec.counters.pull.interval=1000
hive.exec.default.partition.name=__HIVE_DEFAULT_PARTITION__
hive.exec.drop.ignorenonexistent=true
hive.exec.dynamic.partition=true
hive.exec.dynamic.partition.mode=nonstrict
hive.exec.infer.bucket.sort=false
hive.exec.infer.bucket.sort.num.buckets.power.two=false
hive.exec.input.listing.max.threads=0
hive.exec.job.debug.capture.stacktraces=true
hive.exec.job.debug.timeout=30000
hive.exec.local.scratchdir=/var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/saitouyuuki
hive.exec.max.created.files=100000
hive.exec.max.dynamic.partitions=1000
hive.exec.max.dynamic.partitions.pernode=100
hive.exec.mode.local.auto=false
hive.exec.mode.local.auto.input.files.max=4
hive.exec.mode.local.auto.inputbytes.max=134217728
hive.exec.orc.base.delta.ratio=8
hive.exec.orc.split.strategy=HYBRID
hive.exec.parallel=false
hive.exec.parallel.thread.number=8
hive.exec.perf.logger=org.apache.hadoop.hive.ql.log.PerfLogger
hive.exec.rcfile.use.explicit.header=true
hive.exec.rcfile.use.sync.cache=true
hive.exec.reducers.bytes.per.reducer=256000000
hive.exec.reducers.max=1009
hive.exec.rowoffset=false
hive.exec.schema.evolution=true
hive.exec.scratchdir=/tmp/hive
hive.exec.script.allow.partial.consumption=false
hive.exec.script.maxerrsize=100000
hive.exec.script.trust=false
hive.exec.show.job.failure.debug.info=true
hive.exec.stagingdir=.hive-staging
hive.exec.submit.local.task.via.child=true
hive.exec.submitviachild=false
hive.exec.tasklog.debug.timeout=20000
hive.exec.temporary.table.storage=default
hive.execution.engine=mr
hive.execution.mode=container
hive.exim.strict.repl.tables=true
hive.exim.uri.scheme.whitelist=hdfs,pfile,file,s3,s3a
hive.explain.dependency.append.tasktype=false
hive.explain.user=true
hive.fetch.output.serde=org.apache.hadoop.hive.serde2.DelimitedJSONSerDe
hive.fetch.task.aggr=false
hive.fetch.task.conversion=more
hive.fetch.task.conversion.threshold=1073741824
hive.file.max.footer=100
hive.fileformat.check=true
hive.groupby.limit.extrastep=true
hive.groupby.mapaggr.checkinterval=100000
hive.groupby.orderby.position.alias=false
hive.groupby.position.alias=false
hive.groupby.skewindata=false
hive.hash.table.inflation.factor=2.0
hive.hashtable.initialCapacity=100000
hive.hashtable.key.count.adjustment=1.0
hive.hashtable.loadfactor=0.75
hive.hbase.generatehfiles=false
hive.hbase.snapshot.restoredir=/tmp
hive.hbase.wal.enabled=true
hive.heartbeat.interval=1000
hive.hmshandler.force.reload.conf=false
hive.hmshandler.retry.attempts=10
hive.hmshandler.retry.interval=2000
hive.ignore.mapjoin.hint=true
hive.in.test=false
hive.in.tez.test=false
hive.index.compact.binary.search=true
hive.index.compact.file.ignore.hdfs=false
hive.index.compact.query.max.entries=10000000
hive.index.compact.query.max.size=10737418240
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
hive.insert.into.external.tables=true
hive.insert.into.multilevel.dirs=false
hive.int.timestamp.conversion.in.seconds=false
hive.io.rcfile.column.number.conf=0
hive.io.rcfile.record.buffer.size=4194304
hive.io.rcfile.record.interval=2147483647
hive.io.rcfile.tolerate.corruptions=false
hive.io.sarg.cache.max.weight.mb=10
hive.jobname.length=50
hive.join.cache.size=25000
hive.join.emit.interval=1000
hive.lazysimple.extended_boolean_literal=false
hive.limit.optimize.enable=false
hive.limit.optimize.fetch.max=50000
hive.limit.optimize.limit.file=10
hive.limit.pushdown.memory.usage=0.1
hive.limit.query.max.table.partition=-1
hive.limit.row.max.size=100000
hive.llap.allow.permanent.fns=true
hive.llap.am.liveness.connection.sleep.between.retries.ms=2000ms
hive.llap.am.liveness.connection.timeout.ms=10000ms
hive.llap.am.use.fqdn=false
hive.llap.auto.allow.uber=false
hive.llap.auto.auth=false
hive.llap.auto.enforce.stats=true
hive.llap.auto.enforce.tree=true
hive.llap.auto.enforce.vectorized=true
hive.llap.auto.max.input.size=10737418240
hive.llap.auto.max.output.size=1073741824
hive.llap.cache.allow.synthetic.fileid=false
hive.llap.client.consistent.splits=false
hive.llap.daemon.acl=*
hive.llap.daemon.am-reporter.max.threads=4
hive.llap.daemon.am.liveness.heartbeat.interval.ms=10000ms
hive.llap.daemon.communicator.num.threads=10
hive.llap.daemon.delegation.token.lifetime=14d
hive.llap.daemon.download.permanent.fns=false
hive.llap.daemon.logger=query-routing
hive.llap.daemon.memory.per.instance.mb=4096
hive.llap.daemon.num.executors=4
hive.llap.daemon.num.file.cleaner.threads=1
hive.llap.daemon.output.service.max.pending.writes=8
hive.llap.daemon.output.service.port=15003
hive.llap.daemon.output.service.send.buffer.size=131072
hive.llap.daemon.output.stream.timeout=120s
hive.llap.daemon.rpc.num.handlers=5
hive.llap.daemon.rpc.port=0
hive.llap.daemon.service.refresh.interval.sec=60s
hive.llap.daemon.shuffle.dir.watcher.enabled=false
hive.llap.daemon.task.preemption.metrics.intervals=30,60,300
hive.llap.daemon.task.scheduler.enable.preemption=true
hive.llap.daemon.task.scheduler.wait.queue.size=10
hive.llap.daemon.vcpus.per.instance=4
hive.llap.daemon.wait.queue.comparator.class.name=org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator
hive.llap.daemon.web.port=15002
hive.llap.daemon.web.ssl=false
hive.llap.daemon.xmx.headroom=5%
hive.llap.daemon.yarn.container.mb=-1
hive.llap.daemon.yarn.shuffle.port=15551
hive.llap.enable.grace.join.in.llap=false
hive.llap.execution.mode=none
hive.llap.file.cleanup.delay.seconds=300s
hive.llap.hs2.coordinator.enabled=true
hive.llap.io.allocator.alloc.max=16Mb
hive.llap.io.allocator.alloc.min=256Kb
hive.llap.io.allocator.arena.count=8
hive.llap.io.allocator.direct=true
hive.llap.io.allocator.mmap=false
hive.llap.io.allocator.mmap.path=
    /tmp:
hive.llap.io.decoding.metrics.percentiles.intervals=30
hive.llap.io.encode.alloc.size=256Kb
hive.llap.io.encode.enabled=true
hive.llap.io.encode.formats=org.apache.hadoop.mapred.TextInputFormat,
hive.llap.io.encode.slice.lrr=true
hive.llap.io.encode.slice.row.count=100000
hive.llap.io.encode.vector.serde.async.enabled=true
hive.llap.io.encode.vector.serde.enabled=true
hive.llap.io.lrfu.lambda=0.01
hive.llap.io.memory.mode=cache
hive.llap.io.memory.size=1Gb
hive.llap.io.metadata.fraction=0.1
hive.llap.io.nonvector.wrapper.enabled=true
hive.llap.io.orc.time.counters=true
hive.llap.io.threadpool.size=10
hive.llap.io.use.fileid.path=
    true:
hive.llap.io.use.lrfu=true
hive.llap.management.acl=*
hive.llap.management.rpc.port=15004
hive.llap.object.cache.enabled=true
hive.llap.orc.gap.cache=true
hive.llap.remote.token.requires.signing=true
hive.llap.skip.compile.udf.check=false
hive.llap.task.communicator.connection.sleep.between.retries.ms=2000ms
hive.llap.task.communicator.connection.timeout.ms=16000ms
hive.llap.task.communicator.listener.thread-count=30
hive.llap.task.scheduler.locality.delay=0ms
hive.llap.task.scheduler.node.disable.backoff.factor=1.5
hive.llap.task.scheduler.node.reenable.max.timeout.ms=10000ms
hive.llap.task.scheduler.node.reenable.min.timeout.ms=200ms
hive.llap.task.scheduler.num.schedulable.tasks.per.node=0
hive.llap.task.scheduler.timeout.seconds=60s
hive.llap.validate.acls=true
hive.load.dynamic.partitions.thread=15
hive.localize.resource.num.wait.attempts=5
hive.localize.resource.wait.interval=5000
hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager
hive.lock.mapred.only.operation=false
hive.lock.numretries=100
hive.lock.sleep.between.retries=60
hive.lockmgr.zookeeper.default.partition.name=__HIVE_DEFAULT_ZOOKEEPER_PARTITION__
hive.log.every.n.records=0
hive.log.explain.output=false
hive.map.aggr=true
hive.map.aggr.hash.force.flush.memory.threshold=0.9
hive.map.aggr.hash.min.reduction=0.5
hive.map.aggr.hash.percentmemory=0.5
hive.map.groupby.sorted=true
hive.mapjoin.bucket.cache.size=100
hive.mapjoin.check.memory.rows=100000
hive.mapjoin.followby.gby.localtask.max.memory.usage=0.55
hive.mapjoin.followby.map.aggr.hash.percentmemory=0.3
hive.mapjoin.hybridgrace.bloomfilter=true
hive.mapjoin.hybridgrace.hashtable=true
hive.mapjoin.hybridgrace.memcheckfrequency=1024
hive.mapjoin.hybridgrace.minnumpartitions=16
hive.mapjoin.hybridgrace.minwbsize=524288
hive.mapjoin.localtask.max.memory.usage=0.9
hive.mapjoin.optimized.hashtable=true
hive.mapjoin.optimized.hashtable.probe.percent=0.5
hive.mapjoin.optimized.hashtable.wbsize=8388608
hive.mapjoin.smalltable.filesize=25000000
hive.mapper.cannot.span.multiple.partitions=false
hive.mapred.local.mem=0
hive.mapred.partitioner=org.apache.hadoop.hive.ql.io.DefaultHivePartitioner
hive.mapred.reduce.tasks.speculative.execution=true
hive.materializedview.fileformat=ORC
hive.materializedview.rewriting=false
hive.materializedview.serde=org.apache.hadoop.hive.ql.io.orc.OrcSerde
hive.max.open.txns=100000
hive.merge.cardinality.check=true
hive.merge.mapfiles=true
hive.merge.mapredfiles=false
hive.merge.nway.joins=true
hive.merge.orcfile.stripe.level=true
hive.merge.rcfile.block.level=true
hive.merge.size.per.task=256000000
hive.merge.smallfiles.avgsize=16000000
hive.merge.sparkfiles=false
hive.merge.tezfiles=false
hive.metadata.move.exported.metadata.to.trash=true
hive.metastore.aggregate.stats.cache.clean.until=0.8
hive.metastore.aggregate.stats.cache.enabled=true
hive.metastore.aggregate.stats.cache.fpp=0.01
hive.metastore.aggregate.stats.cache.max.full=0.9
hive.metastore.aggregate.stats.cache.max.partitions=10000
hive.metastore.aggregate.stats.cache.max.reader.wait=1000
hive.metastore.aggregate.stats.cache.max.variance=0.01
hive.metastore.aggregate.stats.cache.max.writer.wait=5000
hive.metastore.aggregate.stats.cache.size=10000
hive.metastore.aggregate.stats.cache.ttl=600
hive.metastore.archive.intermediate.archived=_INTERMEDIATE_ARCHIVED
hive.metastore.archive.intermediate.extracted=_INTERMEDIATE_EXTRACTED
hive.metastore.archive.intermediate.original=_INTERMEDIATE_ORIGINAL
hive.metastore.authorization.storage.check.externaltable.drop=true
hive.metastore.authorization.storage.checks=false
hive.metastore.batch.retrieve.max=300
hive.metastore.batch.retrieve.table.partition.max=1000
hive.metastore.cache.pinobjtypes=Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order
hive.metastore.client.capability.check=false
hive.metastore.client.connect.retry.delay=1
hive.metastore.client.drop.partitions.using.expressions=true
hive.metastore.client.socket.lifetime=0
hive.metastore.client.socket.timeout=600
hive.metastore.connect.retries=3
hive.metastore.direct.sql.batch.size=0
hive.metastore.disallow.incompatible.col.type.changes=true
hive.metastore.dml.events=false
hive.metastore.event.clean.freq=0
hive.metastore.event.db.listener.timetolive=86400
hive.metastore.event.expiry.duration=0
hive.metastore.event.message.factory=org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory
hive.metastore.execute.setugi=true
hive.metastore.expression.proxy=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore
hive.metastore.failure.retries=1
hive.metastore.fastpath=
    false:
hive.metastore.filter.hook=org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl
hive.metastore.fs.handler.class=org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl
hive.metastore.fshandler.threads=15
hive.metastore.hbase.aggr.stats.cache.entries=10000
hive.metastore.hbase.aggr.stats.hbase.ttl=604800s
hive.metastore.hbase.aggr.stats.invalidator.frequency=5s
hive.metastore.hbase.aggr.stats.memory.ttl=60s
hive.metastore.hbase.aggregate.stats.cache.size=10000
hive.metastore.hbase.aggregate.stats.false.positive.probability=0.01
hive.metastore.hbase.aggregate.stats.max.partitions=10000
hive.metastore.hbase.aggregate.stats.max.variance=0.1
hive.metastore.hbase.cache.clean.until=0.8
hive.metastore.hbase.cache.max.full=0.9
hive.metastore.hbase.cache.max.reader.wait=1000ms
hive.metastore.hbase.cache.max.writer.wait=5000ms
hive.metastore.hbase.cache.ttl=600s
hive.metastore.hbase.catalog.cache.size=50000
hive.metastore.hbase.connection.class=org.apache.hadoop.hive.metastore.hbase.VanillaHBaseConnection
hive.metastore.hbase.file.metadata.threads=1
hive.metastore.initial.metadata.count.enabled=true
hive.metastore.integral.jdo.pushdown=false
hive.metastore.kerberos.principal=hive-metastore/_HOST@EXAMPLE.COM
hive.metastore.limit.partition.request=-1
hive.metastore.metrics.enabled=false
hive.metastore.orm.retrieveMapNullsAsEmptyStrings=false
hive.metastore.port=9083
hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore
hive.metastore.sasl.enabled=false
hive.metastore.schema.verification=false
hive.metastore.schema.verification.record.version=false
hive.metastore.server.max.message.size=104857600
hive.metastore.server.max.threads=1000
hive.metastore.server.min.threads=200
hive.metastore.server.tcp.keepalive=true
hive.metastore.stats.ndv.densityfunction=false
hive.metastore.stats.ndv.tuner=0.0
hive.metastore.thrift.compact.protocol.enabled=false
hive.metastore.thrift.framed.transport.enabled=false
hive.metastore.try.direct.sql=true
hive.metastore.try.direct.sql.ddl=true
hive.metastore.txn.store.impl=org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler
hive.metastore.use.SSL=false
hive.metastore.warehouse.dir=file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse
hive.msck.path.validation=
    throw:
hive.msck.repair.batch.size=0
hive.multi.insert.move.tasks.share.dependencies=false
hive.multigroupby.singlereducer=true
hive.mv.files.thread=15
hive.new.job.grouping.set.cardinality=30
hive.optimize.bucketingsorting=true
hive.optimize.bucketmapjoin=false
hive.optimize.bucketmapjoin.sortedmerge=false
hive.optimize.constant.propagation=true
hive.optimize.correlation=false
hive.optimize.cte.materialize.threshold=-1
hive.optimize.distinct.rewrite=true
hive.optimize.dynamic.partition.hashjoin=false
hive.optimize.filter.stats.reduction=false
hive.optimize.groupby=true
hive.optimize.index.autoupdate=false
hive.optimize.index.filter=false
hive.optimize.index.filter.compact.maxsize=-1
hive.optimize.index.filter.compact.minsize=5368709120
hive.optimize.index.groupby=false
hive.optimize.limittranspose=false
hive.optimize.limittranspose.reductionpercentage=1.0
hive.optimize.limittranspose.reductiontuples=0
hive.optimize.listbucketing=false
hive.optimize.metadataonly=false
hive.optimize.null.scan=true
hive.optimize.partition.columns.separate=true
hive.optimize.point.lookup=true
hive.optimize.point.lookup.min=31
hive.optimize.ppd=true
hive.optimize.ppd.storage=true
hive.optimize.ppd.windowing=true
hive.optimize.reducededuplication=true
hive.optimize.reducededuplication.min.reducer=4
hive.optimize.remove.identity.project=true
hive.optimize.sampling.orderby=false
hive.optimize.sampling.orderby.number=1000
hive.optimize.sampling.orderby.percent=0.1
hive.optimize.semijoin.conversion=true
hive.optimize.skewjoin=false
hive.optimize.skewjoin.compiletime=false
hive.optimize.sort.dynamic.partition=false
hive.optimize.union.remove=false
hive.orc.cache.stripe.details.mem.size=256Mb
hive.orc.cache.use.soft.references=false
hive.orc.compute.splits.num.threads=10
hive.orc.splits.allow.synthetic.fileid=true
hive.orc.splits.directory.batch.ms=0
hive.orc.splits.include.file.footer=false
hive.orc.splits.include.fileid=true
hive.orc.splits.ms.footer.cache.enabled=false
hive.orc.splits.ms.footer.cache.ppd.enabled=true
hive.order.columnalignment=true
hive.orderby.position.alias=true
hive.parquet.timestamp.skip.conversion=true
hive.ppd.recognizetransivity=true
hive.ppd.remove.duplicatefilters=true
hive.prewarm.enabled=false
hive.prewarm.numcontainers=10
hive.query.result.fileformat=SequenceFile
hive.query.timeout.seconds=0s
hive.querylog.enable.plan.progress=true
hive.querylog.location=/var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/saitouyuuki
hive.querylog.plan.progress.interval=60000
hive.reorder.nway.joins=true
hive.repl.cm.enabled=false
hive.repl.cm.interval=3600s
hive.repl.cm.retain=24h
hive.repl.cmrootdir=/user/hive/cmroot/
hive.repl.rootdir=/user/hive/repl/
hive.repl.task.factory=org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory
hive.resultset.use.unique.column.names=true
hive.rework.mapredwork=false
hive.rpc.query.plan=false
hive.sample.seednumber=0
hive.scratch.dir.permission=700
hive.scratchdir.lock=false
hive.script.auto.progress=false
hive.script.operator.env.blacklist=hive.txn.valid.txns,hive.script.operator.env.blacklist
hive.script.operator.id.env.var=HIVE_SCRIPT_OPERATOR_ID
hive.script.operator.truncate.env=false
hive.script.recordreader=org.apache.hadoop.hive.ql.exec.TextRecordReader
hive.script.recordwriter=org.apache.hadoop.hive.ql.exec.TextRecordWriter
hive.script.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator
hive.security.authorization.enabled=false
hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory
hive.security.authorization.sqlstd.confwhitelist=hive\.auto\..*|hive\.cbo\..*|hive\.convert\..*|hive\.exec\.dynamic\.partition.*|hive\.exec\.max\.dynamic\.partitions.*|hive\.exec\.compress\..*|hive\.exec\.infer\..*|hive\.exec\.mode.local\..*|hive\.exec\.orc\..*|hive\.exec\.parallel.*|hive\.explain\..*|hive\.fetch.task\..*|hive\.groupby\..*|hive\.hbase\..*|hive\.index\..*|hive\.index\..*|hive\.intermediate\..*|hive\.join\..*|hive\.limit\..*|hive\.log\..*|hive\.mapjoin\..*|hive\.merge\..*|hive\.optimize\..*|hive\.orc\..*|hive\.outerjoin\..*|hive\.parquet\..*|hive\.ppd\..*|hive\.prewarm\..*|hive\.server2\.thrift\.resultset\.default\.fetch\.size|hive\.server2\.proxy\.user|hive\.skewjoin\..*|hive\.smbjoin\..*|hive\.stats\..*|hive\.strict\..*|hive\.tez\..*|hive\.vectorized\..*|mapred\.map\..*|mapred\.reduce\..*|mapred\.output\.compression\.codec|mapred\.job\.queuename|mapred\.output\.compression\.type|mapred\.min\.split\.size|mapreduce\.job\.reduce\.slowstart\.completedmaps|mapreduce\.job\.queuename|mapreduce\.job\.tags|mapreduce\.input\.fileinputformat\.split\.minsize|mapreduce\.map\..*|mapreduce\.reduce\..*|mapreduce\.output\.fileoutputformat\.compress\.codec|mapreduce\.output\.fileoutputformat\.compress\.type|oozie\..*|tez\.am\..*|tez\.task\..*|tez\.runtime\..*|tez\.queue\.name|hive\.transpose\.aggr\.join|hive\.exec\.reducers\.bytes\.per\.reducer|hive\.client\.stats\.counters|hive\.exec\.default\.partition\.name|hive\.exec\.drop\.ignorenonexistent|hive\.counters\.group\.name|hive\.default\.fileformat\.managed|hive\.enforce\.bucketmapjoin|hive\.enforce\.sortmergebucketmapjoin|hive\.cache\.expr\.evaluation|hive\.query\.result\.fileformat|hive\.hashtable\.loadfactor|hive\.hashtable\.initialCapacity|hive\.ignore\.mapjoin\.hint|hive\.limit\.row\.max\.size|hive\.mapred\.mode|hive\.map\.aggr|hive\.compute\.query\.using\.stats|hive\.exec\.rowoffset|hive\.variable\.substitute|hive\.variable\.substitute\.depth|hive\.autogen\.columnalias\.prefix\.includefuncname|hive\.autogen\.columnalias\.prefix\.label|hive\.exec\.check\.crossproducts|hive\.cli\.tez\.session\.async|hive\.compat|hive\.exec\.concatenate\.check\.index|hive\.display\.partition\.cols\.separately|hive\.error\.on\.empty\.partition|hive\.execution\.engine|hive\.exec\.copyfile\.maxsize|hive\.exim\.uri\.scheme\.whitelist|hive\.file\.max\.footer|hive\.insert\.into\.multilevel\.dirs|hive\.localize\.resource\.num\.wait\.attempts|hive\.multi\.insert\.move\.tasks\.share\.dependencies|hive\.support\.quoted\.identifiers|hive\.resultset\.use\.unique\.column\.names|hive\.analyze\.stmt\.collect\.partlevel\.stats|hive\.exec\.schema\.evolution|hive\.server2\.logging\.operation\.level|hive\.server2\.thrift\.resultset\.serialize\.in\.tasks|hive\.support\.special\.characters\.tablename|hive\.exec\.job\.debug\.capture\.stacktraces|hive\.exec\.job\.debug\.timeout|hive\.llap\.io\.enabled|hive\.llap\.io\.use\.fileid\.path|hive\.llap\.daemon\.service\.hosts|hive\.llap\.execution\.mode|hive\.llap\.auto\.allow\.uber|hive\.llap\.auto\.enforce\.tree|hive\.llap\.auto\.enforce\.vectorized|hive\.llap\.auto\.enforce\.stats|hive\.llap\.auto\.max\.input\.size|hive\.llap\.auto\.max\.output\.size|hive\.llap\.skip\.compile\.udf\.check|hive\.llap\.client\.consistent\.splits|hive\.llap\.enable\.grace\.join\.in\.llap|hive\.llap\.allow\.permanent\.fns|hive\.exec\.max\.created\.files|hive\.exec\.reducers\.max|hive\.reorder\.nway\.joins|hive\.output\.file\.extension|hive\.exec\.show\.job\.failure\.debug\.info|hive\.exec\.tasklog\.debug\.timeout|hive\.query\.id
hive.security.authorization.task.factory=org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl
hive.security.command.whitelist=set,reset,dfs,add,list,delete,reload,compile
hive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
hive.security.metastore.authorization.auth.reads=true
hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider
hive.serdes.using.metastore.for.schema=org.apache.hadoop.hive.ql.io.orc.OrcSerde,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe,org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe,org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe,org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
hive.server.read.socket.timeout=10
hive.server.tcp.keepalive=true
hive.server2.allow.user.substitution=true
hive.server2.async.exec.async.compile=false
hive.server2.async.exec.keepalive.time=10
hive.server2.async.exec.shutdown.timeout=10
hive.server2.async.exec.threads=100
hive.server2.async.exec.wait.queue.size=100
hive.server2.authentication=NONE
hive.server2.authentication.ldap.groupClassKey=groupOfNames
hive.server2.authentication.ldap.groupMembershipKey=member
hive.server2.authentication.ldap.guidKey=uid
hive.server2.clear.dangling.scratchdir=false
hive.server2.clear.dangling.scratchdir.interval=1800s
hive.server2.close.session.on.disconnect=true
hive.server2.compile.lock.timeout=0s
hive.server2.enable.doAs=true
hive.server2.global.init.file.location=${env:HIVE_CONF_DIR}
hive.server2.idle.operation.timeout=432000000
hive.server2.idle.session.check.operation=true
hive.server2.idle.session.timeout=604800000
hive.server2.in.place.progress=true
hive.server2.llap.concurrent.queries=-1
hive.server2.logging.operation.enabled=true
hive.server2.logging.operation.level=EXECUTION
hive.server2.logging.operation.log.location=/var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/saitouyuuki/operation_logs
hive.server2.long.polling.timeout=5000
hive.server2.map.fair.scheduler.queue=true
hive.server2.max.start.attempts=30
hive.server2.metrics.enabled=false
hive.server2.parallel.ops.in.session=true
hive.server2.session.check.interval=21600000
hive.server2.sleep.interval.between.start.attempts=60s
hive.server2.support.dynamic.service.discovery=false
hive.server2.table.type.mapping=CLASSIC
hive.server2.tez.initialize.default.sessions=false
hive.server2.tez.session.lifetime=162h
hive.server2.tez.session.lifetime.jitter=3h
hive.server2.tez.sessions.custom.queue.allowed=true
hive.server2.tez.sessions.init.threads=16
hive.server2.tez.sessions.per.default.queue=1
hive.server2.thrift.client.connect.retry.limit=1
hive.server2.thrift.client.password=anonymous
hive.server2.thrift.client.retry.delay.seconds=1s
hive.server2.thrift.client.retry.limit=1
hive.server2.thrift.client.user=anonymous
hive.server2.thrift.exponential.backoff.slot.length=100
hive.server2.thrift.http.cookie.auth.enabled=true
hive.server2.thrift.http.cookie.is.httponly=true
hive.server2.thrift.http.cookie.is.secure=true
hive.server2.thrift.http.cookie.max.age=86400
hive.server2.thrift.http.max.idle.time=1800000
hive.server2.thrift.http.path=
    cliservice:
hive.server2.thrift.http.port=10001
hive.server2.thrift.http.request.header.size=6144
hive.server2.thrift.http.response.header.size=6144
hive.server2.thrift.http.worker.keepalive.time=60
hive.server2.thrift.login.timeout=20
hive.server2.thrift.max.message.size=104857600
hive.server2.thrift.max.worker.threads=500
hive.server2.thrift.min.worker.threads=5
hive.server2.thrift.port=10000
hive.server2.thrift.resultset.default.fetch.size=1000
hive.server2.thrift.resultset.max.fetch.size=10000
hive.server2.thrift.resultset.serialize.in.tasks=false
hive.server2.thrift.sasl.qop=auth
hive.server2.thrift.worker.keepalive.time=60
hive.server2.transport.mode=binary
hive.server2.use.SSL=false
hive.server2.webui.host=0.0.0.0
hive.server2.webui.max.historic.queries=25
hive.server2.webui.max.threads=50
hive.server2.webui.port=10002
hive.server2.webui.spnego.principal=HTTP/_HOST@EXAMPLE.COM
hive.server2.webui.use.spnego=false
hive.server2.webui.use.ssl=false
hive.server2.xsrf.filter.enabled=false
hive.server2.zookeeper.namespace=hiveserver2
hive.server2.zookeeper.publish.configs=true
hive.service.metrics.class=org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics
hive.service.metrics.file.frequency=5s
hive.service.metrics.file.location=/tmp/report.json
hive.service.metrics.hadoop2.component=hive
hive.service.metrics.hadoop2.frequency=30s
hive.service.metrics.reporter=JSON_FILE, JMX
hive.session.history.enabled=false
hive.session.id=9f1320e2-c2a9-4da1-bcf2-9d28b8fbce4e
hive.session.silent=false
hive.skewjoin.key=100000
hive.skewjoin.mapjoin.map.tasks=10000
hive.skewjoin.mapjoin.min.split=33554432
hive.smbjoin.cache.rows=10000
hive.spark.client.connect.timeout=1000
hive.spark.client.future.timeout=60
hive.spark.client.rpc.max.size=52428800
hive.spark.client.rpc.sasl.mechanisms=DIGEST-MD5
hive.spark.client.rpc.threads=8
hive.spark.client.secret.bits=256
hive.spark.client.server.connect.timeout=90000
hive.spark.dynamic.partition.pruning=false
hive.spark.dynamic.partition.pruning.max.data.size=104857600
hive.spark.exec.inplace.progress=true
hive.spark.job.monitor.timeout=60
hive.spark.use.file.size.for.mapjoin=false
hive.spark.use.groupby.shuffle=true
hive.spark.use.op.stats=true
hive.ssl.protocol.blacklist=SSLv2,SSLv3
hive.stageid.rearrange=none
hive.start.cleanup.scratchdir=false
hive.stats.atomic=false
hive.stats.autogather=true
hive.stats.collect.scancols=false
hive.stats.collect.tablekeys=false
hive.stats.column.autogather=false
hive.stats.dbclass=fs
hive.stats.deserialization.factor=1.0
hive.stats.fetch.column.stats=false
hive.stats.fetch.partition.stats=true
hive.stats.filter.in.factor=1.0
hive.stats.gather.num.threads=10
hive.stats.jdbc.timeout=30
hive.stats.join.factor=1.1
hive.stats.list.num.entries=10
hive.stats.map.num.entries=10
hive.stats.max.variable.length=100
hive.stats.ndv.error=20.0
hive.stats.reliable=false
hive.stats.retries.wait=3000
hive.strict.checks.bucketing=true
hive.strict.checks.cartesian.product=true
hive.strict.checks.large.query=false
hive.strict.checks.type.safety=true
hive.support.concurrency=false
hive.support.quoted.identifiers=column
hive.support.special.characters.tablename=true
hive.test.authz.sstd.hs2.mode=false
hive.test.fail.compaction=false
hive.test.fail.heartbeater=false
hive.test.mode=false
hive.test.mode.prefix=test_
hive.test.mode.samplefreq=32
hive.test.rollbacktxn=false
hive.tez.auto.reducer.parallelism=false
hive.tez.bigtable.minsize.semijoin.reduction=1000000
hive.tez.bloom.filter.factor=2.0
hive.tez.bucket.pruning=false
hive.tez.bucket.pruning.compat=true
hive.tez.container.max.java.heap.fraction=0.8
hive.tez.container.size=-1
hive.tez.cpu.vcores=-1
hive.tez.dynamic.partition.pruning=true
hive.tez.dynamic.partition.pruning.max.data.size=104857600
hive.tez.dynamic.partition.pruning.max.event.size=1048576
hive.tez.dynamic.semijoin.reduction=true
hive.tez.dynamic.semijoin.reduction.threshold=0.5
hive.tez.enable.memory.manager=true
hive.tez.exec.inplace.progress=true
hive.tez.exec.print.summary=false
hive.tez.hs2.user.access=true
hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat
hive.tez.input.generate.consistent.splits=true
hive.tez.log.level=INFO
hive.tez.max.bloom.filter.entries=100000000
hive.tez.max.partition.factor=2.0
hive.tez.min.bloom.filter.entries=1000000
hive.tez.min.partition.factor=0.25
hive.tez.smb.number.waves=0.5
hive.tez.task.scale.memory.reserve-fraction.min=0.3
hive.tez.task.scale.memory.reserve.fraction=-1.0
hive.tez.task.scale.memory.reserve.fraction.max=0.5
hive.timedout.txn.reaper.interval=180s
hive.timedout.txn.reaper.start=100s
hive.transactional.events.mem=10000000
hive.transactional.table.scan=false
hive.transform.escape.input=false
hive.transpose.aggr.join=false
hive.txn.heartbeat.threadpool.size=5
hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager
hive.txn.manager.dump.lock.state.on.acquire.timeout=false
hive.txn.max.open.batch=1000
hive.txn.operational.properties=0
hive.txn.strict.locking.mode=true
hive.txn.timeout=300
hive.typecheck.on.insert=true
hive.udtf.auto.progress=false
hive.unlock.numretries=10
hive.user.install.directory=/user/
hive.variable.substitute=true
hive.variable.substitute.depth=40
hive.vectorized.adaptor.usage.mode=all
hive.vectorized.execution.enabled=false
hive.vectorized.execution.mapjoin.minmax.enabled=false
hive.vectorized.execution.mapjoin.native.enabled=true
hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled=false
hive.vectorized.execution.mapjoin.native.multikey.only.enabled=false
hive.vectorized.execution.mapjoin.overflow.repeated.threshold=-1
hive.vectorized.execution.reduce.enabled=true
hive.vectorized.execution.reduce.groupby.enabled=true
hive.vectorized.execution.reducesink.new.enabled=true
hive.vectorized.groupby.checkinterval=100000
hive.vectorized.groupby.flush.percent=0.1
hive.vectorized.groupby.maxentries=1000000
hive.vectorized.use.row.serde.deserialize=false
hive.vectorized.use.vector.serde.deserialize=true
hive.vectorized.use.vectorized.input.format=true
hive.warehouse.subdir.inherit.perms=true
hive.writeset.reaper.interval=60s
hive.zookeeper.clean.extra.nodes=false
hive.zookeeper.client.port=2181
hive.zookeeper.connection.basesleeptime=1000
hive.zookeeper.connection.max.retries=3
hive.zookeeper.namespace=hive_zookeeper_namespace
hive.zookeeper.session.timeout=1200000
io.bytes.per.checksum=512
io.compression.codec.bzip2.library=system-native
io.erasurecode.codec.rs-legacy.rawcoders=rs-legacy_java
io.erasurecode.codec.rs.rawcoders=rs_native,rs_java
io.erasurecode.codec.xor.rawcoders=xor_native,xor_java
io.file.buffer.size=65536
io.map.index.interval=128
io.map.index.skip=0
io.mapfile.bloom.error.rate=0.005
io.mapfile.bloom.size=1048576
io.seqfile.compress.blocksize=1000000
io.seqfile.local.dir=/tmp/hadoop-saitouyuuki/io/local
io.serializations=org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization
io.skip.checksum.errors=false
ipc.[port_number].backoff.enable=false
ipc.[port_number].callqueue.impl=java.util.concurrent.LinkedBlockingQueue
ipc.[port_number].cost-provider.impl=org.apache.hadoop.ipc.DefaultCostProvider
ipc.[port_number].decay-scheduler.backoff.responsetime.enable=false
ipc.[port_number].decay-scheduler.backoff.responsetime.thresholds=10s,20s,30s,40s
ipc.[port_number].decay-scheduler.decay-factor=0.5
ipc.[port_number].decay-scheduler.metrics.top.user.count=10
ipc.[port_number].decay-scheduler.period-ms=5000
ipc.[port_number].decay-scheduler.thresholds=13,25,50
ipc.[port_number].faircallqueue.multiplexer.weights=8,4,2,1
ipc.[port_number].identity-provider.impl=org.apache.hadoop.ipc.UserIdentityProvider
ipc.[port_number].scheduler.impl=org.apache.hadoop.ipc.DefaultRpcScheduler
ipc.[port_number].scheduler.priority.levels=4
ipc.[port_number].weighted-cost.handler=1
ipc.[port_number].weighted-cost.lockexclusive=100
ipc.[port_number].weighted-cost.lockfree=1
ipc.[port_number].weighted-cost.lockshared=10
ipc.[port_number].weighted-cost.response=1
ipc.client.bind.wildcard.addr=false
ipc.client.connect.max.retries=10
ipc.client.connect.max.retries.on.timeouts=45
ipc.client.connect.retry.interval=1000
ipc.client.connect.timeout=20000
ipc.client.connection.maxidletime=10000
ipc.client.fallback-to-simple-auth-allowed=false
ipc.client.idlethreshold=4000
ipc.client.kill.max=10
ipc.client.low-latency=false
ipc.client.ping=true
ipc.client.rpc-timeout.ms=0
ipc.client.tcpnodelay=true
ipc.maximum.data.length=134217728
ipc.maximum.response.length=134217728
ipc.ping.interval=60000
ipc.server.listen.queue.size=256
ipc.server.log.slow.rpc=false
ipc.server.max.connections=0
ipc.server.reuseaddr=true
javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory
javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver
javax.jdo.option.ConnectionPassword=
javax.jdo.option.ConnectionURL=jdbc:mysql://localhost/metastore?enabledTLSProtocols=TLSv1.2
javax.jdo.option.ConnectionUserName=root
javax.jdo.option.DetachAllOnCommit=true
javax.jdo.option.Multithreaded=true
javax.jdo.option.NonTransactionalRead=true
map.sort.class=org.apache.hadoop.util.QuickSort
mapreduce.am.max-attempts=2
mapreduce.app-submission.cross-platform=false
mapreduce.client.completion.pollinterval=5000
mapreduce.client.libjars.wildcard=true
mapreduce.client.output.filter=FAILED
mapreduce.client.progressmonitor.pollinterval=1000
mapreduce.client.submit.file.replication=10
mapreduce.cluster.acls.enabled=false
mapreduce.cluster.local.dir=/tmp/hadoop-saitouyuuki/mapred/local
mapreduce.fileoutputcommitter.algorithm.version=1
mapreduce.fileoutputcommitter.task.cleanup.enabled=false
mapreduce.framework.name=local
mapreduce.ifile.readahead=true
mapreduce.ifile.readahead.bytes=4194304
mapreduce.input.fileinputformat.list-status.num-threads=1
mapreduce.input.fileinputformat.split.maxsize=256000000
mapreduce.input.fileinputformat.split.minsize=0
mapreduce.input.fileinputformat.split.minsize.per.node=1
mapreduce.input.fileinputformat.split.minsize.per.rack=1
mapreduce.input.lineinputformat.linespermap=1
mapreduce.job.acl-modify-job= 
mapreduce.job.acl-view-job= 
mapreduce.job.cache.limit.max-resources=0
mapreduce.job.cache.limit.max-resources-mb=0
mapreduce.job.cache.limit.max-single-resource-mb=0
mapreduce.job.classloader=false
mapreduce.job.committer.setup.cleanup.needed=true
mapreduce.job.complete.cancel.delegation.tokens=true
mapreduce.job.counters.max=120
mapreduce.job.dfs.storage.capacity.kill-limit-exceed=false
mapreduce.job.emit-timeline-data=false
mapreduce.job.encrypted-intermediate-data=false
mapreduce.job.encrypted-intermediate-data-key-size-bits=128
mapreduce.job.encrypted-intermediate-data.buffer.kb=128
mapreduce.job.end-notification.max.attempts=5
mapreduce.job.end-notification.max.retry.interval=5000
mapreduce.job.end-notification.retry.attempts=0
mapreduce.job.end-notification.retry.interval=1000
mapreduce.job.finish-when-all-reducers-done=true
mapreduce.job.hdfs-servers=file:///
mapreduce.job.heap.memory-mb.ratio=0.8
mapreduce.job.local-fs.single-disk-limit.bytes=-1
mapreduce.job.local-fs.single-disk-limit.check.interval-ms=5000
mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed=true
mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer
mapreduce.job.maps=2
mapreduce.job.max.map=-1
mapreduce.job.max.split.locations=15
mapreduce.job.maxtaskfailures.per.tracker=3
mapreduce.job.queuename=default
mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle
mapreduce.job.reduce.slowstart.completedmaps=0.05
mapreduce.job.reducer.preempt.delay.sec=0
mapreduce.job.reducer.unconditional-preempt.delay.sec=300
mapreduce.job.reduces=1
mapreduce.job.running.map.limit=0
mapreduce.job.running.reduce.limit=0
mapreduce.job.sharedcache.mode=disabled
mapreduce.job.speculative.minimum-allowed-tasks=10
mapreduce.job.speculative.retry-after-no-speculate=1000
mapreduce.job.speculative.retry-after-speculate=15000
mapreduce.job.speculative.slowtaskthreshold=1.0
mapreduce.job.speculative.speculative-cap-running-tasks=0.1
mapreduce.job.speculative.speculative-cap-total-tasks=0.01
mapreduce.job.split.metainfo.maxsize=10000000
mapreduce.job.token.tracking.ids.enabled=false
mapreduce.job.ubertask.enable=false
mapreduce.job.ubertask.maxmaps=9
mapreduce.job.ubertask.maxreduces=1
mapreduce.jobhistory.address=0.0.0.0:10020
mapreduce.jobhistory.admin.acl=*
mapreduce.jobhistory.admin.address=0.0.0.0:10033
mapreduce.jobhistory.always-scan-user-dir=false
mapreduce.jobhistory.cleaner.enable=true
mapreduce.jobhistory.cleaner.interval-ms=86400000
mapreduce.jobhistory.client.thread-count=10
mapreduce.jobhistory.datestring.cache.size=200000
mapreduce.jobhistory.done-dir=/tmp/hadoop-yarn/staging/history/done
mapreduce.jobhistory.http.policy=HTTP_ONLY
mapreduce.jobhistory.intermediate-done-dir=/tmp/hadoop-yarn/staging/history/done_intermediate
mapreduce.jobhistory.intermediate-user-done-dir.permissions=770
mapreduce.jobhistory.jhist.format=binary
mapreduce.jobhistory.joblist.cache.size=20000
mapreduce.jobhistory.jobname.limit=50
mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab
mapreduce.jobhistory.loadedjob.tasks.max=-1
mapreduce.jobhistory.loadedjobs.cache.size=5
mapreduce.jobhistory.max-age-ms=604800000
mapreduce.jobhistory.minicluster.fixed.ports=false
mapreduce.jobhistory.move.interval-ms=180000
mapreduce.jobhistory.move.thread-count=3
mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD
mapreduce.jobhistory.recovery.enable=false
mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService
mapreduce.jobhistory.recovery.store.fs.uri=/tmp/hadoop-saitouyuuki/mapred/history/recoverystore
mapreduce.jobhistory.recovery.store.leveldb.path=
    /tmp/hadoop-saitouyuuki/mapred/history/recoverystore:
mapreduce.jobhistory.webapp.address=0.0.0.0:19888
mapreduce.jobhistory.webapp.https.address=0.0.0.0:19890
mapreduce.jobhistory.webapp.rest-csrf.custom-header=X-XSRF-Header
mapreduce.jobhistory.webapp.rest-csrf.enabled=false
mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
mapreduce.jobhistory.webapp.xfs-filter.xframe-options=SAMEORIGIN
mapreduce.jvm.system-properties-to-log=os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name
mapreduce.map.cpu.vcores=1
mapreduce.map.log.level=INFO
mapreduce.map.maxattempts=4
mapreduce.map.memory.mb=-1
mapreduce.map.output.compress=false
mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
mapreduce.map.skip.maxrecords=0
mapreduce.map.skip.proc-count.auto-incr=true
mapreduce.map.sort.spill.percent=0.80
mapreduce.map.speculative=true
mapreduce.output.fileoutputformat.compress=false
mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
mapreduce.output.fileoutputformat.compress.type=RECORD
mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory
mapreduce.reduce.cpu.vcores=1
mapreduce.reduce.input.buffer.percent=0.0
mapreduce.reduce.log.level=INFO
mapreduce.reduce.markreset.buffer.percent=0.0
mapreduce.reduce.maxattempts=4
mapreduce.reduce.memory.mb=-1
mapreduce.reduce.merge.inmem.threshold=1000
mapreduce.reduce.shuffle.connect.timeout=180000
mapreduce.reduce.shuffle.fetch.retry.enabled=false
mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000
mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000
mapreduce.reduce.shuffle.input.buffer.percent=0.70
mapreduce.reduce.shuffle.memory.limit.percent=0.25
mapreduce.reduce.shuffle.merge.percent=0.66
mapreduce.reduce.shuffle.parallelcopies=5
mapreduce.reduce.shuffle.read.timeout=180000
mapreduce.reduce.shuffle.retry-delay.max.ms=60000
mapreduce.reduce.skip.maxgroups=0
mapreduce.reduce.skip.proc-count.auto-incr=true
mapreduce.reduce.speculative=true
mapreduce.shuffle.connection-keep-alive.enable=false
mapreduce.shuffle.connection-keep-alive.timeout=5
mapreduce.shuffle.listen.queue.size=128
mapreduce.shuffle.max.connections=0
mapreduce.shuffle.max.threads=0
mapreduce.shuffle.pathcache.concurrency-level=
    16:
mapreduce.shuffle.pathcache.expire-after-access-minutes=
    5:
mapreduce.shuffle.pathcache.max-weight=
    10485760:
mapreduce.shuffle.port=13562
mapreduce.shuffle.ssl.enabled=false
mapreduce.shuffle.ssl.file.buffer.size=65536
mapreduce.shuffle.transfer.buffer.size=131072
mapreduce.task.combine.progress.records=10000
mapreduce.task.exit.timeout=60000
mapreduce.task.exit.timeout.check-interval-ms=20000
mapreduce.task.files.preserve.failedtasks=false
mapreduce.task.io.sort.factor=10
mapreduce.task.io.sort.mb=100
mapreduce.task.local-fs.write-limit.bytes=-1
mapreduce.task.merge.progress.records=10000
mapreduce.task.profile=false
mapreduce.task.profile.map.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.maps=0-2
mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.reduce.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.reduces=0-2
mapreduce.task.skip.start.attempts=2
mapreduce.task.stuck.timeout-ms=600000
mapreduce.task.timeout=600000
mapreduce.task.userlog.limit.kb=0
net.topology.impl=org.apache.hadoop.net.NetworkTopology
net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping
net.topology.script.number.args=100
nfs.exports.allowed.hosts=* rw
parquet.memory.pool.ratio=0.5
rpc.metrics.quantile.enable=false
seq.io.sort.factor=100
seq.io.sort.mb=100
spark.app.id=local-1637634553920
spark.app.name=chapter1
spark.app.startTime=1637634553363
spark.driver.host=192.168.0.30
spark.driver.port=64565
spark.eventLog.enabled=true
spark.executor.id=driver
spark.master=local[*]
spark.rdd.compress=True
spark.serializer.objectStreamReset=100
spark.sql.catalogImplementation=hive
spark.sql.session.timeZone=JST
spark.sql.warehouse.dir=file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse
spark.submit.deployMode=client
spark.submit.pyFiles=
spark.ui.enabled=true
spark.ui.showConsoleProgress=true
stream.stderr.reporter.enabled=true
stream.stderr.reporter.prefix=reporter:
tfile.fs.input.buffer.size=262144
tfile.fs.output.buffer.size=262144
tfile.io.chunk.size=1048576
yarn.acl.enable=false
yarn.acl.reservation-enable=false
yarn.admin.acl=*
yarn.am.liveness-monitor.expiry-interval-ms=600000
yarn.app.attempt.diagnostics.limit.kc=64
yarn.app.mapreduce.am.command-opts=-Xmx1024m
yarn.app.mapreduce.am.container.log.backups=0
yarn.app.mapreduce.am.container.log.limit.kb=0
yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10
yarn.app.mapreduce.am.hard-kill-timeout-ms=10000
yarn.app.mapreduce.am.job.committer.cancel-timeout=60000
yarn.app.mapreduce.am.job.committer.commit-window=10000
yarn.app.mapreduce.am.job.task.listener.thread-count=30
yarn.app.mapreduce.am.log.level=INFO
yarn.app.mapreduce.am.resource.cpu-vcores=1
yarn.app.mapreduce.am.resource.mb=1536
yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000
yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging
yarn.app.mapreduce.am.staging-dir.erasurecoding.enabled=false
yarn.app.mapreduce.am.webapp.https.client.auth=false
yarn.app.mapreduce.am.webapp.https.enabled=false
yarn.app.mapreduce.client-am.ipc.max-retries=3
yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3
yarn.app.mapreduce.client.job.max-retries=3
yarn.app.mapreduce.client.job.retry-interval=2000
yarn.app.mapreduce.client.max-retries=3
yarn.app.mapreduce.shuffle.log.backups=0
yarn.app.mapreduce.shuffle.log.limit.kb=0
yarn.app.mapreduce.shuffle.log.separate=true
yarn.app.mapreduce.task.container.log.backups=0
yarn.bin.path=
    yarn:
yarn.client.application-client-protocol.poll-interval-ms=200
yarn.client.application-client-protocol.poll-timeout-ms=-1
yarn.client.failover-no-ha-proxy-provider=org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider
yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider
yarn.client.failover-retries=0
yarn.client.failover-retries-on-socket-timeouts=0
yarn.client.load.resource-types.from-server=false
yarn.client.max-cached-nodemanagers-proxies=0
yarn.client.nodemanager-client-async.thread-pool-max-size=500
yarn.client.nodemanager-connect.max-wait-ms=180000
yarn.client.nodemanager-connect.retry-interval-ms=10000
yarn.cluster.max-application-priority=0
yarn.dispatcher.cpu-monitor.samples-per-min=60
yarn.dispatcher.drain-events.timeout=300000
yarn.dispatcher.print-events-info.threshold=5000
yarn.fail-fast=false
yarn.federation.cache-ttl.secs=300
yarn.federation.enabled=false
yarn.federation.registry.base-dir=yarnfederation/
yarn.federation.state-store.class=org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore
yarn.federation.subcluster-resolver.class=org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl
yarn.http.policy=HTTP_ONLY
yarn.intermediate-data-encryption.enable=false
yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
yarn.is.minicluster=false
yarn.log-aggregation-enable=false
yarn.log-aggregation-status.time-out.ms=600000
yarn.log-aggregation.debug.filesize=104857600
yarn.log-aggregation.file-controller.TFile.class=org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController
yarn.log-aggregation.file-formats=TFile
yarn.log-aggregation.retain-check-interval-seconds=-1
yarn.log-aggregation.retain-seconds=-1
yarn.minicluster.control-resource-monitoring=false
yarn.minicluster.fixed.ports=false
yarn.minicluster.use-rpc=false
yarn.minicluster.yarn.nodemanager.resource.memory-mb=4096
yarn.nm.liveness-monitor.expiry-interval-ms=600000
yarn.node-attribute.fs-store.impl.class=org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore
yarn.node-labels.configuration-type=centralized
yarn.node-labels.enabled=false
yarn.node-labels.fs-store.impl.class=org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore
yarn.nodemanager.address=0.0.0.0:0
yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX
yarn.nodemanager.amrmproxy.address=0.0.0.0:8049
yarn.nodemanager.amrmproxy.client.thread-count=25
yarn.nodemanager.amrmproxy.enabled=false
yarn.nodemanager.amrmproxy.ha.enable=false
yarn.nodemanager.amrmproxy.interceptor-class.pipeline=org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor
yarn.nodemanager.aux-services.manifest.enabled=false
yarn.nodemanager.aux-services.manifest.reload-ms=0
yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
yarn.nodemanager.collector-service.address=0.0.0.0:8048
yarn.nodemanager.collector-service.thread-count=5
yarn.nodemanager.container-diagnostics-maximum-size=10000
yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor
yarn.nodemanager.container-executor.exit-code-file.timeout-ms=2000
yarn.nodemanager.container-localizer.java.opts=-Xmx256m
yarn.nodemanager.container-localizer.log.level=INFO
yarn.nodemanager.container-log-monitor.dir-size-limit-bytes=1000000000
yarn.nodemanager.container-log-monitor.enable=false
yarn.nodemanager.container-log-monitor.interval-ms=60000
yarn.nodemanager.container-log-monitor.total-size-limit-bytes=10000000000
yarn.nodemanager.container-manager.thread-count=20
yarn.nodemanager.container-metrics.enable=true
yarn.nodemanager.container-metrics.period-ms=-1
yarn.nodemanager.container-metrics.unregister-delay-ms=10000
yarn.nodemanager.container-monitor.enabled=true
yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false
yarn.nodemanager.container-retry-minimum-interval-ms=1000
yarn.nodemanager.container.stderr.pattern={*stderr*,*STDERR*}
yarn.nodemanager.container.stderr.tail.bytes=4096
yarn.nodemanager.containers-launcher.class=org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
yarn.nodemanager.default-container-executor.log-dirs.permissions=710
yarn.nodemanager.delete.debug-delay-sec=0
yarn.nodemanager.delete.thread-count=4
yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled=true
yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled=true
yarn.nodemanager.disk-health-checker.enable=true
yarn.nodemanager.disk-health-checker.interval-ms=120000
yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0
yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0
yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb=0
yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25
yarn.nodemanager.disk-validator=basic
yarn.nodemanager.distributed-scheduling.enabled=false
yarn.nodemanager.elastic-memory-control.enabled=false
yarn.nodemanager.elastic-memory-control.oom-handler=org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler
yarn.nodemanager.elastic-memory-control.timeout-sec=5
yarn.nodemanager.emit-container-events=true
yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ
yarn.nodemanager.health-checker.interval-ms=600000
yarn.nodemanager.health-checker.run-before-startup=false
yarn.nodemanager.health-checker.scripts=script
yarn.nodemanager.health-checker.timeout-ms=1200000
yarn.nodemanager.hostname=0.0.0.0
yarn.nodemanager.keytab=/etc/krb5.keytab
yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms=20
yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms=1000
yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn
yarn.nodemanager.linux-container-executor.cgroups.mount=false
yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false
yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true
yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody
yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$
yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler
yarn.nodemanager.local-cache.max-files-per-directory=8192
yarn.nodemanager.local-dirs=/tmp/hadoop-saitouyuuki/nm-local-dir
yarn.nodemanager.localizer.address=0.0.0.0:8040
yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000
yarn.nodemanager.localizer.cache.target-size-mb=10240
yarn.nodemanager.localizer.client.thread-count=5
yarn.nodemanager.localizer.fetch.thread-count=4
yarn.nodemanager.log-aggregation.compression-type=none
yarn.nodemanager.log-aggregation.num-log-files-per-app=30
yarn.nodemanager.log-aggregation.policy.class=org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy
yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1
yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min=3600
yarn.nodemanager.log-container-debug-info.enabled=true
yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs
yarn.nodemanager.log.deletion-threads-count=4
yarn.nodemanager.log.retain-seconds=10800
yarn.nodemanager.logaggregation.threadpool-size-max=100
yarn.nodemanager.node-attributes.provider.fetch-interval-ms=600000
yarn.nodemanager.node-attributes.provider.fetch-timeout-ms=1200000
yarn.nodemanager.node-attributes.resync-interval-ms=120000
yarn.nodemanager.node-labels.provider.fetch-interval-ms=600000
yarn.nodemanager.node-labels.provider.fetch-timeout-ms=1200000
yarn.nodemanager.node-labels.resync-interval-ms=120000
yarn.nodemanager.numa-awareness.enabled=false
yarn.nodemanager.numa-awareness.numactl.cmd=/usr/bin/numactl
yarn.nodemanager.numa-awareness.read-topology=false
yarn.nodemanager.opportunistic-containers-max-queue-length=0
yarn.nodemanager.opportunistic-containers-use-pause-for-preemption=false
yarn.nodemanager.pluggable-device-framework.enabled=false
yarn.nodemanager.pmem-check-enabled=true
yarn.nodemanager.process-kill-wait.ms=5000
yarn.nodemanager.recovery.compaction-interval-secs=3600
yarn.nodemanager.recovery.dir=/tmp/hadoop-saitouyuuki/yarn-nm-recovery
yarn.nodemanager.recovery.enabled=false
yarn.nodemanager.recovery.supervised=false
yarn.nodemanager.remote-app-log-dir=/tmp/logs
yarn.nodemanager.remote-app-log-dir-include-older=true
yarn.nodemanager.remote-app-log-dir-suffix=logs
yarn.nodemanager.resource-monitor.interval-ms=3000
yarn.nodemanager.resource-plugins.fpga.allowed-fpga-devices=auto
yarn.nodemanager.resource-plugins.fpga.vendor-plugin.class=org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin
yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices=auto
yarn.nodemanager.resource-plugins.gpu.docker-plugin=nvidia-docker-v1
yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidia-docker-v1.endpoint=http://localhost:3476/v1.0/docker/cli
yarn.nodemanager.resource.count-logical-processors-as-cores=false
yarn.nodemanager.resource.cpu-vcores=-1
yarn.nodemanager.resource.detect-hardware-capabilities=false
yarn.nodemanager.resource.memory-mb=-1
yarn.nodemanager.resource.memory.cgroups.soft-limit-percentage=90.0
yarn.nodemanager.resource.memory.cgroups.swappiness=0
yarn.nodemanager.resource.memory.enabled=false
yarn.nodemanager.resource.memory.enforced=true
yarn.nodemanager.resource.pcores-vcores-multiplier=1.0
yarn.nodemanager.resource.percentage-physical-cpu-limit=100
yarn.nodemanager.resource.system-reserved-memory-mb=-1
yarn.nodemanager.resourcemanager.minimum.version=NONE
yarn.nodemanager.runtime.linux.allowed-runtimes=default
yarn.nodemanager.runtime.linux.docker.allowed-container-networks=host,none,bridge
yarn.nodemanager.runtime.linux.docker.allowed-container-runtimes=runc
yarn.nodemanager.runtime.linux.docker.capabilities=CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE
yarn.nodemanager.runtime.linux.docker.default-container-network=host
yarn.nodemanager.runtime.linux.docker.delayed-removal.allowed=false
yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed=true
yarn.nodemanager.runtime.linux.docker.host-pid-namespace.allowed=false
yarn.nodemanager.runtime.linux.docker.image-update=false
yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed=false
yarn.nodemanager.runtime.linux.docker.stop.grace-period=10
yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold=1
yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold=1
yarn.nodemanager.runtime.linux.runc.allowed-container-networks=host,none,bridge
yarn.nodemanager.runtime.linux.runc.allowed-container-runtimes=runc
yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-size=500
yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-timeout-interval-secs=360
yarn.nodemanager.runtime.linux.runc.host-pid-namespace.allowed=false
yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin=org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin
yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.cache-refresh-interval-secs=60
yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.hdfs-hash-file=/runc-root/image-tag-to-hash
yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.num-manifests-to-cache=10
yarn.nodemanager.runtime.linux.runc.image-toplevel-dir=/runc-root
yarn.nodemanager.runtime.linux.runc.layer-mounts-interval-secs=600
yarn.nodemanager.runtime.linux.runc.layer-mounts-to-keep=100
yarn.nodemanager.runtime.linux.runc.manifest-to-resources-plugin=org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin
yarn.nodemanager.runtime.linux.runc.privileged-containers.allowed=false
yarn.nodemanager.runtime.linux.sandbox-mode=disabled
yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions=read
yarn.nodemanager.sleep-delay-before-sigkill.ms=250
yarn.nodemanager.vmem-check-enabled=true
yarn.nodemanager.vmem-pmem-ratio=2.1
yarn.nodemanager.webapp.address=0.0.0.0:8042
yarn.nodemanager.webapp.cross-origin.enabled=false
yarn.nodemanager.webapp.https.address=0.0.0.0:8044
yarn.nodemanager.webapp.rest-csrf.custom-header=X-XSRF-Header
yarn.nodemanager.webapp.rest-csrf.enabled=false
yarn.nodemanager.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
yarn.nodemanager.webapp.xfs-filter.xframe-options=SAMEORIGIN
yarn.nodemanager.windows-container.cpu-limit.enabled=false
yarn.nodemanager.windows-container.memory-limit.enabled=false
yarn.registry.class=org.apache.hadoop.registry.client.impl.FSRegistryOperationsService
yarn.resourcemanager.activities-manager.app-activities.max-queue-length=100
yarn.resourcemanager.activities-manager.app-activities.ttl-ms=600000
yarn.resourcemanager.activities-manager.cleanup-interval-ms=5000
yarn.resourcemanager.activities-manager.scheduler-activities.ttl-ms=600000
yarn.resourcemanager.address=0.0.0.0:8032
yarn.resourcemanager.admin.address=0.0.0.0:8033
yarn.resourcemanager.admin.client.thread-count=1
yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400
yarn.resourcemanager.am.max-attempts=2
yarn.resourcemanager.amlauncher.thread-count=50
yarn.resourcemanager.application-https.policy=NONE
yarn.resourcemanager.application-tag-based-placement.enable=false
yarn.resourcemanager.application-timeouts.monitor.interval-ms=3000
yarn.resourcemanager.application.max-tag.length=100
yarn.resourcemanager.application.max-tags=10
yarn.resourcemanager.auto-update.containers=false
yarn.resourcemanager.client.thread-count=50
yarn.resourcemanager.configuration.file-system-based-store=/yarn/conf
yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider
yarn.resourcemanager.connect.max-wait.ms=900000
yarn.resourcemanager.connect.retry-interval.ms=30000
yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400
yarn.resourcemanager.container.liveness-monitor.interval-ms=600000
yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs=20
yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000
yarn.resourcemanager.delegation-token-renewer.thread-count=50
yarn.resourcemanager.delegation-token-renewer.thread-retry-interval=60s
yarn.resourcemanager.delegation-token-renewer.thread-retry-max-attempts=10
yarn.resourcemanager.delegation-token-renewer.thread-timeout=60s
yarn.resourcemanager.delegation-token.always-cancel=false
yarn.resourcemanager.delegation-token.max-conf-size-bytes=12800
yarn.resourcemanager.delegation.key.update-interval=86400000
yarn.resourcemanager.delegation.token.max-lifetime=604800000
yarn.resourcemanager.delegation.token.renew-interval=86400000
yarn.resourcemanager.epoch.range=0
yarn.resourcemanager.fail-fast=false
yarn.resourcemanager.fs.state-store.num-retries=0
yarn.resourcemanager.fs.state-store.retry-interval-ms=1000
yarn.resourcemanager.fs.state-store.uri=/tmp/hadoop-saitouyuuki/yarn/system/rmstore
yarn.resourcemanager.ha.automatic-failover.embedded=true
yarn.resourcemanager.ha.automatic-failover.enabled=true
yarn.resourcemanager.ha.automatic-failover.zk-base-path=
    /yarn-leader-election:
yarn.resourcemanager.ha.enabled=false
yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size=10
yarn.resourcemanager.hostname=0.0.0.0
yarn.resourcemanager.keytab=/etc/krb5.keytab
yarn.resourcemanager.leveldb-state-store.compaction-interval-secs=3600
yarn.resourcemanager.leveldb-state-store.path=
    /tmp/hadoop-saitouyuuki/yarn/system/rmstore:
yarn.resourcemanager.max-completed-applications=1000
yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10
yarn.resourcemanager.metrics.runtime.buckets=60,300,1440
yarn.resourcemanager.nm-container-queuing.load-comparator=QUEUE_LENGTH
yarn.resourcemanager.nm-container-queuing.max-queue-length=15
yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms=100
yarn.resourcemanager.nm-container-queuing.min-queue-length=5
yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms=10
yarn.resourcemanager.nm-container-queuing.queue-limit-stdev=1.0f
yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms=1000
yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs=86400
yarn.resourcemanager.node-ip-cache.expiry-interval-secs=-1
yarn.resourcemanager.node-labels.provider.fetch-interval-ms=1800000
yarn.resourcemanager.node-removal-untracked.timeout-ms=60000
yarn.resourcemanager.nodemanager-connect-retries=10
yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs=3600
yarn.resourcemanager.nodemanager.minimum.version=NONE
yarn.resourcemanager.nodemanagers.heartbeat-interval-max-ms=1000
yarn.resourcemanager.nodemanagers.heartbeat-interval-min-ms=1000
yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000
yarn.resourcemanager.nodemanagers.heartbeat-interval-scaling-enable=false
yarn.resourcemanager.nodemanagers.heartbeat-interval-slowdown-factor=1.0
yarn.resourcemanager.nodemanagers.heartbeat-interval-speedup-factor=1.0
yarn.resourcemanager.opportunistic-container-allocation.enabled=false
yarn.resourcemanager.opportunistic-container-allocation.nodes-used=10
yarn.resourcemanager.opportunistic.max.container-allocation.per.am.heartbeat=-1
yarn.resourcemanager.placement-constraints.algorithm.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm
yarn.resourcemanager.placement-constraints.algorithm.iterator=SERIAL
yarn.resourcemanager.placement-constraints.algorithm.pool-size=1
yarn.resourcemanager.placement-constraints.handler=disabled
yarn.resourcemanager.placement-constraints.retry-attempts=3
yarn.resourcemanager.placement-constraints.scheduler.pool-size=1
yarn.resourcemanager.proxy-user-privileges.enabled=false
yarn.resourcemanager.recovery.enabled=false
yarn.resourcemanager.reservation-system.enable=false
yarn.resourcemanager.reservation-system.planfollower.time-step=1000
yarn.resourcemanager.resource-profiles.enabled=false
yarn.resourcemanager.resource-profiles.source-file=resource-profiles.json
yarn.resourcemanager.resource-tracker.address=0.0.0.0:8031
yarn.resourcemanager.resource-tracker.client.thread-count=50
yarn.resourcemanager.resource-tracker.nm.ip-hostname-check=false
yarn.resourcemanager.rm.container-allocation.expiry-interval-ms=600000
yarn.resourcemanager.scheduler.address=0.0.0.0:8030
yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
yarn.resourcemanager.scheduler.client.thread-count=50
yarn.resourcemanager.scheduler.monitor.enable=false
yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
yarn.resourcemanager.state-store.max-completed-applications=1000
yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
yarn.resourcemanager.submission-preprocessor.enabled=false
yarn.resourcemanager.submission-preprocessor.file-refresh-interval-ms=60000
yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10
yarn.resourcemanager.system-metrics-publisher.enabled=false
yarn.resourcemanager.webapp.address=0.0.0.0:8088
yarn.resourcemanager.webapp.cross-origin.enabled=false
yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true
yarn.resourcemanager.webapp.https.address=0.0.0.0:8090
yarn.resourcemanager.webapp.rest-csrf.custom-header=X-XSRF-Header
yarn.resourcemanager.webapp.rest-csrf.enabled=false
yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
yarn.resourcemanager.webapp.ui-actions.enabled=true
yarn.resourcemanager.webapp.xfs-filter.xframe-options=SAMEORIGIN
yarn.resourcemanager.work-preserving-recovery.enabled=true
yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000
yarn.resourcemanager.zk-appid-node.split-index=0
yarn.resourcemanager.zk-delegation-token-node.split-index=0
yarn.resourcemanager.zk-max-znode-size.bytes=1048576
yarn.resourcemanager.zk-state-store.parent-path=
    /rmstore:
yarn.rm.system-metrics-publisher.emit-container-events=false
yarn.router.clientrm.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor
yarn.router.interceptor.user.threadpool-size=5
yarn.router.pipeline.cache-max-size=25
yarn.router.rmadmin.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor
yarn.router.webapp.address=0.0.0.0:8089
yarn.router.webapp.https.address=0.0.0.0:8091
yarn.router.webapp.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST
yarn.scheduler.configuration.fs.path=
    file:
    ///tmp/hadoop-saitouyuuki/yarn/system/schedconf:
yarn.scheduler.configuration.leveldb-store.compaction-interval-secs=86400
yarn.scheduler.configuration.leveldb-store.path=
    /tmp/hadoop-saitouyuuki/yarn/system/confstore:
yarn.scheduler.configuration.max.version=100
yarn.scheduler.configuration.mutation.acl-policy.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.DefaultConfigurationMutationACLPolicy
yarn.scheduler.configuration.store.class=file
yarn.scheduler.configuration.store.max-logs=1000
yarn.scheduler.configuration.zk-store.parent-path=
    /confstore:
yarn.scheduler.include-port-in-node-name=false
yarn.scheduler.maximum-allocation-mb=8192
yarn.scheduler.maximum-allocation-vcores=4
yarn.scheduler.minimum-allocation-mb=1024
yarn.scheduler.minimum-allocation-vcores=1
yarn.scheduler.queue-placement-rules=user-group
yarn.sharedcache.admin.address=0.0.0.0:8047
yarn.sharedcache.admin.thread-count=1
yarn.sharedcache.app-checker.class=org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker
yarn.sharedcache.checksum.algo.impl=org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl
yarn.sharedcache.cleaner.initial-delay-mins=10
yarn.sharedcache.cleaner.period-mins=1440
yarn.sharedcache.cleaner.resource-sleep-ms=0
yarn.sharedcache.client-server.address=0.0.0.0:8045
yarn.sharedcache.client-server.thread-count=50
yarn.sharedcache.enabled=false
yarn.sharedcache.nested-level=3
yarn.sharedcache.nm.uploader.replication.factor=10
yarn.sharedcache.nm.uploader.thread-count=20
yarn.sharedcache.root-dir=/sharedcache
yarn.sharedcache.store.class=org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore
yarn.sharedcache.store.in-memory.check-period-mins=720
yarn.sharedcache.store.in-memory.initial-delay-mins=10
yarn.sharedcache.store.in-memory.staleness-period-mins=10080
yarn.sharedcache.uploader.server.address=0.0.0.0:8046
yarn.sharedcache.uploader.server.thread-count=50
yarn.sharedcache.webapp.address=0.0.0.0:8788
yarn.system-metrics-publisher.enabled=false
yarn.timeline-service.address=0.0.0.0:10200
yarn.timeline-service.app-aggregation-interval-secs=15
yarn.timeline-service.app-collector.linger-period.ms=60000
yarn.timeline-service.client.best-effort=false
yarn.timeline-service.client.drain-entities.timeout.ms=2000
yarn.timeline-service.client.fd-clean-interval-secs=60
yarn.timeline-service.client.fd-flush-interval-secs=10
yarn.timeline-service.client.fd-retain-secs=300
yarn.timeline-service.client.internal-timers-ttl-secs=420
yarn.timeline-service.client.max-retries=30
yarn.timeline-service.client.retry-interval-ms=1000
yarn.timeline-service.enabled=false
yarn.timeline-service.entity-group-fs-store.active-dir=/tmp/entity-file-history/active
yarn.timeline-service.entity-group-fs-store.app-cache-size=10
yarn.timeline-service.entity-group-fs-store.cache-store-class=org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore
yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds=3600
yarn.timeline-service.entity-group-fs-store.done-dir=/tmp/entity-file-history/done/
yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size=10485760
yarn.timeline-service.entity-group-fs-store.retain-seconds=604800
yarn.timeline-service.entity-group-fs-store.scan-interval-seconds=60
yarn.timeline-service.entity-group-fs-store.summary-store=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
yarn.timeline-service.entity-group-fs-store.with-user-dir=false
yarn.timeline-service.flowname.max-size=0
yarn.timeline-service.generic-application-history.max-applications=10000
yarn.timeline-service.handler-thread-count=10
yarn.timeline-service.hbase-schema.prefix=prod.
yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds=259200000
yarn.timeline-service.hbase.coprocessor.jar.hdfs.location=/hbase/coprocessor/hadoop-yarn-server-timelineservice.jar
yarn.timeline-service.hostname=0.0.0.0
yarn.timeline-service.http-authentication.simple.anonymous.allowed=true
yarn.timeline-service.http-authentication.type=simple
yarn.timeline-service.http-cross-origin.enabled=false
yarn.timeline-service.keytab=/etc/krb5.keytab
yarn.timeline-service.leveldb-state-store.path=
    /tmp/hadoop-saitouyuuki/yarn/timeline:
yarn.timeline-service.leveldb-timeline-store.path=
    /tmp/hadoop-saitouyuuki/yarn/timeline:
yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600
yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000
yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000
yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000
yarn.timeline-service.reader.class=org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl
yarn.timeline-service.reader.webapp.address=0.0.0.0:8188
yarn.timeline-service.reader.webapp.https.address=0.0.0.0:8190
yarn.timeline-service.recovery.enabled=false
yarn.timeline-service.state-store-class=org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore
yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
yarn.timeline-service.timeline-client.number-of-async-entities-to-merge=10
yarn.timeline-service.ttl-enable=true
yarn.timeline-service.ttl-ms=604800000
yarn.timeline-service.version=1.0f
yarn.timeline-service.webapp.address=0.0.0.0:8188
yarn.timeline-service.webapp.https.address=0.0.0.0:8190
yarn.timeline-service.webapp.rest-csrf.custom-header=X-XSRF-Header
yarn.timeline-service.webapp.rest-csrf.enabled=false
yarn.timeline-service.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
yarn.timeline-service.webapp.xfs-filter.xframe-options=SAMEORIGIN
yarn.timeline-service.writer.async.queue.capacity=100
yarn.timeline-service.writer.class=org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl
yarn.timeline-service.writer.flush-interval-seconds=60
yarn.webapp.api-service.enable=false
yarn.webapp.enable-rest-app-submissions=true
yarn.webapp.filter-entity-list-by-user=false
yarn.webapp.filter-invalid-xml-chars=false
yarn.webapp.ui2.enable=false
yarn.webapp.xfs-filter.enabled=true
yarn.workflow-id.tag-prefix=workflowid:
END========"new HiveConf()"========

21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: create_database: Database(name:metadata_tmp, description:, locationUri:file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse/metadata_tmp.db, parameters:{}, ownerName:saitouyuuki)
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=create_database: Database(name:metadata_tmp, description:, locationUri:file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse/metadata_tmp.db, parameters:{}, ownerName:saitouyuuki)	
21/11/23 11:29:18 @WARN @ObjectStore@ Failed to get database metadata_tmp, returning NoSuchObjectException
21/11/23 11:29:18 @INFO @FileUtils@ Creating directory if it doesn't exist: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse/metadata_tmp.db
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:18 @INFO @SQLStdHiveAccessController@ Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=9f1320e2-c2a9-4da1-bcf2-9d28b8fbce4e, clientType=HIVECLI]
21/11/23 11:29:18 @WARN @SessionState@ METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
21/11/23 11:29:18 @INFO @metastore@ Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: Cleaning up thread local RawStore...
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: Done cleaning up thread local RawStore
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: create_table: Table(tableName:sample_metadata, dbName:metadata_tmp, owner:saitouyuuki, createTime:1637634558, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:database_name, type:string, comment:null), FieldSchema(name:table_name, type:string, comment:null), FieldSchema(name:table_definition, type:string, comment:null), FieldSchema(name:sammary, type:string, comment:null), FieldSchema(name:record_num, type:string, comment:null), FieldSchema(name:selectivity, type:string, comment:null), FieldSchema(name:consistency_flag, type:boolean, comment:null), FieldSchema(name:frequency_access, type:string, comment:null)], location:file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema={"type":"struct","fields":[{"name":"database_name","type":"string","nullable":true,"metadata":{}},{"name":"table_name","type":"string","nullable":true,"metadata":{}},{"name":"table_definition","type":"string","nullable":true,"metadata":{}},{"name":"sammary","type":"string","nullable":true,"metadata":{}},{"name":"record_num","type":"string","nullable":true,"metadata":{}},{"name":"selectivity","type":"string","nullable":true,"metadata":{}},{"name":"consistency_flag","type":"boolean","nullable":true,"metadata":{}},{"name":"frequency_access","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{saitouyuuki=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
21/11/23 11:29:18 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=create_table: Table(tableName:sample_metadata, dbName:metadata_tmp, owner:saitouyuuki, createTime:1637634558, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:database_name, type:string, comment:null), FieldSchema(name:table_name, type:string, comment:null), FieldSchema(name:table_definition, type:string, comment:null), FieldSchema(name:sammary, type:string, comment:null), FieldSchema(name:record_num, type:string, comment:null), FieldSchema(name:selectivity, type:string, comment:null), FieldSchema(name:consistency_flag, type:boolean, comment:null), FieldSchema(name:frequency_access, type:string, comment:null)], location:file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=TRUE, spark.sql.sources.schema={"type":"struct","fields":[{"name":"database_name","type":"string","nullable":true,"metadata":{}},{"name":"table_name","type":"string","nullable":true,"metadata":{}},{"name":"table_definition","type":"string","nullable":true,"metadata":{}},{"name":"sammary","type":"string","nullable":true,"metadata":{}},{"name":"record_num","type":"string","nullable":true,"metadata":{}},{"name":"selectivity","type":"string","nullable":true,"metadata":{}},{"name":"consistency_flag","type":"boolean","nullable":true,"metadata":{}},{"name":"frequency_access","type":"string","nullable":true,"metadata":{}}]}, spark.sql.create.version=3.2.0}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{saitouyuuki=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:saitouyuuki, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))	
21/11/23 11:29:18 @WARN @HiveConf@ HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
21/11/23 11:29:18 @WARN @HiveConf@ HiveConf of name hive.stats.jdbc.timeout does not exist
21/11/23 11:29:18 @WARN @HiveConf@ HiveConf of name hive.stats.retries.wait does not exist
21/11/23 11:29:18 @INFO @HiveMetaStore@ 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/11/23 11:29:18 @INFO @ObjectStore@ ObjectStore, initialize called
21/11/23 11:29:18 @INFO @MetaStoreDirectSql@ Using direct SQL, underlying DB is MYSQL
21/11/23 11:29:18 @INFO @ObjectStore@ Initialized ObjectStore
21/11/23 11:29:18 @INFO @log@ Updating table stats fast for sample_metadata
21/11/23 11:29:18 @INFO @log@ Updated size of table sample_metadata to 5134
21/11/23 11:29:20 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:20 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:20 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:20 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:20 @INFO @HiveMetaStore@ 0: get_tables: db=metadata_tmp pat=*
21/11/23 11:29:20 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_tables: db=metadata_tmp pat=*	
21/11/23 11:29:20 @INFO @CodeGenerator@ Code generated in 97.104166 ms
21/11/23 11:29:20 @INFO @CodeGenerator@ Code generated in 10.245334 ms
21/11/23 11:29:20 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:29:20 @INFO @DAGScheduler@ Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:29:20 @INFO @DAGScheduler@ Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:29:20 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:29:20 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:29:20 @INFO @DAGScheduler@ Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:29:20 @INFO @MemoryStore@ Block broadcast_0 stored as values in memory (estimated size 7.5 KiB, free 434.4 MiB)
21/11/23 11:29:20 @INFO @MemoryStore@ Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.4 MiB)
21/11/23 11:29:20 @INFO @BlockManagerInfo@ Added broadcast_0_piece0 in memory on 192.168.0.30:64566 (size: 3.8 KiB, free: 434.4 MiB)
21/11/23 11:29:20 @INFO @SparkContext@ Created broadcast 0 from broadcast at DAGScheduler.scala:1427
21/11/23 11:29:20 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:29:20 @INFO @TaskSchedulerImpl@ Adding task set 0.0 with 1 tasks resource profile 0
21/11/23 11:29:20 @INFO @TaskSetManager@ Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4673 bytes) taskResourceAssignments Map()
21/11/23 11:29:20 @INFO @Executor@ Running task 0.0 in stage 0.0 (TID 0)
21/11/23 11:29:20 @INFO @Executor@ Finished task 0.0 in stage 0.0 (TID 0). 1418 bytes result sent to driver
21/11/23 11:29:20 @INFO @TaskSetManager@ Finished task 0.0 in stage 0.0 (TID 0) in 78 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:29:20 @INFO @TaskSchedulerImpl@ Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/11/23 11:29:20 @INFO @DAGScheduler@ ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 0.185 s
21/11/23 11:29:20 @INFO @DAGScheduler@ Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:29:20 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 0: Stage finished
21/11/23 11:29:20 @INFO @DAGScheduler@ Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 0.209250 s
21/11/23 11:29:20 @INFO @CodeGenerator@ Code generated in 5.561959 ms
21/11/23 11:29:30 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:29:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:29:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:29:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:29:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:29:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:29:30 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:29:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:29:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:29:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:29:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:29:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:29:30 @INFO @CodeGenerator@ Code generated in 3.521542 ms
21/11/23 11:29:30 @INFO @CodeGenerator@ Code generated in 8.806791 ms
21/11/23 11:29:30 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:29:30 @INFO @DAGScheduler@ Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:29:30 @INFO @DAGScheduler@ Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:29:30 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:29:30 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:29:30 @INFO @DAGScheduler@ Submitting ResultStage 1 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:29:31 @INFO @MemoryStore@ Block broadcast_1 stored as values in memory (estimated size 15.7 KiB, free 434.4 MiB)
21/11/23 11:29:31 @INFO @MemoryStore@ Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
21/11/23 11:29:31 @INFO @BlockManagerInfo@ Added broadcast_1_piece0 in memory on 192.168.0.30:64566 (size: 7.7 KiB, free: 434.4 MiB)
21/11/23 11:29:31 @INFO @SparkContext@ Created broadcast 1 from broadcast at DAGScheduler.scala:1427
21/11/23 11:29:31 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:29:31 @INFO @TaskSchedulerImpl@ Adding task set 1.0 with 1 tasks resource profile 0
21/11/23 11:29:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:31 @INFO @Executor@ Running task 0.0 in stage 1.0 (TID 1)
21/11/23 11:29:31 @INFO @PythonRunner@ Times: total = 326, boot = 321, init = 5, finish = 0
21/11/23 11:29:31 @INFO @Executor@ Finished task 0.0 in stage 1.0 (TID 1). 1781 bytes result sent to driver
21/11/23 11:29:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 1.0 (TID 1) in 353 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:29:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/11/23 11:29:31 @INFO @PythonAccumulatorV2@ Connected to AccumulatorServer at host: 127.0.0.1 port: 64569
21/11/23 11:29:31 @INFO @DAGScheduler@ ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.367 s
21/11/23 11:29:31 @INFO @DAGScheduler@ Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:29:31 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 1: Stage finished
21/11/23 11:29:31 @INFO @DAGScheduler@ Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.370486 s
21/11/23 11:29:31 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:29:31 @INFO @DAGScheduler@ Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions
21/11/23 11:29:31 @INFO @DAGScheduler@ Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:29:31 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:29:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:29:31 @INFO @DAGScheduler@ Submitting ResultStage 2 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:29:31 @INFO @MemoryStore@ Block broadcast_2 stored as values in memory (estimated size 15.7 KiB, free 434.4 MiB)
21/11/23 11:29:31 @INFO @MemoryStore@ Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.3 MiB)
21/11/23 11:29:31 @INFO @BlockManagerInfo@ Added broadcast_2_piece0 in memory on 192.168.0.30:64566 (size: 7.7 KiB, free: 434.4 MiB)
21/11/23 11:29:31 @INFO @SparkContext@ Created broadcast 2 from broadcast at DAGScheduler.scala:1427
21/11/23 11:29:31 @INFO @DAGScheduler@ Submitting 4 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
21/11/23 11:29:31 @INFO @TaskSchedulerImpl@ Adding task set 2.0 with 4 tasks resource profile 0
21/11/23 11:29:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:31 @INFO @TaskSetManager@ Starting task 1.0 in stage 2.0 (TID 3) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:31 @INFO @TaskSetManager@ Starting task 2.0 in stage 2.0 (TID 4) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:31 @INFO @TaskSetManager@ Starting task 3.0 in stage 2.0 (TID 5) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:31 @INFO @Executor@ Running task 0.0 in stage 2.0 (TID 2)
21/11/23 11:29:31 @INFO @Executor@ Running task 1.0 in stage 2.0 (TID 3)
21/11/23 11:29:31 @INFO @Executor@ Running task 2.0 in stage 2.0 (TID 4)
21/11/23 11:29:31 @INFO @Executor@ Running task 3.0 in stage 2.0 (TID 5)
21/11/23 11:29:31 @INFO @PythonRunner@ Times: total = 1, boot = -25, init = 26, finish = 0
21/11/23 11:29:31 @INFO @Executor@ Finished task 3.0 in stage 2.0 (TID 5). 1781 bytes result sent to driver
21/11/23 11:29:31 @INFO @PythonRunner@ Times: total = 4, boot = 4, init = 0, finish = 0
21/11/23 11:29:31 @INFO @PythonRunner@ Times: total = 4, boot = 3, init = 1, finish = 0
21/11/23 11:29:31 @INFO @TaskSetManager@ Finished task 3.0 in stage 2.0 (TID 5) in 13 ms on 192.168.0.30 (executor driver) (1/4)
21/11/23 11:29:31 @INFO @Executor@ Finished task 2.0 in stage 2.0 (TID 4). 1781 bytes result sent to driver
21/11/23 11:29:31 @INFO @PythonRunner@ Times: total = 32, boot = 30, init = 2, finish = 0
21/11/23 11:29:31 @INFO @Executor@ Finished task 1.0 in stage 2.0 (TID 3). 1781 bytes result sent to driver
21/11/23 11:29:31 @INFO @TaskSetManager@ Finished task 1.0 in stage 2.0 (TID 3) in 39 ms on 192.168.0.30 (executor driver) (2/4)
21/11/23 11:29:31 @INFO @TaskSetManager@ Finished task 2.0 in stage 2.0 (TID 4) in 38 ms on 192.168.0.30 (executor driver) (3/4)
21/11/23 11:29:31 @INFO @Executor@ Finished task 0.0 in stage 2.0 (TID 2). 1781 bytes result sent to driver
21/11/23 11:29:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 2.0 (TID 2) in 40 ms on 192.168.0.30 (executor driver) (4/4)
21/11/23 11:29:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 2.0, whose tasks have all completed, from pool 
21/11/23 11:29:31 @INFO @DAGScheduler@ ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.046 s
21/11/23 11:29:31 @INFO @DAGScheduler@ Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:29:31 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 2: Stage finished
21/11/23 11:29:31 @INFO @DAGScheduler@ Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.048548 s
21/11/23 11:29:31 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:29:31 @INFO @DAGScheduler@ Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 5 output partitions
21/11/23 11:29:31 @INFO @DAGScheduler@ Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:29:31 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:29:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:29:31 @INFO @DAGScheduler@ Submitting ResultStage 3 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:29:31 @INFO @MemoryStore@ Block broadcast_3 stored as values in memory (estimated size 15.7 KiB, free 434.3 MiB)
21/11/23 11:29:31 @INFO @MemoryStore@ Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.3 MiB)
21/11/23 11:29:31 @INFO @BlockManagerInfo@ Added broadcast_3_piece0 in memory on 192.168.0.30:64566 (size: 7.7 KiB, free: 434.4 MiB)
21/11/23 11:29:31 @INFO @SparkContext@ Created broadcast 3 from broadcast at DAGScheduler.scala:1427
21/11/23 11:29:31 @INFO @DAGScheduler@ Submitting 5 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9))
21/11/23 11:29:31 @INFO @TaskSchedulerImpl@ Adding task set 3.0 with 5 tasks resource profile 0
21/11/23 11:29:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 3.0 (TID 6) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:31 @INFO @TaskSetManager@ Starting task 1.0 in stage 3.0 (TID 7) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:31 @INFO @TaskSetManager@ Starting task 2.0 in stage 3.0 (TID 8) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:31 @INFO @TaskSetManager@ Starting task 3.0 in stage 3.0 (TID 9) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:31 @INFO @TaskSetManager@ Starting task 4.0 in stage 3.0 (TID 10) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()
21/11/23 11:29:31 @INFO @Executor@ Running task 2.0 in stage 3.0 (TID 8)
21/11/23 11:29:31 @INFO @Executor@ Running task 1.0 in stage 3.0 (TID 7)
21/11/23 11:29:31 @INFO @Executor@ Running task 0.0 in stage 3.0 (TID 6)
21/11/23 11:29:31 @INFO @Executor@ Running task 3.0 in stage 3.0 (TID 9)
21/11/23 11:29:31 @INFO @Executor@ Running task 4.0 in stage 3.0 (TID 10)
21/11/23 11:29:31 @INFO @PythonRunner@ Times: total = 1, boot = -44, init = 45, finish = 0
21/11/23 11:29:31 @INFO @PythonRunner@ Times: total = 1, boot = -41, init = 42, finish = 0
21/11/23 11:29:31 @INFO @PythonRunner@ Times: total = 2, boot = -13, init = 15, finish = 0
21/11/23 11:29:31 @INFO @Executor@ Finished task 2.0 in stage 3.0 (TID 8). 1781 bytes result sent to driver
21/11/23 11:29:31 @INFO @PythonRunner@ Times: total = 4, boot = 3, init = 1, finish = 0
21/11/23 11:29:31 @INFO @Executor@ Finished task 0.0 in stage 3.0 (TID 6). 1781 bytes result sent to driver
21/11/23 11:29:31 @INFO @Executor@ Finished task 1.0 in stage 3.0 (TID 7). 1781 bytes result sent to driver
21/11/23 11:29:31 @INFO @Executor@ Finished task 3.0 in stage 3.0 (TID 9). 1781 bytes result sent to driver
21/11/23 11:29:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 3.0 (TID 6) in 10 ms on 192.168.0.30 (executor driver) (1/5)
21/11/23 11:29:31 @INFO @TaskSetManager@ Finished task 3.0 in stage 3.0 (TID 9) in 10 ms on 192.168.0.30 (executor driver) (2/5)
21/11/23 11:29:31 @INFO @TaskSetManager@ Finished task 1.0 in stage 3.0 (TID 7) in 11 ms on 192.168.0.30 (executor driver) (3/5)
21/11/23 11:29:31 @INFO @TaskSetManager@ Finished task 2.0 in stage 3.0 (TID 8) in 11 ms on 192.168.0.30 (executor driver) (4/5)
21/11/23 11:29:31 @INFO @PythonRunner@ Times: total = 6, boot = 5, init = 1, finish = 0
21/11/23 11:29:31 @INFO @Executor@ Finished task 4.0 in stage 3.0 (TID 10). 2252 bytes result sent to driver
21/11/23 11:29:31 @INFO @TaskSetManager@ Finished task 4.0 in stage 3.0 (TID 10) in 15 ms on 192.168.0.30 (executor driver) (5/5)
21/11/23 11:29:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/11/23 11:29:31 @INFO @DAGScheduler@ ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.021 s
21/11/23 11:29:31 @INFO @DAGScheduler@ Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:29:31 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 3: Stage finished
21/11/23 11:29:31 @INFO @DAGScheduler@ Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.022935 s
21/11/23 11:29:31 @INFO @CodeGenerator@ Code generated in 7.436375 ms
21/11/23 11:29:34 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:34 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:34 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:34 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:34 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:34 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:34 @INFO @InMemoryFileIndex@ It took 5 ms to list leaf files for 1 paths.
21/11/23 11:29:34 @INFO @ParquetFileFormat@ Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @CodeGenerator@ Code generated in 9.199625 ms
21/11/23 11:29:34 @INFO @SparkContext@ Starting job: sql at NativeMethodAccessorImpl.java:0
21/11/23 11:29:34 @INFO @DAGScheduler@ Got job 4 (sql at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:29:34 @INFO @DAGScheduler@ Final stage: ResultStage 4 (sql at NativeMethodAccessorImpl.java:0)
21/11/23 11:29:34 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:29:34 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:29:34 @INFO @DAGScheduler@ Submitting ResultStage 4 (MapPartitionsRDD[10] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:29:34 @INFO @MemoryStore@ Block broadcast_4 stored as values in memory (estimated size 213.8 KiB, free 434.1 MiB)
21/11/23 11:29:34 @INFO @MemoryStore@ Block broadcast_4_piece0 stored as bytes in memory (estimated size 77.4 KiB, free 434.0 MiB)
21/11/23 11:29:34 @INFO @BlockManagerInfo@ Added broadcast_4_piece0 in memory on 192.168.0.30:64566 (size: 77.4 KiB, free: 434.3 MiB)
21/11/23 11:29:34 @INFO @SparkContext@ Created broadcast 4 from broadcast at DAGScheduler.scala:1427
21/11/23 11:29:34 @INFO @DAGScheduler@ Submitting 10 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:29:34 @INFO @TaskSchedulerImpl@ Adding task set 4.0 with 10 tasks resource profile 0
21/11/23 11:29:34 @INFO @TaskSetManager@ Starting task 0.0 in stage 4.0 (TID 11) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:34 @INFO @TaskSetManager@ Starting task 1.0 in stage 4.0 (TID 12) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:34 @INFO @TaskSetManager@ Starting task 2.0 in stage 4.0 (TID 13) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:34 @INFO @TaskSetManager@ Starting task 3.0 in stage 4.0 (TID 14) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:34 @INFO @TaskSetManager@ Starting task 4.0 in stage 4.0 (TID 15) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:34 @INFO @TaskSetManager@ Starting task 5.0 in stage 4.0 (TID 16) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:34 @INFO @TaskSetManager@ Starting task 6.0 in stage 4.0 (TID 17) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:34 @INFO @TaskSetManager@ Starting task 7.0 in stage 4.0 (TID 18) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:34 @INFO @TaskSetManager@ Starting task 8.0 in stage 4.0 (TID 19) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()
21/11/23 11:29:34 @INFO @TaskSetManager@ Starting task 9.0 in stage 4.0 (TID 20) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()
21/11/23 11:29:34 @INFO @Executor@ Running task 4.0 in stage 4.0 (TID 15)
21/11/23 11:29:34 @INFO @Executor@ Running task 1.0 in stage 4.0 (TID 12)
21/11/23 11:29:34 @INFO @Executor@ Running task 0.0 in stage 4.0 (TID 11)
21/11/23 11:29:34 @INFO @Executor@ Running task 2.0 in stage 4.0 (TID 13)
21/11/23 11:29:34 @INFO @Executor@ Running task 5.0 in stage 4.0 (TID 16)
21/11/23 11:29:34 @INFO @Executor@ Running task 3.0 in stage 4.0 (TID 14)
21/11/23 11:29:34 @INFO @Executor@ Running task 7.0 in stage 4.0 (TID 18)
21/11/23 11:29:34 @INFO @Executor@ Running task 6.0 in stage 4.0 (TID 17)
21/11/23 11:29:34 @INFO @Executor@ Running task 8.0 in stage 4.0 (TID 19)
21/11/23 11:29:34 @INFO @Executor@ Running task 9.0 in stage 4.0 (TID 20)
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @PythonRunner@ Times: total = 1, boot = -3351, init = 3352, finish = 0
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @PythonRunner@ Times: total = 1, boot = -3346, init = 3347, finish = 0
21/11/23 11:29:34 @INFO @PythonRunner@ Times: total = 1, boot = -3351, init = 3352, finish = 0
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @PythonRunner@ Times: total = 1, boot = -3348, init = 3349, finish = 0
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @PythonRunner@ Times: total = 17, boot = 15, init = 2, finish = 0
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @PythonRunner@ Times: total = 20, boot = 19, init = 1, finish = 0
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @PythonRunner@ Times: total = 26, boot = 24, init = 2, finish = 0
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:29:34 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:29:34 @INFO @SparkHadoopMapRedUtil@ No need to commit output of task because needsTaskCommit=false: attempt_202111231129343889184552743078804_0004_m_000001_12
21/11/23 11:29:34 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:29:34 @INFO @PythonRunner@ Times: total = 27, boot = 27, init = 0, finish = 0
21/11/23 11:29:34 @INFO @SparkHadoopMapRedUtil@ No need to commit output of task because needsTaskCommit=false: attempt_202111231129347280111614837215795_0004_m_000003_14
21/11/23 11:29:34 @INFO @SparkHadoopMapRedUtil@ No need to commit output of task because needsTaskCommit=false: attempt_202111231129341873960578159328669_0004_m_000006_17
21/11/23 11:29:34 @INFO @SparkHadoopMapRedUtil@ No need to commit output of task because needsTaskCommit=false: attempt_202111231129346801959759371931570_0004_m_000004_15
21/11/23 11:29:34 @INFO @SparkHadoopMapRedUtil@ No need to commit output of task because needsTaskCommit=false: attempt_202111231129348883983041434549853_0004_m_000007_18
21/11/23 11:29:34 @INFO @SparkHadoopMapRedUtil@ No need to commit output of task because needsTaskCommit=false: attempt_202111231129344590736926966758779_0004_m_000008_19
21/11/23 11:29:34 @INFO @SparkHadoopMapRedUtil@ No need to commit output of task because needsTaskCommit=false: attempt_202111231129344775429714290877880_0004_m_000002_13
21/11/23 11:29:34 @INFO @SparkHadoopMapRedUtil@ No need to commit output of task because needsTaskCommit=false: attempt_20211123112934171956652925008239_0004_m_000005_16
21/11/23 11:29:34 @INFO @Executor@ Finished task 1.0 in stage 4.0 (TID 12). 2820 bytes result sent to driver
21/11/23 11:29:34 @INFO @Executor@ Finished task 5.0 in stage 4.0 (TID 16). 2820 bytes result sent to driver
21/11/23 11:29:34 @INFO @Executor@ Finished task 6.0 in stage 4.0 (TID 17). 2820 bytes result sent to driver
21/11/23 11:29:34 @INFO @Executor@ Finished task 7.0 in stage 4.0 (TID 18). 2820 bytes result sent to driver
21/11/23 11:29:34 @INFO @Executor@ Finished task 4.0 in stage 4.0 (TID 15). 2820 bytes result sent to driver
21/11/23 11:29:34 @INFO @Executor@ Finished task 2.0 in stage 4.0 (TID 13). 2820 bytes result sent to driver
21/11/23 11:29:34 @INFO @Executor@ Finished task 3.0 in stage 4.0 (TID 14). 2820 bytes result sent to driver
21/11/23 11:29:34 @INFO @Executor@ Finished task 8.0 in stage 4.0 (TID 19). 2820 bytes result sent to driver
21/11/23 11:29:34 @INFO @TaskSetManager@ Finished task 1.0 in stage 4.0 (TID 12) in 102 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:29:34 @INFO @TaskSetManager@ Finished task 5.0 in stage 4.0 (TID 16) in 101 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:29:34 @INFO @TaskSetManager@ Finished task 6.0 in stage 4.0 (TID 17) in 101 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:29:34 @INFO @TaskSetManager@ Finished task 7.0 in stage 4.0 (TID 18) in 102 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:29:34 @INFO @TaskSetManager@ Finished task 2.0 in stage 4.0 (TID 13) in 103 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:29:34 @INFO @TaskSetManager@ Finished task 4.0 in stage 4.0 (TID 15) in 103 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:29:34 @INFO @TaskSetManager@ Finished task 3.0 in stage 4.0 (TID 14) in 103 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:29:34 @INFO @TaskSetManager@ Finished task 8.0 in stage 4.0 (TID 19) in 102 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:29:34 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "database_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "table_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "table_definition",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sammary",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "record_num",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "selectivity",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "consistency_flag",
    "type" : "boolean",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "frequency_access",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary database_name (STRING);
  optional binary table_name (STRING);
  optional binary table_definition (STRING);
  optional binary sammary (STRING);
  optional binary record_num (STRING);
  optional binary selectivity (STRING);
  optional boolean consistency_flag;
  optional binary frequency_access (STRING);
}

       
21/11/23 11:29:34 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "database_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "table_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "table_definition",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sammary",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "record_num",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "selectivity",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "consistency_flag",
    "type" : "boolean",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "frequency_access",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary database_name (STRING);
  optional binary table_name (STRING);
  optional binary table_definition (STRING);
  optional binary sammary (STRING);
  optional binary record_num (STRING);
  optional binary selectivity (STRING);
  optional boolean consistency_flag;
  optional binary frequency_access (STRING);
}

       
21/11/23 11:29:34 @INFO @CodecPool@ Got brand-new compressor [.snappy]
21/11/23 11:29:34 @INFO @CodecPool@ Got brand-new compressor [.snappy]
21/11/23 11:29:35 @INFO @PythonRunner@ Times: total = 6, boot = 4, init = 2, finish = 0
21/11/23 11:29:35 @INFO @PythonRunner@ Times: total = 9, boot = 7, init = 1, finish = 1
21/11/23 11:29:35 @INFO @FileOutputCommitter@ Saved output of task 'attempt_202111231129346085824798911640212_0004_m_000000_11' to file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata/_temporary/0/task_202111231129346085824798911640212_0004_m_000000
21/11/23 11:29:35 @INFO @SparkHadoopMapRedUtil@ attempt_202111231129346085824798911640212_0004_m_000000_11: Committed
21/11/23 11:29:35 @INFO @Executor@ Finished task 0.0 in stage 4.0 (TID 11). 2820 bytes result sent to driver
21/11/23 11:29:35 @INFO @TaskSetManager@ Finished task 0.0 in stage 4.0 (TID 11) in 424 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:29:35 @INFO @FileOutputCommitter@ Saved output of task 'attempt_202111231129348942865947971519890_0004_m_000009_20' to file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata/_temporary/0/task_202111231129348942865947971519890_0004_m_000009
21/11/23 11:29:35 @INFO @SparkHadoopMapRedUtil@ attempt_202111231129348942865947971519890_0004_m_000009_20: Committed
21/11/23 11:29:35 @INFO @Executor@ Finished task 9.0 in stage 4.0 (TID 20). 2863 bytes result sent to driver
21/11/23 11:29:35 @INFO @TaskSetManager@ Finished task 9.0 in stage 4.0 (TID 20) in 484 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:29:35 @INFO @TaskSchedulerImpl@ Removed TaskSet 4.0, whose tasks have all completed, from pool 
21/11/23 11:29:35 @INFO @DAGScheduler@ ResultStage 4 (sql at NativeMethodAccessorImpl.java:0) finished in 0.513 s
21/11/23 11:29:35 @INFO @DAGScheduler@ Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:29:35 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 4: Stage finished
21/11/23 11:29:35 @INFO @DAGScheduler@ Job 4 finished: sql at NativeMethodAccessorImpl.java:0, took 0.515291 s
21/11/23 11:29:35 @INFO @FileFormatWriter@ Start to commit write Job 536667df-c5af-4f7c-93f7-be35f87e1899.
21/11/23 11:29:35 @INFO @FileFormatWriter@ Write Job 536667df-c5af-4f7c-93f7-be35f87e1899 committed. Elapsed time: 11 ms.
21/11/23 11:29:35 @INFO @FileFormatWriter@ Finished processing stats for write job 536667df-c5af-4f7c-93f7-be35f87e1899.
21/11/23 11:29:35 @INFO @InMemoryFileIndex@ It took 1 ms to list leaf files for 1 paths.
21/11/23 11:29:35 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:35 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:35 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:35 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:35 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:35 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:35 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:35 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:35 @INFO @HiveMetaStore@ 0: alter_table: db=metadata_tmp tbl=sample_metadata newtbl=sample_metadata
21/11/23 11:29:35 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=alter_table: db=metadata_tmp tbl=sample_metadata newtbl=sample_metadata	
21/11/23 11:29:35 @INFO @log@ Updating table stats fast for sample_metadata
21/11/23 11:29:35 @INFO @log@ Updated size of table sample_metadata to 6059
21/11/23 11:29:36 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:29:36 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:29:36 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:36 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:36 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:29:36 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:29:36 @INFO @InMemoryFileIndex@ It took 1 ms to list leaf files for 1 paths.
21/11/23 11:29:36 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:29:36 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:29:36 @INFO @FileSourceStrategy@ Output Data Schema: struct<database_name: string, table_name: string, table_definition: string, sammary: string, record_num: string ... 6 more fields>
21/11/23 11:29:36 @INFO @CodeGenerator@ Code generated in 11.095542 ms
21/11/23 11:29:36 @INFO @MemoryStore@ Block broadcast_5 stored as values in memory (estimated size 194.4 KiB, free 433.8 MiB)
21/11/23 11:29:36 @INFO @MemoryStore@ Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.8 MiB)
21/11/23 11:29:36 @INFO @BlockManagerInfo@ Added broadcast_5_piece0 in memory on 192.168.0.30:64566 (size: 34.4 KiB, free: 434.3 MiB)
21/11/23 11:29:36 @INFO @SparkContext@ Created broadcast 5 from showString at NativeMethodAccessorImpl.java:0
21/11/23 11:29:37 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:29:37 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:29:37 @INFO @DAGScheduler@ Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:29:37 @INFO @DAGScheduler@ Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:29:37 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:29:37 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:29:37 @INFO @DAGScheduler@ Submitting ResultStage 5 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:29:37 @INFO @MemoryStore@ Block broadcast_6 stored as values in memory (estimated size 17.0 KiB, free 433.8 MiB)
21/11/23 11:29:37 @INFO @MemoryStore@ Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.8 MiB)
21/11/23 11:29:37 @INFO @BlockManagerInfo@ Added broadcast_6_piece0 in memory on 192.168.0.30:64566 (size: 6.5 KiB, free: 434.3 MiB)
21/11/23 11:29:37 @INFO @SparkContext@ Created broadcast 6 from broadcast at DAGScheduler.scala:1427
21/11/23 11:29:37 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:29:37 @INFO @TaskSchedulerImpl@ Adding task set 5.0 with 1 tasks resource profile 0
21/11/23 11:29:37 @INFO @TaskSetManager@ Starting task 0.0 in stage 5.0 (TID 21) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 5004 bytes) taskResourceAssignments Map()
21/11/23 11:29:37 @INFO @Executor@ Running task 0.0 in stage 5.0 (TID 21)
21/11/23 11:29:37 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata/part-00009-e9c0b82c-259e-48fa-8ba0-ba5b20f74dd2-c000.snappy.parquet, range: 0-5047, partition values: [empty row]
21/11/23 11:29:37 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:29:37 @INFO @Executor@ Finished task 0.0 in stage 5.0 (TID 21). 2112 bytes result sent to driver
21/11/23 11:29:37 @INFO @TaskSetManager@ Finished task 0.0 in stage 5.0 (TID 21) in 66 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:29:37 @INFO @TaskSchedulerImpl@ Removed TaskSet 5.0, whose tasks have all completed, from pool 
21/11/23 11:29:37 @INFO @DAGScheduler@ ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 0.077 s
21/11/23 11:29:37 @INFO @DAGScheduler@ Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:29:37 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 5: Stage finished
21/11/23 11:29:37 @INFO @DAGScheduler@ Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 0.080246 s
21/11/23 11:29:37 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:29:37 @INFO @DAGScheduler@ Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:29:37 @INFO @DAGScheduler@ Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:29:37 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:29:37 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:29:37 @INFO @DAGScheduler@ Submitting ResultStage 6 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:29:37 @INFO @MemoryStore@ Block broadcast_7 stored as values in memory (estimated size 17.0 KiB, free 433.8 MiB)
21/11/23 11:29:37 @INFO @MemoryStore@ Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 433.8 MiB)
21/11/23 11:29:37 @INFO @BlockManagerInfo@ Added broadcast_7_piece0 in memory on 192.168.0.30:64566 (size: 6.6 KiB, free: 434.3 MiB)
21/11/23 11:29:37 @INFO @SparkContext@ Created broadcast 7 from broadcast at DAGScheduler.scala:1427
21/11/23 11:29:37 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))
21/11/23 11:29:37 @INFO @TaskSchedulerImpl@ Adding task set 6.0 with 1 tasks resource profile 0
21/11/23 11:29:37 @INFO @TaskSetManager@ Starting task 0.0 in stage 6.0 (TID 22) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 5004 bytes) taskResourceAssignments Map()
21/11/23 11:29:37 @INFO @Executor@ Running task 0.0 in stage 6.0 (TID 22)
21/11/23 11:29:37 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata/part-00000-e9c0b82c-259e-48fa-8ba0-ba5b20f74dd2-c000.snappy.parquet, range: 0-1012, partition values: [empty row]
21/11/23 11:29:37 @INFO @Executor@ Finished task 0.0 in stage 6.0 (TID 22). 1598 bytes result sent to driver
21/11/23 11:29:37 @INFO @TaskSetManager@ Finished task 0.0 in stage 6.0 (TID 22) in 6 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:29:37 @INFO @TaskSchedulerImpl@ Removed TaskSet 6.0, whose tasks have all completed, from pool 
21/11/23 11:29:37 @INFO @DAGScheduler@ ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 0.009 s
21/11/23 11:29:37 @INFO @DAGScheduler@ Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:29:37 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 6: Stage finished
21/11/23 11:29:37 @INFO @DAGScheduler@ Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.010661 s
21/11/23 11:29:39 @INFO @SparkUI@ Stopped Spark web UI at http://192.168.0.30:4043
21/11/23 11:29:39 @INFO @MapOutputTrackerMasterEndpoint@ MapOutputTrackerMasterEndpoint stopped!
21/11/23 11:29:39 @INFO @MemoryStore@ MemoryStore cleared
21/11/23 11:29:39 @INFO @BlockManager@ BlockManager stopped
21/11/23 11:29:39 @INFO @BlockManagerMaster@ BlockManagerMaster stopped
21/11/23 11:29:39 @INFO @OutputCommitCoordinator$OutputCommitCoordinatorEndpoint@ OutputCommitCoordinator stopped!
21/11/23 11:29:39 @INFO @SparkContext@ Successfully stopped SparkContext
21/11/23 11:30:00 @WARN @Utils@ Your hostname, yukisaitos-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.30 instead (on interface en0)
21/11/23 11:30:00 @WARN @Utils@ Set SPARK_LOCAL_IP if you need to bind to another address
21/11/23 11:30:00 @INFO @SparkContext@ Running Spark version 3.2.0
21/11/23 11:30:00 @WARN @NativeCodeLoader@ Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/11/23 11:30:00 @INFO @ResourceUtils@ ==============================================================
21/11/23 11:30:00 @INFO @ResourceUtils@ No custom resources configured for spark.driver.
21/11/23 11:30:00 @INFO @ResourceUtils@ ==============================================================
21/11/23 11:30:00 @INFO @SparkContext@ Submitted application: chapter1
21/11/23 11:30:00 @INFO @ResourceProfile@ Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/11/23 11:30:00 @INFO @ResourceProfile@ Limiting resource is cpu
21/11/23 11:30:00 @INFO @ResourceProfileManager@ Added ResourceProfile id: 0
21/11/23 11:30:00 @INFO @SecurityManager@ Changing view acls to: saitouyuuki
21/11/23 11:30:00 @INFO @SecurityManager@ Changing modify acls to: saitouyuuki
21/11/23 11:30:00 @INFO @SecurityManager@ Changing view acls groups to: 
21/11/23 11:30:00 @INFO @SecurityManager@ Changing modify acls groups to: 
21/11/23 11:30:00 @INFO @SecurityManager@ SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(saitouyuuki); groups with view permissions: Set(); users  with modify permissions: Set(saitouyuuki); groups with modify permissions: Set()
21/11/23 11:30:00 @INFO @Utils@ Successfully started service 'sparkDriver' on port 64660.
21/11/23 11:30:00 @INFO @SparkEnv@ Registering MapOutputTracker
21/11/23 11:30:00 @INFO @SparkEnv@ Registering BlockManagerMaster
21/11/23 11:30:00 @INFO @BlockManagerMasterEndpoint@ Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/11/23 11:30:00 @INFO @BlockManagerMasterEndpoint@ BlockManagerMasterEndpoint up
21/11/23 11:30:00 @INFO @SparkEnv@ Registering BlockManagerMasterHeartbeat
21/11/23 11:30:00 @INFO @DiskBlockManager@ Created local directory at /private/var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/blockmgr-3c7ec515-6b02-499b-8c24-8a370724c4b6
21/11/23 11:30:00 @INFO @MemoryStore@ MemoryStore started with capacity 434.4 MiB
21/11/23 11:30:00 @INFO @SparkEnv@ Registering OutputCommitCoordinator
21/11/23 11:30:01 @INFO @Utils@ Successfully started service 'SparkUI' on port 4040.
21/11/23 11:30:01 @INFO @SparkUI@ Bound SparkUI to 0.0.0.0, and started at http://192.168.0.30:4040
21/11/23 11:30:01 @INFO @Executor@ Starting executor ID driver on host 192.168.0.30
21/11/23 11:30:01 @INFO @Utils@ Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64662.
21/11/23 11:30:01 @INFO @NettyBlockTransferService@ Server created on 192.168.0.30:64662
21/11/23 11:30:01 @INFO @BlockManager@ Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/11/23 11:30:01 @INFO @BlockManagerMaster@ Registering BlockManager BlockManagerId(driver, 192.168.0.30, 64662, None)
21/11/23 11:30:01 @INFO @BlockManagerMasterEndpoint@ Registering block manager 192.168.0.30:64662 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.30, 64662, None)
21/11/23 11:30:01 @INFO @BlockManagerMaster@ Registered BlockManager BlockManagerId(driver, 192.168.0.30, 64662, None)
21/11/23 11:30:01 @INFO @BlockManager@ Initialized BlockManager: BlockManagerId(driver, 192.168.0.30, 64662, None)
21/11/23 11:30:01 @INFO @SingleEventLogFileWriter@ Logging events to file:/private/tmp/spark-events/local-1637634601132.inprogress
21/11/23 11:30:01 @INFO @SharedState@ Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
21/11/23 11:30:01 @INFO @SharedState@ Warehouse path is 'file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse'.
21/11/23 11:30:04 @INFO @HiveConf@ Found configuration file file:/opt/homebrew/Cellar/apache-spark/3.2.0/libexec/conf/hive-site.xml
21/11/23 11:30:04 @INFO @HiveUtils@ Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
21/11/23 11:30:04 @INFO @HiveClientImpl@ Warehouse location for Hive client (version 2.3.9) is file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/spark-warehouse
21/11/23 11:30:04 @WARN @HiveConf@ HiveConf of name hive.stats.jdbc.timeout does not exist
21/11/23 11:30:04 @WARN @HiveConf@ HiveConf of name hive.stats.retries.wait does not exist
21/11/23 11:30:04 @INFO @HiveMetaStore@ 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/11/23 11:30:04 @INFO @ObjectStore@ ObjectStore, initialize called
21/11/23 11:30:04 @INFO @Persistence@ Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/11/23 11:30:04 @INFO @Persistence@ Property datanucleus.cache.level2 unknown - will be ignored
21/11/23 11:30:05 @INFO @ObjectStore@ Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/11/23 11:30:05 @INFO @MetaStoreDirectSql@ Using direct SQL, underlying DB is MYSQL
21/11/23 11:30:05 @INFO @ObjectStore@ Initialized ObjectStore
21/11/23 11:30:05 @INFO @HiveMetaStore@ Added admin role in metastore
21/11/23 11:30:05 @INFO @HiveMetaStore@ Added public role in metastore
21/11/23 11:30:05 @INFO @HiveMetaStore@ No user is added in admin role, since config is empty
21/11/23 11:30:05 @INFO @HiveMetaStore@ 0: get_database: default
21/11/23 11:30:05 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: default	
21/11/23 11:30:05 @INFO @HiveMetaStore@ 0: get_database: global_temp
21/11/23 11:30:05 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: global_temp	
21/11/23 11:30:05 @WARN @ObjectStore@ Failed to get database global_temp, returning NoSuchObjectException
21/11/23 11:30:05 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:30:05 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:30:05 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:05 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:06 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:06 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:06 @INFO @InMemoryFileIndex@ It took 36 ms to list leaf files for 1 paths.
21/11/23 11:30:09 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:09 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:09 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:09 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:09 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:09 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:09 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:09 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:09 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:09 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:09 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:09 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:09 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:09 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:09 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:09 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:09 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:09 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:09 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:09 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:09 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:09 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:10 @INFO @CodeGenerator@ Code generated in 102.139333 ms
21/11/23 11:30:10 @INFO @MemoryStore@ Block broadcast_0 stored as values in memory (estimated size 193.4 KiB, free 434.2 MiB)
21/11/23 11:30:10 @INFO @MemoryStore@ Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
21/11/23 11:30:10 @INFO @BlockManagerInfo@ Added broadcast_0_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.4 MiB)
21/11/23 11:30:10 @INFO @SparkContext@ Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:10 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:10 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:10 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:10 @INFO @SQLStdHiveAccessController@ Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=223cf2dd-ff85-4d6e-a08a-b2a15b3dc361, clientType=HIVECLI]
21/11/23 11:30:10 @WARN @SessionState@ METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
21/11/23 11:30:10 @INFO @metastore@ Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
21/11/23 11:30:10 @INFO @HiveMetaStore@ 0: Cleaning up thread local RawStore...
21/11/23 11:30:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
21/11/23 11:30:10 @INFO @HiveMetaStore@ 0: Done cleaning up thread local RawStore
21/11/23 11:30:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
21/11/23 11:30:10 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:10 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:10 @WARN @HiveConf@ HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
21/11/23 11:30:10 @WARN @HiveConf@ HiveConf of name hive.stats.jdbc.timeout does not exist
21/11/23 11:30:10 @WARN @HiveConf@ HiveConf of name hive.stats.retries.wait does not exist
21/11/23 11:30:10 @INFO @HiveMetaStore@ 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/11/23 11:30:10 @INFO @ObjectStore@ ObjectStore, initialize called
21/11/23 11:30:10 @INFO @MetaStoreDirectSql@ Using direct SQL, underlying DB is MYSQL
21/11/23 11:30:10 @INFO @ObjectStore@ Initialized ObjectStore
21/11/23 11:30:10 @INFO @HadoopFSUtils@ Listing leaf files and directories in parallel under 50 paths. The first several paths are: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県.
21/11/23 11:30:10 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:10 @INFO @DAGScheduler@ Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 50 output partitions
21/11/23 11:30:10 @INFO @DAGScheduler@ Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:10 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:10 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:10 @INFO @DAGScheduler@ Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:10 @INFO @MemoryStore@ Block broadcast_1 stored as values in memory (estimated size 101.0 KiB, free 434.1 MiB)
21/11/23 11:30:10 @INFO @MemoryStore@ Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 434.0 MiB)
21/11/23 11:30:10 @INFO @BlockManagerInfo@ Added broadcast_1_piece0 in memory on 192.168.0.30:64662 (size: 36.2 KiB, free: 434.3 MiB)
21/11/23 11:30:10 @INFO @SparkContext@ Created broadcast 1 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:10 @INFO @DAGScheduler@ Submitting 50 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/11/23 11:30:10 @INFO @TaskSchedulerImpl@ Adding task set 0.0 with 50 tasks resource profile 0
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 1.0 in stage 0.0 (TID 1) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 2.0 in stage 0.0 (TID 2) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 4599 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 3.0 in stage 0.0 (TID 3) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 4614 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 4.0 in stage 0.0 (TID 4) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 5.0 in stage 0.0 (TID 5) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 4587 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 6.0 in stage 0.0 (TID 6) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 7.0 in stage 0.0 (TID 7) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 8.0 in stage 0.0 (TID 8) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 9.0 in stage 0.0 (TID 9) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 0.0 in stage 0.0 (TID 0)
21/11/23 11:30:10 @INFO @Executor@ Running task 5.0 in stage 0.0 (TID 5)
21/11/23 11:30:10 @INFO @Executor@ Running task 2.0 in stage 0.0 (TID 2)
21/11/23 11:30:10 @INFO @Executor@ Running task 9.0 in stage 0.0 (TID 9)
21/11/23 11:30:10 @INFO @Executor@ Running task 1.0 in stage 0.0 (TID 1)
21/11/23 11:30:10 @INFO @Executor@ Running task 7.0 in stage 0.0 (TID 7)
21/11/23 11:30:10 @INFO @Executor@ Running task 6.0 in stage 0.0 (TID 6)
21/11/23 11:30:10 @INFO @Executor@ Running task 8.0 in stage 0.0 (TID 8)
21/11/23 11:30:10 @INFO @Executor@ Running task 4.0 in stage 0.0 (TID 4)
21/11/23 11:30:10 @INFO @Executor@ Running task 3.0 in stage 0.0 (TID 3)
21/11/23 11:30:10 @INFO @Executor@ Finished task 3.0 in stage 0.0 (TID 3). 2121 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 6.0 in stage 0.0 (TID 6). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 1.0 in stage 0.0 (TID 1). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 2.0 in stage 0.0 (TID 2). 2088 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 9.0 in stage 0.0 (TID 9). 2076 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 5.0 in stage 0.0 (TID 5). 2064 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 4.0 in stage 0.0 (TID 4). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 8.0 in stage 0.0 (TID 8). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 0.0 in stage 0.0 (TID 0). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 7.0 in stage 0.0 (TID 7). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 10.0 in stage 0.0 (TID 10) (192.168.0.30, executor driver, partition 10, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 10.0 in stage 0.0 (TID 10)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 11.0 in stage 0.0 (TID 11) (192.168.0.30, executor driver, partition 11, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 11.0 in stage 0.0 (TID 11)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 12.0 in stage 0.0 (TID 12) (192.168.0.30, executor driver, partition 12, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 12.0 in stage 0.0 (TID 12)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 13.0 in stage 0.0 (TID 13) (192.168.0.30, executor driver, partition 13, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 14.0 in stage 0.0 (TID 14) (192.168.0.30, executor driver, partition 14, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 14.0 in stage 0.0 (TID 14)
21/11/23 11:30:10 @INFO @Executor@ Running task 13.0 in stage 0.0 (TID 13)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 6.0 in stage 0.0 (TID 6) in 128 ms on 192.168.0.30 (executor driver) (1/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 9.0 in stage 0.0 (TID 9) in 130 ms on 192.168.0.30 (executor driver) (2/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 3.0 in stage 0.0 (TID 3) in 131 ms on 192.168.0.30 (executor driver) (3/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 15.0 in stage 0.0 (TID 15) (192.168.0.30, executor driver, partition 15, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 15.0 in stage 0.0 (TID 15)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 16.0 in stage 0.0 (TID 16) (192.168.0.30, executor driver, partition 16, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 16.0 in stage 0.0 (TID 16)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 5.0 in stage 0.0 (TID 5) in 133 ms on 192.168.0.30 (executor driver) (4/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 0.0 in stage 0.0 (TID 0) in 149 ms on 192.168.0.30 (executor driver) (5/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 17.0 in stage 0.0 (TID 17) (192.168.0.30, executor driver, partition 17, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 18.0 in stage 0.0 (TID 18) (192.168.0.30, executor driver, partition 18, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 19.0 in stage 0.0 (TID 19) (192.168.0.30, executor driver, partition 19, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 19.0 in stage 0.0 (TID 19)
21/11/23 11:30:10 @INFO @Executor@ Running task 17.0 in stage 0.0 (TID 17)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 4.0 in stage 0.0 (TID 4) in 139 ms on 192.168.0.30 (executor driver) (6/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 18.0 in stage 0.0 (TID 18)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 1.0 in stage 0.0 (TID 1) in 141 ms on 192.168.0.30 (executor driver) (7/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 8.0 in stage 0.0 (TID 8) in 140 ms on 192.168.0.30 (executor driver) (8/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 7.0 in stage 0.0 (TID 7) in 141 ms on 192.168.0.30 (executor driver) (9/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 2.0 in stage 0.0 (TID 2) in 143 ms on 192.168.0.30 (executor driver) (10/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 12.0 in stage 0.0 (TID 12). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 11.0 in stage 0.0 (TID 11). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 20.0 in stage 0.0 (TID 20) (192.168.0.30, executor driver, partition 20, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 21.0 in stage 0.0 (TID 21) (192.168.0.30, executor driver, partition 21, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Finished task 10.0 in stage 0.0 (TID 10). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 12.0 in stage 0.0 (TID 12) in 22 ms on 192.168.0.30 (executor driver) (11/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 20.0 in stage 0.0 (TID 20)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 11.0 in stage 0.0 (TID 11) in 23 ms on 192.168.0.30 (executor driver) (12/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 21.0 in stage 0.0 (TID 21)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 22.0 in stage 0.0 (TID 22) (192.168.0.30, executor driver, partition 22, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 22.0 in stage 0.0 (TID 22)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 10.0 in stage 0.0 (TID 10) in 26 ms on 192.168.0.30 (executor driver) (13/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 13.0 in stage 0.0 (TID 13). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 16.0 in stage 0.0 (TID 16). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 23.0 in stage 0.0 (TID 23) (192.168.0.30, executor driver, partition 23, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 23.0 in stage 0.0 (TID 23)
21/11/23 11:30:10 @INFO @Executor@ Finished task 14.0 in stage 0.0 (TID 14). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 24.0 in stage 0.0 (TID 24) (192.168.0.30, executor driver, partition 24, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Finished task 19.0 in stage 0.0 (TID 19). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Running task 24.0 in stage 0.0 (TID 24)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 25.0 in stage 0.0 (TID 25) (192.168.0.30, executor driver, partition 25, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 26.0 in stage 0.0 (TID 26) (192.168.0.30, executor driver, partition 26, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 16.0 in stage 0.0 (TID 16) in 35 ms on 192.168.0.30 (executor driver) (14/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 19.0 in stage 0.0 (TID 19) in 32 ms on 192.168.0.30 (executor driver) (15/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 25.0 in stage 0.0 (TID 25)
21/11/23 11:30:10 @INFO @Executor@ Running task 26.0 in stage 0.0 (TID 26)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 14.0 in stage 0.0 (TID 14) in 45 ms on 192.168.0.30 (executor driver) (16/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 13.0 in stage 0.0 (TID 13) in 46 ms on 192.168.0.30 (executor driver) (17/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 20.0 in stage 0.0 (TID 20). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 27.0 in stage 0.0 (TID 27) (192.168.0.30, executor driver, partition 27, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 20.0 in stage 0.0 (TID 20) in 32 ms on 192.168.0.30 (executor driver) (18/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 15.0 in stage 0.0 (TID 15). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Running task 27.0 in stage 0.0 (TID 27)
21/11/23 11:30:10 @INFO @Executor@ Finished task 18.0 in stage 0.0 (TID 18). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 28.0 in stage 0.0 (TID 28) (192.168.0.30, executor driver, partition 28, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 29.0 in stage 0.0 (TID 29) (192.168.0.30, executor driver, partition 29, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 29.0 in stage 0.0 (TID 29)
21/11/23 11:30:10 @INFO @Executor@ Finished task 17.0 in stage 0.0 (TID 17). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 18.0 in stage 0.0 (TID 18) in 45 ms on 192.168.0.30 (executor driver) (19/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 28.0 in stage 0.0 (TID 28)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 30.0 in stage 0.0 (TID 30) (192.168.0.30, executor driver, partition 30, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 15.0 in stage 0.0 (TID 15) in 51 ms on 192.168.0.30 (executor driver) (20/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 30.0 in stage 0.0 (TID 30)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 17.0 in stage 0.0 (TID 17) in 48 ms on 192.168.0.30 (executor driver) (21/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 21.0 in stage 0.0 (TID 21). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 31.0 in stage 0.0 (TID 31) (192.168.0.30, executor driver, partition 31, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 21.0 in stage 0.0 (TID 21) in 41 ms on 192.168.0.30 (executor driver) (22/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 22.0 in stage 0.0 (TID 22). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Running task 31.0 in stage 0.0 (TID 31)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 32.0 in stage 0.0 (TID 32) (192.168.0.30, executor driver, partition 32, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 22.0 in stage 0.0 (TID 22) in 40 ms on 192.168.0.30 (executor driver) (23/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 32.0 in stage 0.0 (TID 32)
21/11/23 11:30:10 @INFO @Executor@ Finished task 23.0 in stage 0.0 (TID 23). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 33.0 in stage 0.0 (TID 33) (192.168.0.30, executor driver, partition 33, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 33.0 in stage 0.0 (TID 33)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 23.0 in stage 0.0 (TID 23) in 42 ms on 192.168.0.30 (executor driver) (24/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 25.0 in stage 0.0 (TID 25). 1984 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 34.0 in stage 0.0 (TID 34) (192.168.0.30, executor driver, partition 34, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 25.0 in stage 0.0 (TID 25) in 42 ms on 192.168.0.30 (executor driver) (25/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 34.0 in stage 0.0 (TID 34)
21/11/23 11:30:10 @INFO @Executor@ Finished task 26.0 in stage 0.0 (TID 26). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 35.0 in stage 0.0 (TID 35) (192.168.0.30, executor driver, partition 35, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 35.0 in stage 0.0 (TID 35)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 26.0 in stage 0.0 (TID 26) in 44 ms on 192.168.0.30 (executor driver) (26/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 24.0 in stage 0.0 (TID 24). 1984 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 36.0 in stage 0.0 (TID 36) (192.168.0.30, executor driver, partition 36, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 36.0 in stage 0.0 (TID 36)
21/11/23 11:30:10 @INFO @Executor@ Finished task 31.0 in stage 0.0 (TID 31). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 29.0 in stage 0.0 (TID 29). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 37.0 in stage 0.0 (TID 37) (192.168.0.30, executor driver, partition 37, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Finished task 30.0 in stage 0.0 (TID 30). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 38.0 in stage 0.0 (TID 38) (192.168.0.30, executor driver, partition 38, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 37.0 in stage 0.0 (TID 37)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 39.0 in stage 0.0 (TID 39) (192.168.0.30, executor driver, partition 39, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 38.0 in stage 0.0 (TID 38)
21/11/23 11:30:10 @INFO @Executor@ Running task 39.0 in stage 0.0 (TID 39)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 24.0 in stage 0.0 (TID 24) in 59 ms on 192.168.0.30 (executor driver) (27/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 32.0 in stage 0.0 (TID 32). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 29.0 in stage 0.0 (TID 29) in 42 ms on 192.168.0.30 (executor driver) (28/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 27.0 in stage 0.0 (TID 27). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 30.0 in stage 0.0 (TID 30) in 41 ms on 192.168.0.30 (executor driver) (29/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 31.0 in stage 0.0 (TID 31) in 37 ms on 192.168.0.30 (executor driver) (30/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 28.0 in stage 0.0 (TID 28). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 40.0 in stage 0.0 (TID 40) (192.168.0.30, executor driver, partition 40, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 41.0 in stage 0.0 (TID 41) (192.168.0.30, executor driver, partition 41, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 32.0 in stage 0.0 (TID 32) in 37 ms on 192.168.0.30 (executor driver) (31/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 41.0 in stage 0.0 (TID 41)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 42.0 in stage 0.0 (TID 42) (192.168.0.30, executor driver, partition 42, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 27.0 in stage 0.0 (TID 27) in 50 ms on 192.168.0.30 (executor driver) (32/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 28.0 in stage 0.0 (TID 28) in 48 ms on 192.168.0.30 (executor driver) (33/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 33.0 in stage 0.0 (TID 33). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Running task 40.0 in stage 0.0 (TID 40)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 43.0 in stage 0.0 (TID 43) (192.168.0.30, executor driver, partition 43, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 42.0 in stage 0.0 (TID 42)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 33.0 in stage 0.0 (TID 33) in 26 ms on 192.168.0.30 (executor driver) (34/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 43.0 in stage 0.0 (TID 43)
21/11/23 11:30:10 @INFO @Executor@ Finished task 35.0 in stage 0.0 (TID 35). 2033 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 44.0 in stage 0.0 (TID 44) (192.168.0.30, executor driver, partition 44, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 35.0 in stage 0.0 (TID 35) in 25 ms on 192.168.0.30 (executor driver) (35/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 44.0 in stage 0.0 (TID 44)
21/11/23 11:30:10 @INFO @Executor@ Finished task 34.0 in stage 0.0 (TID 34). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 45.0 in stage 0.0 (TID 45) (192.168.0.30, executor driver, partition 45, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 34.0 in stage 0.0 (TID 34) in 55 ms on 192.168.0.30 (executor driver) (36/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 36.0 in stage 0.0 (TID 36). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Running task 45.0 in stage 0.0 (TID 45)
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 46.0 in stage 0.0 (TID 46) (192.168.0.30, executor driver, partition 46, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 46.0 in stage 0.0 (TID 46)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 36.0 in stage 0.0 (TID 36) in 54 ms on 192.168.0.30 (executor driver) (37/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 39.0 in stage 0.0 (TID 39). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 47.0 in stage 0.0 (TID 47) (192.168.0.30, executor driver, partition 47, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 39.0 in stage 0.0 (TID 39) in 60 ms on 192.168.0.30 (executor driver) (38/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 47.0 in stage 0.0 (TID 47)
21/11/23 11:30:10 @INFO @Executor@ Finished task 38.0 in stage 0.0 (TID 38). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 48.0 in stage 0.0 (TID 48) (192.168.0.30, executor driver, partition 48, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 38.0 in stage 0.0 (TID 38) in 65 ms on 192.168.0.30 (executor driver) (39/50)
21/11/23 11:30:10 @INFO @Executor@ Running task 48.0 in stage 0.0 (TID 48)
21/11/23 11:30:10 @INFO @Executor@ Finished task 37.0 in stage 0.0 (TID 37). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 43.0 in stage 0.0 (TID 43). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 49.0 in stage 0.0 (TID 49) (192.168.0.30, executor driver, partition 49, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 49.0 in stage 0.0 (TID 49)
21/11/23 11:30:10 @INFO @Executor@ Finished task 42.0 in stage 0.0 (TID 42). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 37.0 in stage 0.0 (TID 37) in 71 ms on 192.168.0.30 (executor driver) (40/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 41.0 in stage 0.0 (TID 41). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 42.0 in stage 0.0 (TID 42) in 61 ms on 192.168.0.30 (executor driver) (41/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 43.0 in stage 0.0 (TID 43) in 58 ms on 192.168.0.30 (executor driver) (42/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 41.0 in stage 0.0 (TID 41) in 63 ms on 192.168.0.30 (executor driver) (43/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 44.0 in stage 0.0 (TID 44). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 46.0 in stage 0.0 (TID 46). 2027 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 40.0 in stage 0.0 (TID 40). 2070 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 45.0 in stage 0.0 (TID 45). 1984 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 40.0 in stage 0.0 (TID 40) in 70 ms on 192.168.0.30 (executor driver) (44/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 46.0 in stage 0.0 (TID 46) in 31 ms on 192.168.0.30 (executor driver) (45/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 45.0 in stage 0.0 (TID 45) in 34 ms on 192.168.0.30 (executor driver) (46/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 44.0 in stage 0.0 (TID 44) in 61 ms on 192.168.0.30 (executor driver) (47/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 47.0 in stage 0.0 (TID 47). 1984 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 47.0 in stage 0.0 (TID 47) in 24 ms on 192.168.0.30 (executor driver) (48/50)
21/11/23 11:30:10 @INFO @Executor@ Finished task 48.0 in stage 0.0 (TID 48). 1984 bytes result sent to driver
21/11/23 11:30:10 @INFO @Executor@ Finished task 49.0 in stage 0.0 (TID 49). 2033 bytes result sent to driver
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 48.0 in stage 0.0 (TID 48) in 22 ms on 192.168.0.30 (executor driver) (49/50)
21/11/23 11:30:10 @INFO @TaskSetManager@ Finished task 49.0 in stage 0.0 (TID 49) in 19 ms on 192.168.0.30 (executor driver) (50/50)
21/11/23 11:30:10 @INFO @TaskSchedulerImpl@ Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/11/23 11:30:10 @INFO @DAGScheduler@ ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 0.380 s
21/11/23 11:30:10 @INFO @DAGScheduler@ Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:10 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 0: Stage finished
21/11/23 11:30:10 @INFO @DAGScheduler@ Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 0.401361 s
21/11/23 11:30:10 @INFO @InMemoryFileIndex@ It took 546 ms to list leaf files for 50 paths.
21/11/23 11:30:10 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:10 @INFO @DAGScheduler@ Registering RDD 6 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0
21/11/23 11:30:10 @INFO @DAGScheduler@ Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:10 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:10 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:10 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:10 @INFO @DAGScheduler@ Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:10 @INFO @MemoryStore@ Block broadcast_2 stored as values in memory (estimated size 16.0 KiB, free 434.0 MiB)
21/11/23 11:30:10 @INFO @MemoryStore@ Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.0 MiB)
21/11/23 11:30:10 @INFO @BlockManagerInfo@ Added broadcast_2_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.3 MiB)
21/11/23 11:30:10 @INFO @SparkContext@ Created broadcast 2 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:10 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:10 @INFO @TaskSchedulerImpl@ Adding task set 1.0 with 10 tasks resource profile 0
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 0.0 in stage 1.0 (TID 50) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 1.0 in stage 1.0 (TID 51) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 2.0 in stage 1.0 (TID 52) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 3.0 in stage 1.0 (TID 53) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 4.0 in stage 1.0 (TID 54) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 5.0 in stage 1.0 (TID 55) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 6.0 in stage 1.0 (TID 56) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 7.0 in stage 1.0 (TID 57) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 8.0 in stage 1.0 (TID 58) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @TaskSetManager@ Starting task 9.0 in stage 1.0 (TID 59) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:10 @INFO @Executor@ Running task 0.0 in stage 1.0 (TID 50)
21/11/23 11:30:10 @INFO @Executor@ Running task 7.0 in stage 1.0 (TID 57)
21/11/23 11:30:10 @INFO @Executor@ Running task 5.0 in stage 1.0 (TID 55)
21/11/23 11:30:10 @INFO @Executor@ Running task 1.0 in stage 1.0 (TID 51)
21/11/23 11:30:10 @INFO @Executor@ Running task 4.0 in stage 1.0 (TID 54)
21/11/23 11:30:10 @INFO @Executor@ Running task 9.0 in stage 1.0 (TID 59)
21/11/23 11:30:10 @INFO @Executor@ Running task 8.0 in stage 1.0 (TID 58)
21/11/23 11:30:10 @INFO @Executor@ Running task 3.0 in stage 1.0 (TID 53)
21/11/23 11:30:10 @INFO @Executor@ Running task 6.0 in stage 1.0 (TID 56)
21/11/23 11:30:10 @INFO @Executor@ Running task 2.0 in stage 1.0 (TID 52)
21/11/23 11:30:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:10 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:11 @INFO @Executor@ Finished task 8.0 in stage 1.0 (TID 58). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 2.0 in stage 1.0 (TID 52). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 0.0 in stage 1.0 (TID 50). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 6.0 in stage 1.0 (TID 56). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 9.0 in stage 1.0 (TID 59). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 5.0 in stage 1.0 (TID 55). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 3.0 in stage 1.0 (TID 53). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 7.0 in stage 1.0 (TID 57). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 1.0 in stage 1.0 (TID 51). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 4.0 in stage 1.0 (TID 54). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 0.0 in stage 1.0 (TID 50) in 354 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 8.0 in stage 1.0 (TID 58) in 350 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 2.0 in stage 1.0 (TID 52) in 351 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 6.0 in stage 1.0 (TID 56) in 350 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 9.0 in stage 1.0 (TID 59) in 350 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 7.0 in stage 1.0 (TID 57) in 350 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 3.0 in stage 1.0 (TID 53) in 352 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 5.0 in stage 1.0 (TID 55) in 351 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 4.0 in stage 1.0 (TID 54) in 352 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 1.0 in stage 1.0 (TID 51) in 352 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:11 @INFO @TaskSchedulerImpl@ Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/11/23 11:30:11 @INFO @DAGScheduler@ ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.379 s
21/11/23 11:30:11 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:11 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:11 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:11 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:11 @INFO @CodeGenerator@ Code generated in 6.168917 ms
21/11/23 11:30:11 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:11 @INFO @DAGScheduler@ Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:11 @INFO @DAGScheduler@ Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:11 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 2)
21/11/23 11:30:11 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:11 @INFO @DAGScheduler@ Submitting ResultStage 3 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:11 @INFO @MemoryStore@ Block broadcast_3 stored as values in memory (estimated size 11.2 KiB, free 434.0 MiB)
21/11/23 11:30:11 @INFO @MemoryStore@ Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.0 MiB)
21/11/23 11:30:11 @INFO @BlockManagerInfo@ Added broadcast_3_piece0 in memory on 192.168.0.30:64662 (size: 5.6 KiB, free: 434.3 MiB)
21/11/23 11:30:11 @INFO @SparkContext@ Created broadcast 3 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:11 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:11 @INFO @TaskSchedulerImpl@ Adding task set 3.0 with 1 tasks resource profile 0
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 0.0 in stage 3.0 (TID 60) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @Executor@ Running task 0.0 in stage 3.0 (TID 60)
21/11/23 11:30:11 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:11 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 7 ms
21/11/23 11:30:11 @INFO @Executor@ Finished task 0.0 in stage 3.0 (TID 60). 2657 bytes result sent to driver
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 0.0 in stage 3.0 (TID 60) in 39 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:11 @INFO @TaskSchedulerImpl@ Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/11/23 11:30:11 @INFO @DAGScheduler@ ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.044 s
21/11/23 11:30:11 @INFO @DAGScheduler@ Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:11 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 3: Stage finished
21/11/23 11:30:11 @INFO @DAGScheduler@ Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.048776 s
21/11/23 11:30:11 @INFO @CodeGenerator@ Code generated in 5.134791 ms
21/11/23 11:30:11 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:11 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:11 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:11 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:11 @INFO @MemoryStore@ Block broadcast_4 stored as values in memory (estimated size 193.4 KiB, free 433.8 MiB)
21/11/23 11:30:11 @INFO @MemoryStore@ Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.8 MiB)
21/11/23 11:30:11 @INFO @BlockManagerInfo@ Added broadcast_4_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.3 MiB)
21/11/23 11:30:11 @INFO @SparkContext@ Created broadcast 4 from collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7
21/11/23 11:30:11 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:11 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:11 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:11 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:11 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:11 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:11 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:11 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:11 @INFO @InMemoryFileIndex@ It took 6 ms to list leaf files for 50 paths.
21/11/23 11:30:11 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:11 @INFO @DAGScheduler@ Registering RDD 13 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) as input to shuffle 1
21/11/23 11:30:11 @INFO @DAGScheduler@ Got map stage job 3 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) with 10 output partitions
21/11/23 11:30:11 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 4 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7)
21/11/23 11:30:11 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:11 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:11 @INFO @DAGScheduler@ Submitting ShuffleMapStage 4 (MapPartitionsRDD[13] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7), which has no missing parents
21/11/23 11:30:11 @INFO @MemoryStore@ Block broadcast_5 stored as values in memory (estimated size 16.0 KiB, free 433.8 MiB)
21/11/23 11:30:11 @INFO @MemoryStore@ Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.8 MiB)
21/11/23 11:30:11 @INFO @BlockManagerInfo@ Added broadcast_5_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.3 MiB)
21/11/23 11:30:11 @INFO @SparkContext@ Created broadcast 5 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:11 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[13] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:11 @INFO @TaskSchedulerImpl@ Adding task set 4.0 with 10 tasks resource profile 0
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 0.0 in stage 4.0 (TID 61) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 1.0 in stage 4.0 (TID 62) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 2.0 in stage 4.0 (TID 63) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 3.0 in stage 4.0 (TID 64) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 4.0 in stage 4.0 (TID 65) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 5.0 in stage 4.0 (TID 66) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 6.0 in stage 4.0 (TID 67) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 7.0 in stage 4.0 (TID 68) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 8.0 in stage 4.0 (TID 69) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 9.0 in stage 4.0 (TID 70) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @Executor@ Running task 5.0 in stage 4.0 (TID 66)
21/11/23 11:30:11 @INFO @Executor@ Running task 3.0 in stage 4.0 (TID 64)
21/11/23 11:30:11 @INFO @Executor@ Running task 0.0 in stage 4.0 (TID 61)
21/11/23 11:30:11 @INFO @Executor@ Running task 4.0 in stage 4.0 (TID 65)
21/11/23 11:30:11 @INFO @Executor@ Running task 1.0 in stage 4.0 (TID 62)
21/11/23 11:30:11 @INFO @Executor@ Running task 6.0 in stage 4.0 (TID 67)
21/11/23 11:30:11 @INFO @Executor@ Running task 9.0 in stage 4.0 (TID 70)
21/11/23 11:30:11 @INFO @Executor@ Running task 2.0 in stage 4.0 (TID 63)
21/11/23 11:30:11 @INFO @Executor@ Running task 7.0 in stage 4.0 (TID 68)
21/11/23 11:30:11 @INFO @Executor@ Running task 8.0 in stage 4.0 (TID 69)
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:11 @INFO @Executor@ Finished task 9.0 in stage 4.0 (TID 70). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 9.0 in stage 4.0 (TID 70) in 28 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:11 @INFO @Executor@ Finished task 5.0 in stage 4.0 (TID 66). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 0.0 in stage 4.0 (TID 61). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 0.0 in stage 4.0 (TID 61) in 32 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 5.0 in stage 4.0 (TID 66) in 31 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:11 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:11 @INFO @Executor@ Finished task 8.0 in stage 4.0 (TID 69). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 8.0 in stage 4.0 (TID 69) in 32 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:11 @INFO @Executor@ Finished task 6.0 in stage 4.0 (TID 67). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 6.0 in stage 4.0 (TID 67) in 33 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:11 @INFO @Executor@ Finished task 2.0 in stage 4.0 (TID 63). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @Executor@ Finished task 7.0 in stage 4.0 (TID 68). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 2.0 in stage 4.0 (TID 63) in 34 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 7.0 in stage 4.0 (TID 68) in 34 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:11 @INFO @Executor@ Finished task 4.0 in stage 4.0 (TID 65). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 4.0 in stage 4.0 (TID 65) in 35 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:11 @INFO @Executor@ Finished task 3.0 in stage 4.0 (TID 64). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 3.0 in stage 4.0 (TID 64) in 36 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:11 @INFO @Executor@ Finished task 1.0 in stage 4.0 (TID 62). 2124 bytes result sent to driver
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 1.0 in stage 4.0 (TID 62) in 37 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:11 @INFO @TaskSchedulerImpl@ Removed TaskSet 4.0, whose tasks have all completed, from pool 
21/11/23 11:30:11 @INFO @DAGScheduler@ ShuffleMapStage 4 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) finished in 0.042 s
21/11/23 11:30:11 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:11 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:11 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:11 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:11 @INFO @CodeGenerator@ Code generated in 5.593666 ms
21/11/23 11:30:11 @INFO @SparkContext@ Starting job: collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7
21/11/23 11:30:11 @INFO @DAGScheduler@ Got job 4 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) with 1 output partitions
21/11/23 11:30:11 @INFO @DAGScheduler@ Final stage: ResultStage 6 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7)
21/11/23 11:30:11 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 5)
21/11/23 11:30:11 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:11 @INFO @DAGScheduler@ Submitting ResultStage 6 (MapPartitionsRDD[16] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7), which has no missing parents
21/11/23 11:30:11 @INFO @MemoryStore@ Block broadcast_6 stored as values in memory (estimated size 11.0 KiB, free 433.7 MiB)
21/11/23 11:30:11 @INFO @MemoryStore@ Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.7 MiB)
21/11/23 11:30:11 @INFO @BlockManagerInfo@ Added broadcast_6_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.3 MiB)
21/11/23 11:30:11 @INFO @SparkContext@ Created broadcast 6 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:11 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[16] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:11 @INFO @TaskSchedulerImpl@ Adding task set 6.0 with 1 tasks resource profile 0
21/11/23 11:30:11 @INFO @TaskSetManager@ Starting task 0.0 in stage 6.0 (TID 71) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:11 @INFO @Executor@ Running task 0.0 in stage 6.0 (TID 71)
21/11/23 11:30:11 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:11 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:11 @INFO @Executor@ Finished task 0.0 in stage 6.0 (TID 71). 2648 bytes result sent to driver
21/11/23 11:30:11 @INFO @TaskSetManager@ Finished task 0.0 in stage 6.0 (TID 71) in 6 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:11 @INFO @TaskSchedulerImpl@ Removed TaskSet 6.0, whose tasks have all completed, from pool 
21/11/23 11:30:11 @INFO @DAGScheduler@ ResultStage 6 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) finished in 0.009 s
21/11/23 11:30:11 @INFO @DAGScheduler@ Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:11 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 6: Stage finished
21/11/23 11:30:11 @INFO @DAGScheduler@ Job 4 finished: collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7, took 0.010307 s
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:21 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:21 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:21 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:21 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:21 @INFO @MemoryStore@ Block broadcast_7 stored as values in memory (estimated size 193.4 KiB, free 433.6 MiB)
21/11/23 11:30:21 @INFO @MemoryStore@ Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.5 MiB)
21/11/23 11:30:21 @INFO @BlockManagerInfo@ Added broadcast_7_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.2 MiB)
21/11/23 11:30:21 @INFO @SparkContext@ Created broadcast 7 from showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:21 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:21 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:21 @INFO @HadoopFSUtils@ Listing leaf files and directories in parallel under 50 paths. The first several paths are: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県.
21/11/23 11:30:22 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:22 @INFO @DAGScheduler@ Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 50 output partitions
21/11/23 11:30:22 @INFO @DAGScheduler@ Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:22 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:22 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:22 @INFO @DAGScheduler@ Submitting ResultStage 7 (MapPartitionsRDD[19] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_8 stored as values in memory (estimated size 101.0 KiB, free 433.4 MiB)
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_8_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.4 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Added broadcast_8_piece0 in memory on 192.168.0.30:64662 (size: 36.2 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @SparkContext@ Created broadcast 8 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:22 @INFO @DAGScheduler@ Submitting 50 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Adding task set 7.0 with 50 tasks resource profile 0
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 0.0 in stage 7.0 (TID 72) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 1.0 in stage 7.0 (TID 73) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 2.0 in stage 7.0 (TID 74) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 4599 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 3.0 in stage 7.0 (TID 75) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 4614 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 4.0 in stage 7.0 (TID 76) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 5.0 in stage 7.0 (TID 77) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 4587 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 6.0 in stage 7.0 (TID 78) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 7.0 in stage 7.0 (TID 79) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 8.0 in stage 7.0 (TID 80) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 9.0 in stage 7.0 (TID 81) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 6.0 in stage 7.0 (TID 78)
21/11/23 11:30:22 @INFO @Executor@ Running task 1.0 in stage 7.0 (TID 73)
21/11/23 11:30:22 @INFO @Executor@ Running task 8.0 in stage 7.0 (TID 80)
21/11/23 11:30:22 @INFO @Executor@ Running task 7.0 in stage 7.0 (TID 79)
21/11/23 11:30:22 @INFO @Executor@ Running task 3.0 in stage 7.0 (TID 75)
21/11/23 11:30:22 @INFO @Executor@ Running task 4.0 in stage 7.0 (TID 76)
21/11/23 11:30:22 @INFO @Executor@ Running task 5.0 in stage 7.0 (TID 77)
21/11/23 11:30:22 @INFO @Executor@ Running task 2.0 in stage 7.0 (TID 74)
21/11/23 11:30:22 @INFO @Executor@ Running task 9.0 in stage 7.0 (TID 81)
21/11/23 11:30:22 @INFO @Executor@ Running task 0.0 in stage 7.0 (TID 72)
21/11/23 11:30:22 @INFO @Executor@ Finished task 1.0 in stage 7.0 (TID 73). 2027 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 10.0 in stage 7.0 (TID 82) (192.168.0.30, executor driver, partition 10, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Finished task 6.0 in stage 7.0 (TID 78). 2027 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 11.0 in stage 7.0 (TID 83) (192.168.0.30, executor driver, partition 11, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 1.0 in stage 7.0 (TID 73) in 22 ms on 192.168.0.30 (executor driver) (1/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 3.0 in stage 7.0 (TID 75). 2078 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Running task 10.0 in stage 7.0 (TID 82)
21/11/23 11:30:22 @INFO @Executor@ Running task 11.0 in stage 7.0 (TID 83)
21/11/23 11:30:22 @INFO @Executor@ Finished task 4.0 in stage 7.0 (TID 76). 2027 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 8.0 in stage 7.0 (TID 80). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 6.0 in stage 7.0 (TID 78) in 22 ms on 192.168.0.30 (executor driver) (2/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 7.0 in stage 7.0 (TID 79). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 0.0 in stage 7.0 (TID 72). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 12.0 in stage 7.0 (TID 84) (192.168.0.30, executor driver, partition 12, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 13.0 in stage 7.0 (TID 85) (192.168.0.30, executor driver, partition 13, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 12.0 in stage 7.0 (TID 84)
21/11/23 11:30:22 @INFO @Executor@ Finished task 2.0 in stage 7.0 (TID 74). 2045 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 14.0 in stage 7.0 (TID 86) (192.168.0.30, executor driver, partition 14, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Finished task 9.0 in stage 7.0 (TID 81). 1990 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 15.0 in stage 7.0 (TID 87) (192.168.0.30, executor driver, partition 15, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 13.0 in stage 7.0 (TID 85)
21/11/23 11:30:22 @INFO @Executor@ Finished task 5.0 in stage 7.0 (TID 77). 2021 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Running task 14.0 in stage 7.0 (TID 86)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 16.0 in stage 7.0 (TID 88) (192.168.0.30, executor driver, partition 16, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 15.0 in stage 7.0 (TID 87)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 17.0 in stage 7.0 (TID 89) (192.168.0.30, executor driver, partition 17, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 18.0 in stage 7.0 (TID 90) (192.168.0.30, executor driver, partition 18, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 16.0 in stage 7.0 (TID 88)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 19.0 in stage 7.0 (TID 91) (192.168.0.30, executor driver, partition 19, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 3.0 in stage 7.0 (TID 75) in 26 ms on 192.168.0.30 (executor driver) (3/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 17.0 in stage 7.0 (TID 89)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 0.0 in stage 7.0 (TID 72) in 28 ms on 192.168.0.30 (executor driver) (4/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 18.0 in stage 7.0 (TID 90)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 2.0 in stage 7.0 (TID 74) in 27 ms on 192.168.0.30 (executor driver) (5/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 9.0 in stage 7.0 (TID 81) in 27 ms on 192.168.0.30 (executor driver) (6/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 5.0 in stage 7.0 (TID 77) in 28 ms on 192.168.0.30 (executor driver) (7/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 19.0 in stage 7.0 (TID 91)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 8.0 in stage 7.0 (TID 80) in 28 ms on 192.168.0.30 (executor driver) (8/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 4.0 in stage 7.0 (TID 76) in 29 ms on 192.168.0.30 (executor driver) (9/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 7.0 in stage 7.0 (TID 79) in 30 ms on 192.168.0.30 (executor driver) (10/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 11.0 in stage 7.0 (TID 83). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 20.0 in stage 7.0 (TID 92) (192.168.0.30, executor driver, partition 20, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 11.0 in stage 7.0 (TID 83) in 18 ms on 192.168.0.30 (executor driver) (11/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 20.0 in stage 7.0 (TID 92)
21/11/23 11:30:22 @INFO @Executor@ Finished task 12.0 in stage 7.0 (TID 84). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 21.0 in stage 7.0 (TID 93) (192.168.0.30, executor driver, partition 21, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 21.0 in stage 7.0 (TID 93)
21/11/23 11:30:22 @INFO @Executor@ Finished task 10.0 in stage 7.0 (TID 82). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 12.0 in stage 7.0 (TID 84) in 20 ms on 192.168.0.30 (executor driver) (12/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 10.0 in stage 7.0 (TID 82) in 23 ms on 192.168.0.30 (executor driver) (13/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 22.0 in stage 7.0 (TID 94) (192.168.0.30, executor driver, partition 22, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Finished task 16.0 in stage 7.0 (TID 88). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 23.0 in stage 7.0 (TID 95) (192.168.0.30, executor driver, partition 23, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 22.0 in stage 7.0 (TID 94)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 16.0 in stage 7.0 (TID 88) in 20 ms on 192.168.0.30 (executor driver) (14/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 23.0 in stage 7.0 (TID 95)
21/11/23 11:30:22 @INFO @Executor@ Finished task 19.0 in stage 7.0 (TID 91). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 24.0 in stage 7.0 (TID 96) (192.168.0.30, executor driver, partition 24, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 19.0 in stage 7.0 (TID 91) in 21 ms on 192.168.0.30 (executor driver) (15/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 24.0 in stage 7.0 (TID 96)
21/11/23 11:30:22 @INFO @Executor@ Finished task 15.0 in stage 7.0 (TID 87). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 13.0 in stage 7.0 (TID 85). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 25.0 in stage 7.0 (TID 97) (192.168.0.30, executor driver, partition 25, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Finished task 17.0 in stage 7.0 (TID 89). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Running task 25.0 in stage 7.0 (TID 97)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 26.0 in stage 7.0 (TID 98) (192.168.0.30, executor driver, partition 26, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Finished task 14.0 in stage 7.0 (TID 86). 2027 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Running task 26.0 in stage 7.0 (TID 98)
21/11/23 11:30:22 @INFO @Executor@ Finished task 18.0 in stage 7.0 (TID 90). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 27.0 in stage 7.0 (TID 99) (192.168.0.30, executor driver, partition 27, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 13.0 in stage 7.0 (TID 85) in 26 ms on 192.168.0.30 (executor driver) (16/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 27.0 in stage 7.0 (TID 99)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 15.0 in stage 7.0 (TID 87) in 25 ms on 192.168.0.30 (executor driver) (17/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 17.0 in stage 7.0 (TID 89) in 25 ms on 192.168.0.30 (executor driver) (18/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 28.0 in stage 7.0 (TID 100) (192.168.0.30, executor driver, partition 28, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 29.0 in stage 7.0 (TID 101) (192.168.0.30, executor driver, partition 29, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 14.0 in stage 7.0 (TID 86) in 27 ms on 192.168.0.30 (executor driver) (19/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 18.0 in stage 7.0 (TID 90) in 26 ms on 192.168.0.30 (executor driver) (20/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 28.0 in stage 7.0 (TID 100)
21/11/23 11:30:22 @INFO @Executor@ Running task 29.0 in stage 7.0 (TID 101)
21/11/23 11:30:22 @INFO @Executor@ Finished task 20.0 in stage 7.0 (TID 92). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 21.0 in stage 7.0 (TID 93). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 30.0 in stage 7.0 (TID 102) (192.168.0.30, executor driver, partition 30, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Finished task 22.0 in stage 7.0 (TID 94). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Running task 30.0 in stage 7.0 (TID 102)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 31.0 in stage 7.0 (TID 103) (192.168.0.30, executor driver, partition 31, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 32.0 in stage 7.0 (TID 104) (192.168.0.30, executor driver, partition 32, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 21.0 in stage 7.0 (TID 93) in 21 ms on 192.168.0.30 (executor driver) (21/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 22.0 in stage 7.0 (TID 94) in 20 ms on 192.168.0.30 (executor driver) (22/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 23.0 in stage 7.0 (TID 95). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Running task 31.0 in stage 7.0 (TID 103)
21/11/23 11:30:22 @INFO @Executor@ Running task 32.0 in stage 7.0 (TID 104)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 20.0 in stage 7.0 (TID 92) in 25 ms on 192.168.0.30 (executor driver) (23/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 24.0 in stage 7.0 (TID 96). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 33.0 in stage 7.0 (TID 105) (192.168.0.30, executor driver, partition 33, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 34.0 in stage 7.0 (TID 106) (192.168.0.30, executor driver, partition 34, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Finished task 26.0 in stage 7.0 (TID 98). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 23.0 in stage 7.0 (TID 95) in 21 ms on 192.168.0.30 (executor driver) (24/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 24.0 in stage 7.0 (TID 96) in 20 ms on 192.168.0.30 (executor driver) (25/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 25.0 in stage 7.0 (TID 97). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Running task 33.0 in stage 7.0 (TID 105)
21/11/23 11:30:22 @INFO @Executor@ Running task 34.0 in stage 7.0 (TID 106)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 35.0 in stage 7.0 (TID 107) (192.168.0.30, executor driver, partition 35, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 36.0 in stage 7.0 (TID 108) (192.168.0.30, executor driver, partition 36, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 35.0 in stage 7.0 (TID 107)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 26.0 in stage 7.0 (TID 98) in 20 ms on 192.168.0.30 (executor driver) (26/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 36.0 in stage 7.0 (TID 108)
21/11/23 11:30:22 @INFO @Executor@ Finished task 27.0 in stage 7.0 (TID 99). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 37.0 in stage 7.0 (TID 109) (192.168.0.30, executor driver, partition 37, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 27.0 in stage 7.0 (TID 99) in 20 ms on 192.168.0.30 (executor driver) (27/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 25.0 in stage 7.0 (TID 97) in 22 ms on 192.168.0.30 (executor driver) (28/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 37.0 in stage 7.0 (TID 109)
21/11/23 11:30:22 @INFO @Executor@ Finished task 29.0 in stage 7.0 (TID 101). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 38.0 in stage 7.0 (TID 110) (192.168.0.30, executor driver, partition 38, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 38.0 in stage 7.0 (TID 110)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 29.0 in stage 7.0 (TID 101) in 20 ms on 192.168.0.30 (executor driver) (29/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 28.0 in stage 7.0 (TID 100). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 39.0 in stage 7.0 (TID 111) (192.168.0.30, executor driver, partition 39, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 28.0 in stage 7.0 (TID 100) in 22 ms on 192.168.0.30 (executor driver) (30/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 39.0 in stage 7.0 (TID 111)
21/11/23 11:30:22 @INFO @Executor@ Finished task 32.0 in stage 7.0 (TID 104). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 31.0 in stage 7.0 (TID 103). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 40.0 in stage 7.0 (TID 112) (192.168.0.30, executor driver, partition 40, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 41.0 in stage 7.0 (TID 113) (192.168.0.30, executor driver, partition 41, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 31.0 in stage 7.0 (TID 103) in 19 ms on 192.168.0.30 (executor driver) (31/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 40.0 in stage 7.0 (TID 112)
21/11/23 11:30:22 @INFO @Executor@ Running task 41.0 in stage 7.0 (TID 113)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 32.0 in stage 7.0 (TID 104) in 20 ms on 192.168.0.30 (executor driver) (32/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 34.0 in stage 7.0 (TID 106). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 42.0 in stage 7.0 (TID 114) (192.168.0.30, executor driver, partition 42, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Finished task 33.0 in stage 7.0 (TID 105). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 34.0 in stage 7.0 (TID 106) in 20 ms on 192.168.0.30 (executor driver) (33/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 42.0 in stage 7.0 (TID 114)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 43.0 in stage 7.0 (TID 115) (192.168.0.30, executor driver, partition 43, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 33.0 in stage 7.0 (TID 105) in 20 ms on 192.168.0.30 (executor driver) (34/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 43.0 in stage 7.0 (TID 115)
21/11/23 11:30:22 @INFO @Executor@ Finished task 30.0 in stage 7.0 (TID 102). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 38.0 in stage 7.0 (TID 110). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 44.0 in stage 7.0 (TID 116) (192.168.0.30, executor driver, partition 44, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 45.0 in stage 7.0 (TID 117) (192.168.0.30, executor driver, partition 45, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 44.0 in stage 7.0 (TID 116)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 38.0 in stage 7.0 (TID 110) in 18 ms on 192.168.0.30 (executor driver) (35/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 45.0 in stage 7.0 (TID 117)
21/11/23 11:30:22 @INFO @Executor@ Finished task 36.0 in stage 7.0 (TID 108). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 30.0 in stage 7.0 (TID 102) in 26 ms on 192.168.0.30 (executor driver) (36/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 46.0 in stage 7.0 (TID 118) (192.168.0.30, executor driver, partition 46, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Finished task 37.0 in stage 7.0 (TID 109). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Running task 46.0 in stage 7.0 (TID 118)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 36.0 in stage 7.0 (TID 108) in 23 ms on 192.168.0.30 (executor driver) (37/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 47.0 in stage 7.0 (TID 119) (192.168.0.30, executor driver, partition 47, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 37.0 in stage 7.0 (TID 109) in 22 ms on 192.168.0.30 (executor driver) (38/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 35.0 in stage 7.0 (TID 107). 2033 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Running task 47.0 in stage 7.0 (TID 119)
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 48.0 in stage 7.0 (TID 120) (192.168.0.30, executor driver, partition 48, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 35.0 in stage 7.0 (TID 107) in 26 ms on 192.168.0.30 (executor driver) (39/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 48.0 in stage 7.0 (TID 120)
21/11/23 11:30:22 @INFO @Executor@ Finished task 39.0 in stage 7.0 (TID 111). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 49.0 in stage 7.0 (TID 121) (192.168.0.30, executor driver, partition 49, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 39.0 in stage 7.0 (TID 111) in 22 ms on 192.168.0.30 (executor driver) (40/50)
21/11/23 11:30:22 @INFO @Executor@ Running task 49.0 in stage 7.0 (TID 121)
21/11/23 11:30:22 @INFO @Executor@ Finished task 40.0 in stage 7.0 (TID 112). 2027 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 40.0 in stage 7.0 (TID 112) in 20 ms on 192.168.0.30 (executor driver) (41/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 42.0 in stage 7.0 (TID 114). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 43.0 in stage 7.0 (TID 115). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 44.0 in stage 7.0 (TID 116). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 47.0 in stage 7.0 (TID 119). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 41.0 in stage 7.0 (TID 113). 2027 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 42.0 in stage 7.0 (TID 114) in 22 ms on 192.168.0.30 (executor driver) (42/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 44.0 in stage 7.0 (TID 116) in 20 ms on 192.168.0.30 (executor driver) (43/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 47.0 in stage 7.0 (TID 119) in 18 ms on 192.168.0.30 (executor driver) (44/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 41.0 in stage 7.0 (TID 113) in 26 ms on 192.168.0.30 (executor driver) (45/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 43.0 in stage 7.0 (TID 115) in 23 ms on 192.168.0.30 (executor driver) (46/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 45.0 in stage 7.0 (TID 117). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 45.0 in stage 7.0 (TID 117) in 21 ms on 192.168.0.30 (executor driver) (47/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 49.0 in stage 7.0 (TID 121). 1990 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 49.0 in stage 7.0 (TID 121) in 16 ms on 192.168.0.30 (executor driver) (48/50)
21/11/23 11:30:22 @INFO @Executor@ Finished task 46.0 in stage 7.0 (TID 118). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 48.0 in stage 7.0 (TID 120). 1984 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 46.0 in stage 7.0 (TID 118) in 22 ms on 192.168.0.30 (executor driver) (49/50)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 48.0 in stage 7.0 (TID 120) in 19 ms on 192.168.0.30 (executor driver) (50/50)
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Removed TaskSet 7.0, whose tasks have all completed, from pool 
21/11/23 11:30:22 @INFO @DAGScheduler@ ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 0.123 s
21/11/23 11:30:22 @INFO @DAGScheduler@ Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 7: Stage finished
21/11/23 11:30:22 @INFO @DAGScheduler@ Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 0.124604 s
21/11/23 11:30:22 @INFO @InMemoryFileIndex@ It took 166 ms to list leaf files for 50 paths.
21/11/23 11:30:22 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:22 @INFO @DAGScheduler@ Registering RDD 23 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 2
21/11/23 11:30:22 @INFO @DAGScheduler@ Got map stage job 6 (showString at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:22 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 8 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:22 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:22 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:22 @INFO @DAGScheduler@ Submitting ShuffleMapStage 8 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_9 stored as values in memory (estimated size 16.0 KiB, free 433.4 MiB)
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.4 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Added broadcast_9_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @SparkContext@ Created broadcast 9 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:22 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Adding task set 8.0 with 10 tasks resource profile 0
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 0.0 in stage 8.0 (TID 122) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 1.0 in stage 8.0 (TID 123) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 2.0 in stage 8.0 (TID 124) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 3.0 in stage 8.0 (TID 125) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 4.0 in stage 8.0 (TID 126) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 5.0 in stage 8.0 (TID 127) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 6.0 in stage 8.0 (TID 128) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 7.0 in stage 8.0 (TID 129) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 8.0 in stage 8.0 (TID 130) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 9.0 in stage 8.0 (TID 131) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 0.0 in stage 8.0 (TID 122)
21/11/23 11:30:22 @INFO @Executor@ Running task 4.0 in stage 8.0 (TID 126)
21/11/23 11:30:22 @INFO @Executor@ Running task 1.0 in stage 8.0 (TID 123)
21/11/23 11:30:22 @INFO @Executor@ Running task 7.0 in stage 8.0 (TID 129)
21/11/23 11:30:22 @INFO @Executor@ Running task 9.0 in stage 8.0 (TID 131)
21/11/23 11:30:22 @INFO @Executor@ Running task 5.0 in stage 8.0 (TID 127)
21/11/23 11:30:22 @INFO @Executor@ Running task 8.0 in stage 8.0 (TID 130)
21/11/23 11:30:22 @INFO @Executor@ Running task 2.0 in stage 8.0 (TID 124)
21/11/23 11:30:22 @INFO @Executor@ Running task 3.0 in stage 8.0 (TID 125)
21/11/23 11:30:22 @INFO @Executor@ Running task 6.0 in stage 8.0 (TID 128)
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:22 @INFO @Executor@ Finished task 9.0 in stage 8.0 (TID 131). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:22 @INFO @Executor@ Finished task 0.0 in stage 8.0 (TID 122). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 9.0 in stage 8.0 (TID 131) in 22 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 0.0 in stage 8.0 (TID 122) in 24 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 4.0 in stage 8.0 (TID 126). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 4.0 in stage 8.0 (TID 126) in 25 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 7.0 in stage 8.0 (TID 129). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:22 @INFO @Executor@ Finished task 6.0 in stage 8.0 (TID 128). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 7.0 in stage 8.0 (TID 129) in 27 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 1.0 in stage 8.0 (TID 123). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 6.0 in stage 8.0 (TID 128) in 29 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 8.0 in stage 8.0 (TID 130). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @Executor@ Finished task 5.0 in stage 8.0 (TID 127). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 1.0 in stage 8.0 (TID 123) in 30 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 5.0 in stage 8.0 (TID 127) in 30 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 8.0 in stage 8.0 (TID 130) in 30 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 2.0 in stage 8.0 (TID 124). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 2.0 in stage 8.0 (TID 124) in 30 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 3.0 in stage 8.0 (TID 125). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 3.0 in stage 8.0 (TID 125) in 31 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Removed TaskSet 8.0, whose tasks have all completed, from pool 
21/11/23 11:30:22 @INFO @DAGScheduler@ ShuffleMapStage 8 (showString at NativeMethodAccessorImpl.java:0) finished in 0.035 s
21/11/23 11:30:22 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:22 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:22 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:22 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:22 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:22 @INFO @DAGScheduler@ Got job 7 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:22 @INFO @DAGScheduler@ Final stage: ResultStage 10 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:22 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 9)
21/11/23 11:30:22 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:22 @INFO @DAGScheduler@ Submitting ResultStage 10 (MapPartitionsRDD[26] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_10 stored as values in memory (estimated size 11.2 KiB, free 433.4 MiB)
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 433.3 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Added broadcast_10_piece0 in memory on 192.168.0.30:64662 (size: 5.6 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @SparkContext@ Created broadcast 10 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:22 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[26] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Adding task set 10.0 with 1 tasks resource profile 0
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 0.0 in stage 10.0 (TID 132) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 0.0 in stage 10.0 (TID 132)
21/11/23 11:30:22 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:22 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:22 @INFO @Executor@ Finished task 0.0 in stage 10.0 (TID 132). 2614 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 0.0 in stage 10.0 (TID 132) in 3 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Removed TaskSet 10.0, whose tasks have all completed, from pool 
21/11/23 11:30:22 @INFO @DAGScheduler@ ResultStage 10 (showString at NativeMethodAccessorImpl.java:0) finished in 0.006 s
21/11/23 11:30:22 @INFO @DAGScheduler@ Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 10: Stage finished
21/11/23 11:30:22 @INFO @DAGScheduler@ Job 7 finished: showString at NativeMethodAccessorImpl.java:0, took 0.007324 s
21/11/23 11:30:22 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:22 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:22 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:22 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_11 stored as values in memory (estimated size 193.4 KiB, free 433.2 MiB)
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_11_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.1 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Added broadcast_11_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @SparkContext@ Created broadcast 11 from collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7
21/11/23 11:30:22 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:22 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:22 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:22 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:22 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:22 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:22 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:22 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:22 @INFO @InMemoryFileIndex@ It took 3 ms to list leaf files for 50 paths.
21/11/23 11:30:22 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:22 @INFO @DAGScheduler@ Registering RDD 30 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) as input to shuffle 3
21/11/23 11:30:22 @INFO @DAGScheduler@ Got map stage job 8 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) with 10 output partitions
21/11/23 11:30:22 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 11 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7)
21/11/23 11:30:22 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:22 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:22 @INFO @DAGScheduler@ Submitting ShuffleMapStage 11 (MapPartitionsRDD[30] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7), which has no missing parents
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_12 stored as values in memory (estimated size 16.0 KiB, free 433.1 MiB)
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_12_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.1 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Added broadcast_12_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @SparkContext@ Created broadcast 12 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:22 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[30] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Adding task set 11.0 with 10 tasks resource profile 0
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 0.0 in stage 11.0 (TID 133) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 1.0 in stage 11.0 (TID 134) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 2.0 in stage 11.0 (TID 135) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 3.0 in stage 11.0 (TID 136) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 4.0 in stage 11.0 (TID 137) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 5.0 in stage 11.0 (TID 138) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 6.0 in stage 11.0 (TID 139) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 7.0 in stage 11.0 (TID 140) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 8.0 in stage 11.0 (TID 141) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 9.0 in stage 11.0 (TID 142) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 0.0 in stage 11.0 (TID 133)
21/11/23 11:30:22 @INFO @Executor@ Running task 2.0 in stage 11.0 (TID 135)
21/11/23 11:30:22 @INFO @Executor@ Running task 6.0 in stage 11.0 (TID 139)
21/11/23 11:30:22 @INFO @Executor@ Running task 3.0 in stage 11.0 (TID 136)
21/11/23 11:30:22 @INFO @Executor@ Running task 4.0 in stage 11.0 (TID 137)
21/11/23 11:30:22 @INFO @Executor@ Running task 1.0 in stage 11.0 (TID 134)
21/11/23 11:30:22 @INFO @Executor@ Running task 7.0 in stage 11.0 (TID 140)
21/11/23 11:30:22 @INFO @Executor@ Running task 9.0 in stage 11.0 (TID 142)
21/11/23 11:30:22 @INFO @Executor@ Running task 5.0 in stage 11.0 (TID 138)
21/11/23 11:30:22 @INFO @Executor@ Running task 8.0 in stage 11.0 (TID 141)
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:22 @INFO @Executor@ Finished task 7.0 in stage 11.0 (TID 140). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 7.0 in stage 11.0 (TID 140) in 24 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 0.0 in stage 11.0 (TID 133). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 0.0 in stage 11.0 (TID 133) in 26 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 8.0 in stage 11.0 (TID 141). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 8.0 in stage 11.0 (TID 141) in 26 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 3.0 in stage 11.0 (TID 136). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 3.0 in stage 11.0 (TID 136) in 28 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 6.0 in stage 11.0 (TID 139). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 6.0 in stage 11.0 (TID 139) in 30 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 2.0 in stage 11.0 (TID 135). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 2.0 in stage 11.0 (TID 135) in 30 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 9.0 in stage 11.0 (TID 142). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 9.0 in stage 11.0 (TID 142) in 30 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 4.0 in stage 11.0 (TID 137). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 4.0 in stage 11.0 (TID 137) in 33 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 5.0 in stage 11.0 (TID 138). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 5.0 in stage 11.0 (TID 138) in 33 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:22 @INFO @Executor@ Finished task 1.0 in stage 11.0 (TID 134). 2124 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 1.0 in stage 11.0 (TID 134) in 34 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Removed TaskSet 11.0, whose tasks have all completed, from pool 
21/11/23 11:30:22 @INFO @DAGScheduler@ ShuffleMapStage 11 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) finished in 0.038 s
21/11/23 11:30:22 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:22 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:22 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:22 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:22 @INFO @SparkContext@ Starting job: collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7
21/11/23 11:30:22 @INFO @DAGScheduler@ Got job 9 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) with 1 output partitions
21/11/23 11:30:22 @INFO @DAGScheduler@ Final stage: ResultStage 13 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7)
21/11/23 11:30:22 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 12)
21/11/23 11:30:22 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:22 @INFO @DAGScheduler@ Submitting ResultStage 13 (MapPartitionsRDD[33] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7), which has no missing parents
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_13 stored as values in memory (estimated size 11.0 KiB, free 433.1 MiB)
21/11/23 11:30:22 @INFO @MemoryStore@ Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.1 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Added broadcast_13_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.1 MiB)
21/11/23 11:30:22 @INFO @SparkContext@ Created broadcast 13 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:22 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[33] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Adding task set 13.0 with 1 tasks resource profile 0
21/11/23 11:30:22 @INFO @TaskSetManager@ Starting task 0.0 in stage 13.0 (TID 143) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:22 @INFO @Executor@ Running task 0.0 in stage 13.0 (TID 143)
21/11/23 11:30:22 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:22 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:22 @INFO @Executor@ Finished task 0.0 in stage 13.0 (TID 143). 2605 bytes result sent to driver
21/11/23 11:30:22 @INFO @TaskSetManager@ Finished task 0.0 in stage 13.0 (TID 143) in 4 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Removed TaskSet 13.0, whose tasks have all completed, from pool 
21/11/23 11:30:22 @INFO @DAGScheduler@ ResultStage 13 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7) finished in 0.009 s
21/11/23 11:30:22 @INFO @DAGScheduler@ Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:22 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 13: Stage finished
21/11/23 11:30:22 @INFO @DAGScheduler@ Job 9 finished: collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/307700952.py:7, took 0.012560 s
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_1_piece0 on 192.168.0.30:64662 in memory (size: 36.2 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_5_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_9_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_12_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_6_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_10_piece0 on 192.168.0.30:64662 in memory (size: 5.6 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_8_piece0 on 192.168.0.30:64662 in memory (size: 36.2 KiB, free: 434.2 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_7_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.3 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_0_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.3 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_2_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.3 MiB)
21/11/23 11:30:22 @INFO @BlockManagerInfo@ Removed broadcast_3_piece0 on 192.168.0.30:64662 in memory (size: 5.6 KiB, free: 434.3 MiB)
21/11/23 11:30:23 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:23 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:23 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:23 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:24 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:24 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:24 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:24 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:24 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:24 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:24 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:24 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:24 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:24 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:24 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:24 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:24 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:24 @INFO @FileSourceStrategy@ Output Data Schema: struct<jinko_male: string, jinko_femail: string>
21/11/23 11:30:24 @INFO @CodeGenerator@ Code generated in 14.119542 ms
21/11/23 11:30:24 @INFO @MemoryStore@ Block broadcast_14 stored as values in memory (estimated size 193.7 KiB, free 433.8 MiB)
21/11/23 11:30:24 @INFO @MemoryStore@ Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 433.7 MiB)
21/11/23 11:30:24 @INFO @BlockManagerInfo@ Added broadcast_14_piece0 in memory on 192.168.0.30:64662 (size: 34.1 KiB, free: 434.3 MiB)
21/11/23 11:30:24 @INFO @SparkContext@ Created broadcast 14 from showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:24 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:24 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:24 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:24 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:24 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:24 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:24 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:24 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:24 @INFO @HadoopFSUtils@ Listing leaf files and directories in parallel under 50 paths. The first several paths are: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県.
21/11/23 11:30:24 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:24 @INFO @DAGScheduler@ Got job 10 (showString at NativeMethodAccessorImpl.java:0) with 50 output partitions
21/11/23 11:30:24 @INFO @DAGScheduler@ Final stage: ResultStage 14 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:24 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:24 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:24 @INFO @DAGScheduler@ Submitting ResultStage 14 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:24 @INFO @MemoryStore@ Block broadcast_15 stored as values in memory (estimated size 101.0 KiB, free 433.6 MiB)
21/11/23 11:30:24 @INFO @MemoryStore@ Block broadcast_15_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.6 MiB)
21/11/23 11:30:24 @INFO @BlockManagerInfo@ Added broadcast_15_piece0 in memory on 192.168.0.30:64662 (size: 36.2 KiB, free: 434.3 MiB)
21/11/23 11:30:24 @INFO @SparkContext@ Created broadcast 15 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:24 @INFO @DAGScheduler@ Submitting 50 missing tasks from ResultStage 14 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/11/23 11:30:24 @INFO @TaskSchedulerImpl@ Adding task set 14.0 with 50 tasks resource profile 0
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 0.0 in stage 14.0 (TID 144) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 1.0 in stage 14.0 (TID 145) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 2.0 in stage 14.0 (TID 146) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 4599 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 3.0 in stage 14.0 (TID 147) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 4614 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 4.0 in stage 14.0 (TID 148) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 5.0 in stage 14.0 (TID 149) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 4587 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 6.0 in stage 14.0 (TID 150) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 7.0 in stage 14.0 (TID 151) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 8.0 in stage 14.0 (TID 152) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 9.0 in stage 14.0 (TID 153) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Running task 2.0 in stage 14.0 (TID 146)
21/11/23 11:30:24 @INFO @Executor@ Running task 5.0 in stage 14.0 (TID 149)
21/11/23 11:30:24 @INFO @Executor@ Running task 6.0 in stage 14.0 (TID 150)
21/11/23 11:30:24 @INFO @Executor@ Running task 4.0 in stage 14.0 (TID 148)
21/11/23 11:30:24 @INFO @Executor@ Running task 7.0 in stage 14.0 (TID 151)
21/11/23 11:30:24 @INFO @Executor@ Running task 9.0 in stage 14.0 (TID 153)
21/11/23 11:30:24 @INFO @Executor@ Running task 3.0 in stage 14.0 (TID 147)
21/11/23 11:30:24 @INFO @Executor@ Running task 8.0 in stage 14.0 (TID 152)
21/11/23 11:30:24 @INFO @Executor@ Running task 1.0 in stage 14.0 (TID 145)
21/11/23 11:30:24 @INFO @Executor@ Running task 0.0 in stage 14.0 (TID 144)
21/11/23 11:30:24 @INFO @Executor@ Finished task 5.0 in stage 14.0 (TID 149). 1978 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 3.0 in stage 14.0 (TID 147). 2078 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 1.0 in stage 14.0 (TID 145). 2027 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 10.0 in stage 14.0 (TID 154) (192.168.0.30, executor driver, partition 10, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 11.0 in stage 14.0 (TID 155) (192.168.0.30, executor driver, partition 11, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 9.0 in stage 14.0 (TID 153). 1990 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 6.0 in stage 14.0 (TID 150). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 12.0 in stage 14.0 (TID 156) (192.168.0.30, executor driver, partition 12, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 0.0 in stage 14.0 (TID 144). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Running task 10.0 in stage 14.0 (TID 154)
21/11/23 11:30:24 @INFO @Executor@ Finished task 7.0 in stage 14.0 (TID 151). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 13.0 in stage 14.0 (TID 157) (192.168.0.30, executor driver, partition 13, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Running task 12.0 in stage 14.0 (TID 156)
21/11/23 11:30:24 @INFO @Executor@ Running task 11.0 in stage 14.0 (TID 155)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 14.0 in stage 14.0 (TID 158) (192.168.0.30, executor driver, partition 14, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 2.0 in stage 14.0 (TID 146). 2002 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 8.0 in stage 14.0 (TID 152). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Running task 13.0 in stage 14.0 (TID 157)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 15.0 in stage 14.0 (TID 159) (192.168.0.30, executor driver, partition 15, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 4.0 in stage 14.0 (TID 148). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 16.0 in stage 14.0 (TID 160) (192.168.0.30, executor driver, partition 16, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 5.0 in stage 14.0 (TID 149) in 15 ms on 192.168.0.30 (executor driver) (1/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 14.0 in stage 14.0 (TID 158)
21/11/23 11:30:24 @INFO @Executor@ Running task 15.0 in stage 14.0 (TID 159)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 9.0 in stage 14.0 (TID 153) in 14 ms on 192.168.0.30 (executor driver) (2/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 16.0 in stage 14.0 (TID 160)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 17.0 in stage 14.0 (TID 161) (192.168.0.30, executor driver, partition 17, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 18.0 in stage 14.0 (TID 162) (192.168.0.30, executor driver, partition 18, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 19.0 in stage 14.0 (TID 163) (192.168.0.30, executor driver, partition 19, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 0.0 in stage 14.0 (TID 144) in 16 ms on 192.168.0.30 (executor driver) (3/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 17.0 in stage 14.0 (TID 161)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 7.0 in stage 14.0 (TID 151) in 15 ms on 192.168.0.30 (executor driver) (4/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 2.0 in stage 14.0 (TID 146) in 16 ms on 192.168.0.30 (executor driver) (5/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 18.0 in stage 14.0 (TID 162)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 8.0 in stage 14.0 (TID 152) in 15 ms on 192.168.0.30 (executor driver) (6/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 19.0 in stage 14.0 (TID 163)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 4.0 in stage 14.0 (TID 148) in 16 ms on 192.168.0.30 (executor driver) (7/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 1.0 in stage 14.0 (TID 145) in 17 ms on 192.168.0.30 (executor driver) (8/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 3.0 in stage 14.0 (TID 147) in 17 ms on 192.168.0.30 (executor driver) (9/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 6.0 in stage 14.0 (TID 150) in 16 ms on 192.168.0.30 (executor driver) (10/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 10.0 in stage 14.0 (TID 154). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 11.0 in stage 14.0 (TID 155). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 12.0 in stage 14.0 (TID 156). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 20.0 in stage 14.0 (TID 164) (192.168.0.30, executor driver, partition 20, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 21.0 in stage 14.0 (TID 165) (192.168.0.30, executor driver, partition 21, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 22.0 in stage 14.0 (TID 166) (192.168.0.30, executor driver, partition 22, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 15.0 in stage 14.0 (TID 159). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 11.0 in stage 14.0 (TID 155) in 13 ms on 192.168.0.30 (executor driver) (11/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 12.0 in stage 14.0 (TID 156) in 13 ms on 192.168.0.30 (executor driver) (12/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 20.0 in stage 14.0 (TID 164)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 10.0 in stage 14.0 (TID 154) in 13 ms on 192.168.0.30 (executor driver) (13/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 19.0 in stage 14.0 (TID 163). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 15.0 in stage 14.0 (TID 159) in 13 ms on 192.168.0.30 (executor driver) (14/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 16.0 in stage 14.0 (TID 160). 2027 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 14.0 in stage 14.0 (TID 158). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 18.0 in stage 14.0 (TID 162). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 23.0 in stage 14.0 (TID 167) (192.168.0.30, executor driver, partition 23, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Running task 22.0 in stage 14.0 (TID 166)
21/11/23 11:30:24 @INFO @Executor@ Running task 21.0 in stage 14.0 (TID 165)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 24.0 in stage 14.0 (TID 168) (192.168.0.30, executor driver, partition 24, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 17.0 in stage 14.0 (TID 161). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 25.0 in stage 14.0 (TID 169) (192.168.0.30, executor driver, partition 25, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Running task 23.0 in stage 14.0 (TID 167)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 26.0 in stage 14.0 (TID 170) (192.168.0.30, executor driver, partition 26, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Running task 24.0 in stage 14.0 (TID 168)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 27.0 in stage 14.0 (TID 171) (192.168.0.30, executor driver, partition 27, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Running task 25.0 in stage 14.0 (TID 169)
21/11/23 11:30:24 @INFO @Executor@ Finished task 13.0 in stage 14.0 (TID 157). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Running task 26.0 in stage 14.0 (TID 170)
21/11/23 11:30:24 @INFO @Executor@ Running task 27.0 in stage 14.0 (TID 171)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 28.0 in stage 14.0 (TID 172) (192.168.0.30, executor driver, partition 28, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 19.0 in stage 14.0 (TID 163) in 13 ms on 192.168.0.30 (executor driver) (15/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 28.0 in stage 14.0 (TID 172)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 17.0 in stage 14.0 (TID 161) in 14 ms on 192.168.0.30 (executor driver) (16/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 18.0 in stage 14.0 (TID 162) in 14 ms on 192.168.0.30 (executor driver) (17/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 14.0 in stage 14.0 (TID 158) in 16 ms on 192.168.0.30 (executor driver) (18/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 16.0 in stage 14.0 (TID 160) in 15 ms on 192.168.0.30 (executor driver) (19/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 29.0 in stage 14.0 (TID 173) (192.168.0.30, executor driver, partition 29, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 13.0 in stage 14.0 (TID 157) in 16 ms on 192.168.0.30 (executor driver) (20/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 29.0 in stage 14.0 (TID 173)
21/11/23 11:30:24 @INFO @Executor@ Finished task 20.0 in stage 14.0 (TID 164). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 30.0 in stage 14.0 (TID 174) (192.168.0.30, executor driver, partition 30, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 20.0 in stage 14.0 (TID 164) in 9 ms on 192.168.0.30 (executor driver) (21/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 30.0 in stage 14.0 (TID 174)
21/11/23 11:30:24 @INFO @Executor@ Finished task 27.0 in stage 14.0 (TID 171). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 23.0 in stage 14.0 (TID 167). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 31.0 in stage 14.0 (TID 175) (192.168.0.30, executor driver, partition 31, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 32.0 in stage 14.0 (TID 176) (192.168.0.30, executor driver, partition 32, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 23.0 in stage 14.0 (TID 167) in 12 ms on 192.168.0.30 (executor driver) (22/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 27.0 in stage 14.0 (TID 171) in 11 ms on 192.168.0.30 (executor driver) (23/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 32.0 in stage 14.0 (TID 176)
21/11/23 11:30:24 @INFO @Executor@ Running task 31.0 in stage 14.0 (TID 175)
21/11/23 11:30:24 @INFO @Executor@ Finished task 22.0 in stage 14.0 (TID 166). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 29.0 in stage 14.0 (TID 173). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 28.0 in stage 14.0 (TID 172). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 24.0 in stage 14.0 (TID 168). 2027 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 21.0 in stage 14.0 (TID 165). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 33.0 in stage 14.0 (TID 177) (192.168.0.30, executor driver, partition 33, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 34.0 in stage 14.0 (TID 178) (192.168.0.30, executor driver, partition 34, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 25.0 in stage 14.0 (TID 169). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 35.0 in stage 14.0 (TID 179) (192.168.0.30, executor driver, partition 35, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 26.0 in stage 14.0 (TID 170). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Running task 34.0 in stage 14.0 (TID 178)
21/11/23 11:30:24 @INFO @Executor@ Running task 33.0 in stage 14.0 (TID 177)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 36.0 in stage 14.0 (TID 180) (192.168.0.30, executor driver, partition 36, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 29.0 in stage 14.0 (TID 173) in 12 ms on 192.168.0.30 (executor driver) (24/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 35.0 in stage 14.0 (TID 179)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 24.0 in stage 14.0 (TID 168) in 14 ms on 192.168.0.30 (executor driver) (25/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 36.0 in stage 14.0 (TID 180)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 22.0 in stage 14.0 (TID 166) in 16 ms on 192.168.0.30 (executor driver) (26/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 37.0 in stage 14.0 (TID 181) (192.168.0.30, executor driver, partition 37, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 30.0 in stage 14.0 (TID 174). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Running task 37.0 in stage 14.0 (TID 181)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 38.0 in stage 14.0 (TID 182) (192.168.0.30, executor driver, partition 38, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 39.0 in stage 14.0 (TID 183) (192.168.0.30, executor driver, partition 39, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Running task 38.0 in stage 14.0 (TID 182)
21/11/23 11:30:24 @INFO @Executor@ Running task 39.0 in stage 14.0 (TID 183)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 40.0 in stage 14.0 (TID 184) (192.168.0.30, executor driver, partition 40, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 28.0 in stage 14.0 (TID 172) in 15 ms on 192.168.0.30 (executor driver) (27/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 40.0 in stage 14.0 (TID 184)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 30.0 in stage 14.0 (TID 174) in 10 ms on 192.168.0.30 (executor driver) (28/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 26.0 in stage 14.0 (TID 170) in 16 ms on 192.168.0.30 (executor driver) (29/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 25.0 in stage 14.0 (TID 169) in 16 ms on 192.168.0.30 (executor driver) (30/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 21.0 in stage 14.0 (TID 165) in 18 ms on 192.168.0.30 (executor driver) (31/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 32.0 in stage 14.0 (TID 176). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 41.0 in stage 14.0 (TID 185) (192.168.0.30, executor driver, partition 41, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 32.0 in stage 14.0 (TID 176) in 11 ms on 192.168.0.30 (executor driver) (32/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 41.0 in stage 14.0 (TID 185)
21/11/23 11:30:24 @INFO @Executor@ Finished task 33.0 in stage 14.0 (TID 177). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 36.0 in stage 14.0 (TID 180). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 34.0 in stage 14.0 (TID 178). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 42.0 in stage 14.0 (TID 186) (192.168.0.30, executor driver, partition 42, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 31.0 in stage 14.0 (TID 175). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Running task 42.0 in stage 14.0 (TID 186)
21/11/23 11:30:24 @INFO @Executor@ Finished task 35.0 in stage 14.0 (TID 179). 1990 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 43.0 in stage 14.0 (TID 187) (192.168.0.30, executor driver, partition 43, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 44.0 in stage 14.0 (TID 188) (192.168.0.30, executor driver, partition 44, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Running task 43.0 in stage 14.0 (TID 187)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 45.0 in stage 14.0 (TID 189) (192.168.0.30, executor driver, partition 45, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 46.0 in stage 14.0 (TID 190) (192.168.0.30, executor driver, partition 46, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 37.0 in stage 14.0 (TID 181). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 38.0 in stage 14.0 (TID 182). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Running task 44.0 in stage 14.0 (TID 188)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 33.0 in stage 14.0 (TID 177) in 13 ms on 192.168.0.30 (executor driver) (33/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 34.0 in stage 14.0 (TID 178) in 13 ms on 192.168.0.30 (executor driver) (34/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 31.0 in stage 14.0 (TID 175) in 14 ms on 192.168.0.30 (executor driver) (35/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 35.0 in stage 14.0 (TID 179) in 13 ms on 192.168.0.30 (executor driver) (36/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 46.0 in stage 14.0 (TID 190)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 36.0 in stage 14.0 (TID 180) in 12 ms on 192.168.0.30 (executor driver) (37/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 45.0 in stage 14.0 (TID 189)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 47.0 in stage 14.0 (TID 191) (192.168.0.30, executor driver, partition 47, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 48.0 in stage 14.0 (TID 192) (192.168.0.30, executor driver, partition 48, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Finished task 40.0 in stage 14.0 (TID 184). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Running task 48.0 in stage 14.0 (TID 192)
21/11/23 11:30:24 @INFO @Executor@ Running task 47.0 in stage 14.0 (TID 191)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 37.0 in stage 14.0 (TID 181) in 13 ms on 192.168.0.30 (executor driver) (38/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 49.0 in stage 14.0 (TID 193) (192.168.0.30, executor driver, partition 49, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 38.0 in stage 14.0 (TID 182) in 13 ms on 192.168.0.30 (executor driver) (39/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 40.0 in stage 14.0 (TID 184) in 13 ms on 192.168.0.30 (executor driver) (40/50)
21/11/23 11:30:24 @INFO @Executor@ Running task 49.0 in stage 14.0 (TID 193)
21/11/23 11:30:24 @INFO @Executor@ Finished task 39.0 in stage 14.0 (TID 183). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 39.0 in stage 14.0 (TID 183) in 14 ms on 192.168.0.30 (executor driver) (41/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 41.0 in stage 14.0 (TID 185). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 44.0 in stage 14.0 (TID 188). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 41.0 in stage 14.0 (TID 185) in 12 ms on 192.168.0.30 (executor driver) (42/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 42.0 in stage 14.0 (TID 186). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 44.0 in stage 14.0 (TID 188) in 10 ms on 192.168.0.30 (executor driver) (43/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 43.0 in stage 14.0 (TID 187). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 45.0 in stage 14.0 (TID 189). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 43.0 in stage 14.0 (TID 187) in 11 ms on 192.168.0.30 (executor driver) (44/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 42.0 in stage 14.0 (TID 186) in 12 ms on 192.168.0.30 (executor driver) (45/50)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 45.0 in stage 14.0 (TID 189) in 11 ms on 192.168.0.30 (executor driver) (46/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 46.0 in stage 14.0 (TID 190). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 46.0 in stage 14.0 (TID 190) in 12 ms on 192.168.0.30 (executor driver) (47/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 48.0 in stage 14.0 (TID 192). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 48.0 in stage 14.0 (TID 192) in 10 ms on 192.168.0.30 (executor driver) (48/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 47.0 in stage 14.0 (TID 191). 1984 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 47.0 in stage 14.0 (TID 191) in 11 ms on 192.168.0.30 (executor driver) (49/50)
21/11/23 11:30:24 @INFO @Executor@ Finished task 49.0 in stage 14.0 (TID 193). 1990 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 49.0 in stage 14.0 (TID 193) in 9 ms on 192.168.0.30 (executor driver) (50/50)
21/11/23 11:30:24 @INFO @TaskSchedulerImpl@ Removed TaskSet 14.0, whose tasks have all completed, from pool 
21/11/23 11:30:24 @INFO @DAGScheduler@ ResultStage 14 (showString at NativeMethodAccessorImpl.java:0) finished in 0.075 s
21/11/23 11:30:24 @INFO @DAGScheduler@ Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:24 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 14: Stage finished
21/11/23 11:30:24 @INFO @DAGScheduler@ Job 10 finished: showString at NativeMethodAccessorImpl.java:0, took 0.076939 s
21/11/23 11:30:24 @INFO @InMemoryFileIndex@ It took 118 ms to list leaf files for 50 paths.
21/11/23 11:30:24 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:24 @INFO @DAGScheduler@ Registering RDD 40 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 4
21/11/23 11:30:24 @INFO @DAGScheduler@ Got map stage job 11 (showString at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:24 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 15 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:24 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:24 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:24 @INFO @DAGScheduler@ Submitting ShuffleMapStage 15 (MapPartitionsRDD[40] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:24 @INFO @MemoryStore@ Block broadcast_16 stored as values in memory (estimated size 20.4 KiB, free 433.6 MiB)
21/11/23 11:30:24 @INFO @MemoryStore@ Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 433.6 MiB)
21/11/23 11:30:24 @INFO @BlockManagerInfo@ Added broadcast_16_piece0 in memory on 192.168.0.30:64662 (size: 8.8 KiB, free: 434.3 MiB)
21/11/23 11:30:24 @INFO @SparkContext@ Created broadcast 16 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:24 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[40] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:24 @INFO @TaskSchedulerImpl@ Adding task set 15.0 with 10 tasks resource profile 0
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 0.0 in stage 15.0 (TID 194) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 1.0 in stage 15.0 (TID 195) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 2.0 in stage 15.0 (TID 196) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 3.0 in stage 15.0 (TID 197) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 4.0 in stage 15.0 (TID 198) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 5.0 in stage 15.0 (TID 199) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 6.0 in stage 15.0 (TID 200) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 7.0 in stage 15.0 (TID 201) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 8.0 in stage 15.0 (TID 202) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 9.0 in stage 15.0 (TID 203) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Running task 0.0 in stage 15.0 (TID 194)
21/11/23 11:30:24 @INFO @Executor@ Running task 1.0 in stage 15.0 (TID 195)
21/11/23 11:30:24 @INFO @Executor@ Running task 7.0 in stage 15.0 (TID 201)
21/11/23 11:30:24 @INFO @Executor@ Running task 5.0 in stage 15.0 (TID 199)
21/11/23 11:30:24 @INFO @Executor@ Running task 4.0 in stage 15.0 (TID 198)
21/11/23 11:30:24 @INFO @Executor@ Running task 3.0 in stage 15.0 (TID 197)
21/11/23 11:30:24 @INFO @Executor@ Running task 2.0 in stage 15.0 (TID 196)
21/11/23 11:30:24 @INFO @Executor@ Running task 9.0 in stage 15.0 (TID 203)
21/11/23 11:30:24 @INFO @Executor@ Running task 8.0 in stage 15.0 (TID 202)
21/11/23 11:30:24 @INFO @Executor@ Running task 6.0 in stage 15.0 (TID 200)
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:24 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:30:24 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:30:24 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:30:24 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:30:24 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:30:24 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:30:24 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:30:24 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:30:24 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:30:24 @INFO @CodecPool@ Got brand-new decompressor [.snappy]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:24 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:24 @INFO @Executor@ Finished task 4.0 in stage 15.0 (TID 198). 2124 bytes result sent to driver
21/11/23 11:30:24 @INFO @Executor@ Finished task 1.0 in stage 15.0 (TID 195). 2124 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 4.0 in stage 15.0 (TID 198) in 271 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 1.0 in stage 15.0 (TID 195) in 272 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:24 @INFO @Executor@ Finished task 6.0 in stage 15.0 (TID 200). 2124 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 6.0 in stage 15.0 (TID 200) in 271 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:24 @INFO @Executor@ Finished task 8.0 in stage 15.0 (TID 202). 2124 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 8.0 in stage 15.0 (TID 202) in 272 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:24 @INFO @Executor@ Finished task 2.0 in stage 15.0 (TID 196). 2124 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 2.0 in stage 15.0 (TID 196) in 272 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:24 @INFO @Executor@ Finished task 9.0 in stage 15.0 (TID 203). 2124 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 9.0 in stage 15.0 (TID 203) in 272 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:24 @INFO @Executor@ Finished task 0.0 in stage 15.0 (TID 194). 2124 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 0.0 in stage 15.0 (TID 194) in 274 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:24 @INFO @Executor@ Finished task 5.0 in stage 15.0 (TID 199). 2124 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 5.0 in stage 15.0 (TID 199) in 273 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:24 @INFO @Executor@ Finished task 7.0 in stage 15.0 (TID 201). 2124 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 7.0 in stage 15.0 (TID 201) in 274 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:24 @INFO @Executor@ Finished task 3.0 in stage 15.0 (TID 197). 2124 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 3.0 in stage 15.0 (TID 197) in 274 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:24 @INFO @TaskSchedulerImpl@ Removed TaskSet 15.0, whose tasks have all completed, from pool 
21/11/23 11:30:24 @INFO @DAGScheduler@ ShuffleMapStage 15 (showString at NativeMethodAccessorImpl.java:0) finished in 0.280 s
21/11/23 11:30:24 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:24 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:24 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:24 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:24 @INFO @CodeGenerator@ Code generated in 8.616875 ms
21/11/23 11:30:24 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:24 @INFO @DAGScheduler@ Got job 12 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:24 @INFO @DAGScheduler@ Final stage: ResultStage 17 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:24 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 16)
21/11/23 11:30:24 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:24 @INFO @DAGScheduler@ Submitting ResultStage 17 (MapPartitionsRDD[43] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:24 @INFO @MemoryStore@ Block broadcast_17 stored as values in memory (estimated size 14.2 KiB, free 433.5 MiB)
21/11/23 11:30:24 @INFO @MemoryStore@ Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 433.5 MiB)
21/11/23 11:30:24 @INFO @BlockManagerInfo@ Added broadcast_17_piece0 in memory on 192.168.0.30:64662 (size: 6.6 KiB, free: 434.2 MiB)
21/11/23 11:30:24 @INFO @SparkContext@ Created broadcast 17 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:24 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[43] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:24 @INFO @TaskSchedulerImpl@ Adding task set 17.0 with 1 tasks resource profile 0
21/11/23 11:30:24 @INFO @TaskSetManager@ Starting task 0.0 in stage 17.0 (TID 204) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:24 @INFO @Executor@ Running task 0.0 in stage 17.0 (TID 204)
21/11/23 11:30:24 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (660.0 B) non-empty blocks including 10 (660.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:24 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:24 @INFO @Executor@ Finished task 0.0 in stage 17.0 (TID 204). 2622 bytes result sent to driver
21/11/23 11:30:24 @INFO @TaskSetManager@ Finished task 0.0 in stage 17.0 (TID 204) in 8 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:24 @INFO @TaskSchedulerImpl@ Removed TaskSet 17.0, whose tasks have all completed, from pool 
21/11/23 11:30:24 @INFO @DAGScheduler@ ResultStage 17 (showString at NativeMethodAccessorImpl.java:0) finished in 0.010 s
21/11/23 11:30:24 @INFO @DAGScheduler@ Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:24 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 17: Stage finished
21/11/23 11:30:24 @INFO @DAGScheduler@ Job 12 finished: showString at NativeMethodAccessorImpl.java:0, took 0.011344 s
21/11/23 11:30:24 @INFO @CodeGenerator@ Code generated in 3.179208 ms
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:26 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:26 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:26 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:26 @INFO @FileSourceStrategy@ Output Data Schema: struct<jinko_male: string, jinko_femail: string>
21/11/23 11:30:26 @INFO @MemoryStore@ Block broadcast_18 stored as values in memory (estimated size 193.7 KiB, free 433.3 MiB)
21/11/23 11:30:26 @INFO @MemoryStore@ Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 433.3 MiB)
21/11/23 11:30:26 @INFO @BlockManagerInfo@ Added broadcast_18_piece0 in memory on 192.168.0.30:64662 (size: 34.1 KiB, free: 434.2 MiB)
21/11/23 11:30:26 @INFO @SparkContext@ Created broadcast 18 from showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:26 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:26 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:26 @INFO @HadoopFSUtils@ Listing leaf files and directories in parallel under 50 paths. The first several paths are: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県, file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県.
21/11/23 11:30:26 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:26 @INFO @DAGScheduler@ Got job 13 (showString at NativeMethodAccessorImpl.java:0) with 50 output partitions
21/11/23 11:30:26 @INFO @DAGScheduler@ Final stage: ResultStage 18 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:26 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:26 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:26 @INFO @DAGScheduler@ Submitting ResultStage 18 (MapPartitionsRDD[46] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:26 @INFO @MemoryStore@ Block broadcast_19 stored as values in memory (estimated size 101.0 KiB, free 433.2 MiB)
21/11/23 11:30:26 @INFO @MemoryStore@ Block broadcast_19_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.2 MiB)
21/11/23 11:30:26 @INFO @BlockManagerInfo@ Added broadcast_19_piece0 in memory on 192.168.0.30:64662 (size: 36.2 KiB, free: 434.2 MiB)
21/11/23 11:30:26 @INFO @SparkContext@ Created broadcast 19 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:26 @INFO @DAGScheduler@ Submitting 50 missing tasks from ResultStage 18 (MapPartitionsRDD[46] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
21/11/23 11:30:26 @INFO @TaskSchedulerImpl@ Adding task set 18.0 with 50 tasks resource profile 0
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 0.0 in stage 18.0 (TID 205) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 1.0 in stage 18.0 (TID 206) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 2.0 in stage 18.0 (TID 207) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 4599 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 3.0 in stage 18.0 (TID 208) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 4614 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 4.0 in stage 18.0 (TID 209) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 5.0 in stage 18.0 (TID 210) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 4587 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 6.0 in stage 18.0 (TID 211) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 7.0 in stage 18.0 (TID 212) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 8.0 in stage 18.0 (TID 213) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 9.0 in stage 18.0 (TID 214) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 1.0 in stage 18.0 (TID 206)
21/11/23 11:30:26 @INFO @Executor@ Running task 8.0 in stage 18.0 (TID 213)
21/11/23 11:30:26 @INFO @Executor@ Running task 2.0 in stage 18.0 (TID 207)
21/11/23 11:30:26 @INFO @Executor@ Running task 5.0 in stage 18.0 (TID 210)
21/11/23 11:30:26 @INFO @Executor@ Running task 3.0 in stage 18.0 (TID 208)
21/11/23 11:30:26 @INFO @Executor@ Running task 4.0 in stage 18.0 (TID 209)
21/11/23 11:30:26 @INFO @Executor@ Running task 0.0 in stage 18.0 (TID 205)
21/11/23 11:30:26 @INFO @Executor@ Running task 7.0 in stage 18.0 (TID 212)
21/11/23 11:30:26 @INFO @Executor@ Running task 6.0 in stage 18.0 (TID 211)
21/11/23 11:30:26 @INFO @Executor@ Running task 9.0 in stage 18.0 (TID 214)
21/11/23 11:30:26 @INFO @Executor@ Finished task 7.0 in stage 18.0 (TID 212). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 1.0 in stage 18.0 (TID 206). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 0.0 in stage 18.0 (TID 205). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 5.0 in stage 18.0 (TID 210). 1978 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 3.0 in stage 18.0 (TID 208). 2035 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 6.0 in stage 18.0 (TID 211). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 10.0 in stage 18.0 (TID 215) (192.168.0.30, executor driver, partition 10, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Finished task 9.0 in stage 18.0 (TID 214). 1990 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 11.0 in stage 18.0 (TID 216) (192.168.0.30, executor driver, partition 11, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 10.0 in stage 18.0 (TID 215)
21/11/23 11:30:26 @INFO @Executor@ Finished task 8.0 in stage 18.0 (TID 213). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 4.0 in stage 18.0 (TID 209). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Running task 11.0 in stage 18.0 (TID 216)
21/11/23 11:30:26 @INFO @Executor@ Finished task 2.0 in stage 18.0 (TID 207). 2002 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 12.0 in stage 18.0 (TID 217) (192.168.0.30, executor driver, partition 12, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 12.0 in stage 18.0 (TID 217)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 13.0 in stage 18.0 (TID 218) (192.168.0.30, executor driver, partition 13, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 7.0 in stage 18.0 (TID 212) in 14 ms on 192.168.0.30 (executor driver) (1/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 3.0 in stage 18.0 (TID 208) in 14 ms on 192.168.0.30 (executor driver) (2/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 13.0 in stage 18.0 (TID 218)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 14.0 in stage 18.0 (TID 219) (192.168.0.30, executor driver, partition 14, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 15.0 in stage 18.0 (TID 220) (192.168.0.30, executor driver, partition 15, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 14.0 in stage 18.0 (TID 219)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 16.0 in stage 18.0 (TID 221) (192.168.0.30, executor driver, partition 16, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 17.0 in stage 18.0 (TID 222) (192.168.0.30, executor driver, partition 17, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 15.0 in stage 18.0 (TID 220)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 18.0 in stage 18.0 (TID 223) (192.168.0.30, executor driver, partition 18, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 16.0 in stage 18.0 (TID 221)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 19.0 in stage 18.0 (TID 224) (192.168.0.30, executor driver, partition 19, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 1.0 in stage 18.0 (TID 206) in 15 ms on 192.168.0.30 (executor driver) (3/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 17.0 in stage 18.0 (TID 222)
21/11/23 11:30:26 @INFO @Executor@ Running task 18.0 in stage 18.0 (TID 223)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 9.0 in stage 18.0 (TID 214) in 14 ms on 192.168.0.30 (executor driver) (4/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 8.0 in stage 18.0 (TID 213) in 15 ms on 192.168.0.30 (executor driver) (5/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 4.0 in stage 18.0 (TID 209) in 16 ms on 192.168.0.30 (executor driver) (6/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 0.0 in stage 18.0 (TID 205) in 16 ms on 192.168.0.30 (executor driver) (7/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 19.0 in stage 18.0 (TID 224)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 6.0 in stage 18.0 (TID 211) in 16 ms on 192.168.0.30 (executor driver) (8/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 5.0 in stage 18.0 (TID 210) in 16 ms on 192.168.0.30 (executor driver) (9/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 2.0 in stage 18.0 (TID 207) in 16 ms on 192.168.0.30 (executor driver) (10/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 11.0 in stage 18.0 (TID 216). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 20.0 in stage 18.0 (TID 225) (192.168.0.30, executor driver, partition 20, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 20.0 in stage 18.0 (TID 225)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 11.0 in stage 18.0 (TID 216) in 9 ms on 192.168.0.30 (executor driver) (11/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 13.0 in stage 18.0 (TID 218). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 21.0 in stage 18.0 (TID 226) (192.168.0.30, executor driver, partition 21, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 13.0 in stage 18.0 (TID 218) in 10 ms on 192.168.0.30 (executor driver) (12/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 10.0 in stage 18.0 (TID 215). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Running task 21.0 in stage 18.0 (TID 226)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 22.0 in stage 18.0 (TID 227) (192.168.0.30, executor driver, partition 22, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Finished task 12.0 in stage 18.0 (TID 217). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Running task 22.0 in stage 18.0 (TID 227)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 23.0 in stage 18.0 (TID 228) (192.168.0.30, executor driver, partition 23, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 23.0 in stage 18.0 (TID 228)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 12.0 in stage 18.0 (TID 217) in 11 ms on 192.168.0.30 (executor driver) (13/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 10.0 in stage 18.0 (TID 215) in 12 ms on 192.168.0.30 (executor driver) (14/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 15.0 in stage 18.0 (TID 220). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 24.0 in stage 18.0 (TID 229) (192.168.0.30, executor driver, partition 24, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 15.0 in stage 18.0 (TID 220) in 13 ms on 192.168.0.30 (executor driver) (15/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 14.0 in stage 18.0 (TID 219). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Running task 24.0 in stage 18.0 (TID 229)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 25.0 in stage 18.0 (TID 230) (192.168.0.30, executor driver, partition 25, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 14.0 in stage 18.0 (TID 219) in 13 ms on 192.168.0.30 (executor driver) (16/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 25.0 in stage 18.0 (TID 230)
21/11/23 11:30:26 @INFO @Executor@ Finished task 18.0 in stage 18.0 (TID 223). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 26.0 in stage 18.0 (TID 231) (192.168.0.30, executor driver, partition 26, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 26.0 in stage 18.0 (TID 231)
21/11/23 11:30:26 @INFO @Executor@ Finished task 16.0 in stage 18.0 (TID 221). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 19.0 in stage 18.0 (TID 224). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 27.0 in stage 18.0 (TID 232) (192.168.0.30, executor driver, partition 27, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 16.0 in stage 18.0 (TID 221) in 13 ms on 192.168.0.30 (executor driver) (17/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 18.0 in stage 18.0 (TID 223) in 14 ms on 192.168.0.30 (executor driver) (18/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 28.0 in stage 18.0 (TID 233) (192.168.0.30, executor driver, partition 28, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Finished task 17.0 in stage 18.0 (TID 222). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Running task 27.0 in stage 18.0 (TID 232)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 29.0 in stage 18.0 (TID 234) (192.168.0.30, executor driver, partition 29, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 28.0 in stage 18.0 (TID 233)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 19.0 in stage 18.0 (TID 224) in 14 ms on 192.168.0.30 (executor driver) (19/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 29.0 in stage 18.0 (TID 234)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 17.0 in stage 18.0 (TID 222) in 15 ms on 192.168.0.30 (executor driver) (20/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 20.0 in stage 18.0 (TID 225). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 30.0 in stage 18.0 (TID 235) (192.168.0.30, executor driver, partition 30, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 20.0 in stage 18.0 (TID 225) in 10 ms on 192.168.0.30 (executor driver) (21/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 30.0 in stage 18.0 (TID 235)
21/11/23 11:30:26 @INFO @Executor@ Finished task 23.0 in stage 18.0 (TID 228). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 31.0 in stage 18.0 (TID 236) (192.168.0.30, executor driver, partition 31, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 31.0 in stage 18.0 (TID 236)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 23.0 in stage 18.0 (TID 228) in 8 ms on 192.168.0.30 (executor driver) (22/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 21.0 in stage 18.0 (TID 226). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 32.0 in stage 18.0 (TID 237) (192.168.0.30, executor driver, partition 32, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 21.0 in stage 18.0 (TID 226) in 12 ms on 192.168.0.30 (executor driver) (23/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 32.0 in stage 18.0 (TID 237)
21/11/23 11:30:26 @INFO @Executor@ Finished task 22.0 in stage 18.0 (TID 227). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 33.0 in stage 18.0 (TID 238) (192.168.0.30, executor driver, partition 33, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 33.0 in stage 18.0 (TID 238)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 22.0 in stage 18.0 (TID 227) in 13 ms on 192.168.0.30 (executor driver) (24/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 25.0 in stage 18.0 (TID 230). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 34.0 in stage 18.0 (TID 239) (192.168.0.30, executor driver, partition 34, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Finished task 26.0 in stage 18.0 (TID 231). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Running task 34.0 in stage 18.0 (TID 239)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 35.0 in stage 18.0 (TID 240) (192.168.0.30, executor driver, partition 35, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 25.0 in stage 18.0 (TID 230) in 11 ms on 192.168.0.30 (executor driver) (25/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 26.0 in stage 18.0 (TID 231) in 10 ms on 192.168.0.30 (executor driver) (26/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 35.0 in stage 18.0 (TID 240)
21/11/23 11:30:26 @INFO @Executor@ Finished task 29.0 in stage 18.0 (TID 234). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 24.0 in stage 18.0 (TID 229). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 27.0 in stage 18.0 (TID 232). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 36.0 in stage 18.0 (TID 241) (192.168.0.30, executor driver, partition 36, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 36.0 in stage 18.0 (TID 241)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 37.0 in stage 18.0 (TID 242) (192.168.0.30, executor driver, partition 37, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 38.0 in stage 18.0 (TID 243) (192.168.0.30, executor driver, partition 38, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 37.0 in stage 18.0 (TID 242)
21/11/23 11:30:26 @INFO @Executor@ Running task 38.0 in stage 18.0 (TID 243)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 24.0 in stage 18.0 (TID 229) in 15 ms on 192.168.0.30 (executor driver) (27/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 29.0 in stage 18.0 (TID 234) in 12 ms on 192.168.0.30 (executor driver) (28/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 27.0 in stage 18.0 (TID 232) in 14 ms on 192.168.0.30 (executor driver) (29/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 28.0 in stage 18.0 (TID 233). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 39.0 in stage 18.0 (TID 244) (192.168.0.30, executor driver, partition 39, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 39.0 in stage 18.0 (TID 244)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 28.0 in stage 18.0 (TID 233) in 14 ms on 192.168.0.30 (executor driver) (30/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 30.0 in stage 18.0 (TID 235). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 40.0 in stage 18.0 (TID 245) (192.168.0.30, executor driver, partition 40, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 30.0 in stage 18.0 (TID 235) in 13 ms on 192.168.0.30 (executor driver) (31/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 40.0 in stage 18.0 (TID 245)
21/11/23 11:30:26 @INFO @Executor@ Finished task 31.0 in stage 18.0 (TID 236). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 41.0 in stage 18.0 (TID 246) (192.168.0.30, executor driver, partition 41, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 31.0 in stage 18.0 (TID 236) in 13 ms on 192.168.0.30 (executor driver) (32/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 41.0 in stage 18.0 (TID 246)
21/11/23 11:30:26 @INFO @Executor@ Finished task 32.0 in stage 18.0 (TID 237). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 42.0 in stage 18.0 (TID 247) (192.168.0.30, executor driver, partition 42, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 32.0 in stage 18.0 (TID 237) in 11 ms on 192.168.0.30 (executor driver) (33/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 42.0 in stage 18.0 (TID 247)
21/11/23 11:30:26 @INFO @Executor@ Finished task 33.0 in stage 18.0 (TID 238). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 43.0 in stage 18.0 (TID 248) (192.168.0.30, executor driver, partition 43, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 43.0 in stage 18.0 (TID 248)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 33.0 in stage 18.0 (TID 238) in 13 ms on 192.168.0.30 (executor driver) (34/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 34.0 in stage 18.0 (TID 239). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 44.0 in stage 18.0 (TID 249) (192.168.0.30, executor driver, partition 44, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Finished task 35.0 in stage 18.0 (TID 240). 1990 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 45.0 in stage 18.0 (TID 250) (192.168.0.30, executor driver, partition 45, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Finished task 36.0 in stage 18.0 (TID 241). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Running task 44.0 in stage 18.0 (TID 249)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 34.0 in stage 18.0 (TID 239) in 14 ms on 192.168.0.30 (executor driver) (35/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 45.0 in stage 18.0 (TID 250)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 46.0 in stage 18.0 (TID 251) (192.168.0.30, executor driver, partition 46, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Finished task 38.0 in stage 18.0 (TID 243). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 37.0 in stage 18.0 (TID 242). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 47.0 in stage 18.0 (TID 252) (192.168.0.30, executor driver, partition 47, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 46.0 in stage 18.0 (TID 251)
21/11/23 11:30:26 @INFO @Executor@ Running task 47.0 in stage 18.0 (TID 252)
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 48.0 in stage 18.0 (TID 253) (192.168.0.30, executor driver, partition 48, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 35.0 in stage 18.0 (TID 240) in 15 ms on 192.168.0.30 (executor driver) (36/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 48.0 in stage 18.0 (TID 253)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 37.0 in stage 18.0 (TID 242) in 13 ms on 192.168.0.30 (executor driver) (37/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 38.0 in stage 18.0 (TID 243) in 13 ms on 192.168.0.30 (executor driver) (38/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 36.0 in stage 18.0 (TID 241) in 14 ms on 192.168.0.30 (executor driver) (39/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 39.0 in stage 18.0 (TID 244). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 49.0 in stage 18.0 (TID 254) (192.168.0.30, executor driver, partition 49, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 39.0 in stage 18.0 (TID 244) in 16 ms on 192.168.0.30 (executor driver) (40/50)
21/11/23 11:30:26 @INFO @Executor@ Running task 49.0 in stage 18.0 (TID 254)
21/11/23 11:30:26 @INFO @Executor@ Finished task 40.0 in stage 18.0 (TID 245). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 41.0 in stage 18.0 (TID 246). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 40.0 in stage 18.0 (TID 245) in 13 ms on 192.168.0.30 (executor driver) (41/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 41.0 in stage 18.0 (TID 246) in 12 ms on 192.168.0.30 (executor driver) (42/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 42.0 in stage 18.0 (TID 247). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 42.0 in stage 18.0 (TID 247) in 12 ms on 192.168.0.30 (executor driver) (43/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 43.0 in stage 18.0 (TID 248). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 43.0 in stage 18.0 (TID 248) in 12 ms on 192.168.0.30 (executor driver) (44/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 47.0 in stage 18.0 (TID 252). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 45.0 in stage 18.0 (TID 250). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 47.0 in stage 18.0 (TID 252) in 10 ms on 192.168.0.30 (executor driver) (45/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 44.0 in stage 18.0 (TID 249). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 46.0 in stage 18.0 (TID 251). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 45.0 in stage 18.0 (TID 250) in 11 ms on 192.168.0.30 (executor driver) (46/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 48.0 in stage 18.0 (TID 253). 1984 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 46.0 in stage 18.0 (TID 251) in 11 ms on 192.168.0.30 (executor driver) (47/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 44.0 in stage 18.0 (TID 249) in 12 ms on 192.168.0.30 (executor driver) (48/50)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 48.0 in stage 18.0 (TID 253) in 11 ms on 192.168.0.30 (executor driver) (49/50)
21/11/23 11:30:26 @INFO @Executor@ Finished task 49.0 in stage 18.0 (TID 254). 1990 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 49.0 in stage 18.0 (TID 254) in 7 ms on 192.168.0.30 (executor driver) (50/50)
21/11/23 11:30:26 @INFO @TaskSchedulerImpl@ Removed TaskSet 18.0, whose tasks have all completed, from pool 
21/11/23 11:30:26 @INFO @DAGScheduler@ ResultStage 18 (showString at NativeMethodAccessorImpl.java:0) finished in 0.075 s
21/11/23 11:30:26 @INFO @DAGScheduler@ Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:26 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 18: Stage finished
21/11/23 11:30:26 @INFO @DAGScheduler@ Job 13 finished: showString at NativeMethodAccessorImpl.java:0, took 0.076956 s
21/11/23 11:30:26 @INFO @InMemoryFileIndex@ It took 164 ms to list leaf files for 50 paths.
21/11/23 11:30:26 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:26 @INFO @DAGScheduler@ Registering RDD 50 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 5
21/11/23 11:30:26 @INFO @DAGScheduler@ Got map stage job 14 (showString at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:26 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 19 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:26 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:26 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:26 @INFO @DAGScheduler@ Submitting ShuffleMapStage 19 (MapPartitionsRDD[50] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:26 @INFO @MemoryStore@ Block broadcast_20 stored as values in memory (estimated size 20.4 KiB, free 433.2 MiB)
21/11/23 11:30:26 @INFO @MemoryStore@ Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 433.1 MiB)
21/11/23 11:30:26 @INFO @BlockManagerInfo@ Added broadcast_20_piece0 in memory on 192.168.0.30:64662 (size: 8.8 KiB, free: 434.2 MiB)
21/11/23 11:30:26 @INFO @SparkContext@ Created broadcast 20 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:26 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[50] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:26 @INFO @TaskSchedulerImpl@ Adding task set 19.0 with 10 tasks resource profile 0
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 0.0 in stage 19.0 (TID 255) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 1.0 in stage 19.0 (TID 256) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 2.0 in stage 19.0 (TID 257) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 3.0 in stage 19.0 (TID 258) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 4.0 in stage 19.0 (TID 259) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 5.0 in stage 19.0 (TID 260) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 6.0 in stage 19.0 (TID 261) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 7.0 in stage 19.0 (TID 262) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 8.0 in stage 19.0 (TID 263) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 9.0 in stage 19.0 (TID 264) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 3.0 in stage 19.0 (TID 258)
21/11/23 11:30:26 @INFO @Executor@ Running task 6.0 in stage 19.0 (TID 261)
21/11/23 11:30:26 @INFO @Executor@ Running task 5.0 in stage 19.0 (TID 260)
21/11/23 11:30:26 @INFO @Executor@ Running task 2.0 in stage 19.0 (TID 257)
21/11/23 11:30:26 @INFO @Executor@ Running task 1.0 in stage 19.0 (TID 256)
21/11/23 11:30:26 @INFO @Executor@ Running task 4.0 in stage 19.0 (TID 259)
21/11/23 11:30:26 @INFO @Executor@ Running task 0.0 in stage 19.0 (TID 255)
21/11/23 11:30:26 @INFO @Executor@ Running task 9.0 in stage 19.0 (TID 264)
21/11/23 11:30:26 @INFO @Executor@ Running task 7.0 in stage 19.0 (TID 262)
21/11/23 11:30:26 @INFO @Executor@ Running task 8.0 in stage 19.0 (TID 263)
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:26 @INFO @Executor@ Finished task 0.0 in stage 19.0 (TID 255). 2124 bytes result sent to driver
21/11/23 11:30:26 @INFO @Executor@ Finished task 3.0 in stage 19.0 (TID 258). 2124 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 3.0 in stage 19.0 (TID 258) in 18 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:26 @INFO @Executor@ Finished task 6.0 in stage 19.0 (TID 261). 2124 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 0.0 in stage 19.0 (TID 255) in 18 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 6.0 in stage 19.0 (TID 261) in 18 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:26 @INFO @Executor@ Finished task 2.0 in stage 19.0 (TID 257). 2124 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 2.0 in stage 19.0 (TID 257) in 19 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:26 @INFO @Executor@ Finished task 9.0 in stage 19.0 (TID 264). 2124 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 9.0 in stage 19.0 (TID 264) in 18 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:26 @INFO @Executor@ Finished task 8.0 in stage 19.0 (TID 263). 2124 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 8.0 in stage 19.0 (TID 263) in 20 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:26 @INFO @Executor@ Finished task 5.0 in stage 19.0 (TID 260). 2124 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 5.0 in stage 19.0 (TID 260) in 20 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:26 @INFO @Executor@ Finished task 7.0 in stage 19.0 (TID 262). 2124 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 7.0 in stage 19.0 (TID 262) in 21 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:26 @INFO @Executor@ Finished task 4.0 in stage 19.0 (TID 259). 2124 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 4.0 in stage 19.0 (TID 259) in 21 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:26 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:26 @INFO @Executor@ Finished task 1.0 in stage 19.0 (TID 256). 2124 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 1.0 in stage 19.0 (TID 256) in 24 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:26 @INFO @TaskSchedulerImpl@ Removed TaskSet 19.0, whose tasks have all completed, from pool 
21/11/23 11:30:26 @INFO @DAGScheduler@ ShuffleMapStage 19 (showString at NativeMethodAccessorImpl.java:0) finished in 0.028 s
21/11/23 11:30:26 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:26 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:26 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:26 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:26 @INFO @CodeGenerator@ Code generated in 13.086875 ms
21/11/23 11:30:26 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:26 @INFO @DAGScheduler@ Got job 15 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:26 @INFO @DAGScheduler@ Final stage: ResultStage 21 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:26 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 20)
21/11/23 11:30:26 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:26 @INFO @DAGScheduler@ Submitting ResultStage 21 (MapPartitionsRDD[53] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:26 @INFO @MemoryStore@ Block broadcast_21 stored as values in memory (estimated size 20.7 KiB, free 433.1 MiB)
21/11/23 11:30:26 @INFO @MemoryStore@ Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 433.1 MiB)
21/11/23 11:30:26 @INFO @BlockManagerInfo@ Added broadcast_21_piece0 in memory on 192.168.0.30:64662 (size: 8.5 KiB, free: 434.2 MiB)
21/11/23 11:30:26 @INFO @SparkContext@ Created broadcast 21 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:26 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[53] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:26 @INFO @TaskSchedulerImpl@ Adding task set 21.0 with 1 tasks resource profile 0
21/11/23 11:30:26 @INFO @TaskSetManager@ Starting task 0.0 in stage 21.0 (TID 265) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:26 @INFO @Executor@ Running task 0.0 in stage 21.0 (TID 265)
21/11/23 11:30:26 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (660.0 B) non-empty blocks including 10 (660.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:26 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:26 @INFO @Executor@ Finished task 0.0 in stage 21.0 (TID 265). 2846 bytes result sent to driver
21/11/23 11:30:26 @INFO @TaskSetManager@ Finished task 0.0 in stage 21.0 (TID 265) in 5 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:26 @INFO @TaskSchedulerImpl@ Removed TaskSet 21.0, whose tasks have all completed, from pool 
21/11/23 11:30:26 @INFO @DAGScheduler@ ResultStage 21 (showString at NativeMethodAccessorImpl.java:0) finished in 0.007 s
21/11/23 11:30:26 @INFO @DAGScheduler@ Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:26 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 21: Stage finished
21/11/23 11:30:26 @INFO @DAGScheduler@ Job 15 finished: showString at NativeMethodAccessorImpl.java:0, took 0.008608 s
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_22 stored as values in memory (estimated size 193.4 KiB, free 432.9 MiB)
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 432.9 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Added broadcast_22_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.1 MiB)
21/11/23 11:30:30 @INFO @SparkContext@ Created broadcast 22 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_15_piece0 on 192.168.0.30:64662 in memory (size: 36.2 KiB, free: 434.2 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_21_piece0 on 192.168.0.30:64662 in memory (size: 8.5 KiB, free: 434.2 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_18_piece0 on 192.168.0.30:64662 in memory (size: 34.1 KiB, free: 434.2 MiB)
21/11/23 11:30:30 @INFO @InMemoryFileIndex@ It took 4 ms to list leaf files for 50 paths.
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_13_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 434.2 MiB)
21/11/23 11:30:30 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_14_piece0 on 192.168.0.30:64662 in memory (size: 34.1 KiB, free: 434.2 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_19_piece0 on 192.168.0.30:64662 in memory (size: 36.2 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @DAGScheduler@ Registering RDD 57 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 6
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_20_piece0 on 192.168.0.30:64662 in memory (size: 8.8 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @DAGScheduler@ Got map stage job 16 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:30 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 22 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:30 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:30 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting ShuffleMapStage 22 (MapPartitionsRDD[57] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_16_piece0 on 192.168.0.30:64662 in memory (size: 8.8 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_23 stored as values in memory (estimated size 16.0 KiB, free 433.7 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_17_piece0 on 192.168.0.30:64662 in memory (size: 6.6 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.7 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Added broadcast_23_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @SparkContext@ Created broadcast 23 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[57] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Adding task set 22.0 with 10 tasks resource profile 0
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 0.0 in stage 22.0 (TID 266) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 1.0 in stage 22.0 (TID 267) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 2.0 in stage 22.0 (TID 268) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 3.0 in stage 22.0 (TID 269) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 4.0 in stage 22.0 (TID 270) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 5.0 in stage 22.0 (TID 271) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 6.0 in stage 22.0 (TID 272) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 7.0 in stage 22.0 (TID 273) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 8.0 in stage 22.0 (TID 274) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 9.0 in stage 22.0 (TID 275) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @Executor@ Running task 1.0 in stage 22.0 (TID 267)
21/11/23 11:30:30 @INFO @Executor@ Running task 5.0 in stage 22.0 (TID 271)
21/11/23 11:30:30 @INFO @Executor@ Running task 2.0 in stage 22.0 (TID 268)
21/11/23 11:30:30 @INFO @Executor@ Running task 3.0 in stage 22.0 (TID 269)
21/11/23 11:30:30 @INFO @Executor@ Running task 0.0 in stage 22.0 (TID 266)
21/11/23 11:30:30 @INFO @Executor@ Running task 6.0 in stage 22.0 (TID 272)
21/11/23 11:30:30 @INFO @Executor@ Running task 4.0 in stage 22.0 (TID 270)
21/11/23 11:30:30 @INFO @Executor@ Running task 8.0 in stage 22.0 (TID 274)
21/11/23 11:30:30 @INFO @Executor@ Running task 7.0 in stage 22.0 (TID 273)
21/11/23 11:30:30 @INFO @Executor@ Running task 9.0 in stage 22.0 (TID 275)
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:30 @INFO @Executor@ Finished task 3.0 in stage 22.0 (TID 269). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 3.0 in stage 22.0 (TID 269) in 16 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 1.0 in stage 22.0 (TID 267). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 1.0 in stage 22.0 (TID 267) in 18 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:30 @INFO @Executor@ Finished task 4.0 in stage 22.0 (TID 270). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 4.0 in stage 22.0 (TID 270) in 19 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 2.0 in stage 22.0 (TID 268). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 2.0 in stage 22.0 (TID 268) in 20 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 9.0 in stage 22.0 (TID 275). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 9.0 in stage 22.0 (TID 275) in 19 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 0.0 in stage 22.0 (TID 266). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 0.0 in stage 22.0 (TID 266) in 21 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 8.0 in stage 22.0 (TID 274). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 8.0 in stage 22.0 (TID 274) in 21 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 5.0 in stage 22.0 (TID 271). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 5.0 in stage 22.0 (TID 271) in 21 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 7.0 in stage 22.0 (TID 273). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 7.0 in stage 22.0 (TID 273) in 22 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 6.0 in stage 22.0 (TID 272). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 6.0 in stage 22.0 (TID 272) in 22 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Removed TaskSet 22.0, whose tasks have all completed, from pool 
21/11/23 11:30:30 @INFO @DAGScheduler@ ShuffleMapStage 22 (count at NativeMethodAccessorImpl.java:0) finished in 0.027 s
21/11/23 11:30:30 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:30 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:30 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:30 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:30 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:30 @INFO @DAGScheduler@ Got job 17 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:30 @INFO @DAGScheduler@ Final stage: ResultStage 24 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:30 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 23)
21/11/23 11:30:30 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting ResultStage 24 (MapPartitionsRDD[60] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_24 stored as values in memory (estimated size 11.0 KiB, free 433.7 MiB)
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_24_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.7 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Added broadcast_24_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @SparkContext@ Created broadcast 24 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[60] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Adding task set 24.0 with 1 tasks resource profile 0
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 0.0 in stage 24.0 (TID 276) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @Executor@ Running task 0.0 in stage 24.0 (TID 276)
21/11/23 11:30:30 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:30 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:30 @INFO @Executor@ Finished task 0.0 in stage 24.0 (TID 276). 2605 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 0.0 in stage 24.0 (TID 276) in 3 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Removed TaskSet 24.0, whose tasks have all completed, from pool 
21/11/23 11:30:30 @INFO @DAGScheduler@ ResultStage 24 (count at NativeMethodAccessorImpl.java:0) finished in 0.005 s
21/11/23 11:30:30 @INFO @DAGScheduler@ Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 24: Stage finished
21/11/23 11:30:30 @INFO @DAGScheduler@ Job 17 finished: count at NativeMethodAccessorImpl.java:0, took 0.005886 s
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_23_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_4_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Removed broadcast_24_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_25 stored as values in memory (estimated size 193.4 KiB, free 433.8 MiB)
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_25_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.7 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Added broadcast_25_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @SparkContext@ Created broadcast 25 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @InMemoryFileIndex@ It took 3 ms to list leaf files for 50 paths.
21/11/23 11:30:30 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:30 @INFO @DAGScheduler@ Registering RDD 64 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 7
21/11/23 11:30:30 @INFO @DAGScheduler@ Got map stage job 18 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:30 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:30 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:30 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting ShuffleMapStage 25 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_26 stored as values in memory (estimated size 16.0 KiB, free 433.7 MiB)
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_26_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.7 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Added broadcast_26_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @SparkContext@ Created broadcast 26 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[64] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Adding task set 25.0 with 10 tasks resource profile 0
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 0.0 in stage 25.0 (TID 277) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 1.0 in stage 25.0 (TID 278) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 2.0 in stage 25.0 (TID 279) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 3.0 in stage 25.0 (TID 280) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 4.0 in stage 25.0 (TID 281) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 5.0 in stage 25.0 (TID 282) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 6.0 in stage 25.0 (TID 283) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 7.0 in stage 25.0 (TID 284) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 8.0 in stage 25.0 (TID 285) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 9.0 in stage 25.0 (TID 286) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @Executor@ Running task 0.0 in stage 25.0 (TID 277)
21/11/23 11:30:30 @INFO @Executor@ Running task 1.0 in stage 25.0 (TID 278)
21/11/23 11:30:30 @INFO @Executor@ Running task 2.0 in stage 25.0 (TID 279)
21/11/23 11:30:30 @INFO @Executor@ Running task 8.0 in stage 25.0 (TID 285)
21/11/23 11:30:30 @INFO @Executor@ Running task 6.0 in stage 25.0 (TID 283)
21/11/23 11:30:30 @INFO @Executor@ Running task 7.0 in stage 25.0 (TID 284)
21/11/23 11:30:30 @INFO @Executor@ Running task 3.0 in stage 25.0 (TID 280)
21/11/23 11:30:30 @INFO @Executor@ Running task 5.0 in stage 25.0 (TID 282)
21/11/23 11:30:30 @INFO @Executor@ Running task 4.0 in stage 25.0 (TID 281)
21/11/23 11:30:30 @INFO @Executor@ Running task 9.0 in stage 25.0 (TID 286)
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:30 @INFO @Executor@ Finished task 4.0 in stage 25.0 (TID 281). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @Executor@ Finished task 2.0 in stage 25.0 (TID 279). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 4.0 in stage 25.0 (TID 281) in 15 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 2.0 in stage 25.0 (TID 279) in 15 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 6.0 in stage 25.0 (TID 283). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 6.0 in stage 25.0 (TID 283) in 16 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 7.0 in stage 25.0 (TID 284). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 7.0 in stage 25.0 (TID 284) in 16 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:30 @INFO @Executor@ Finished task 5.0 in stage 25.0 (TID 282). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 5.0 in stage 25.0 (TID 282) in 16 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 9.0 in stage 25.0 (TID 286). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 9.0 in stage 25.0 (TID 286) in 17 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 3.0 in stage 25.0 (TID 280). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 3.0 in stage 25.0 (TID 280) in 17 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 0.0 in stage 25.0 (TID 277). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 0.0 in stage 25.0 (TID 277) in 18 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 1.0 in stage 25.0 (TID 278). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 1.0 in stage 25.0 (TID 278) in 18 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 8.0 in stage 25.0 (TID 285). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 8.0 in stage 25.0 (TID 285) in 18 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Removed TaskSet 25.0, whose tasks have all completed, from pool 
21/11/23 11:30:30 @INFO @DAGScheduler@ ShuffleMapStage 25 (count at NativeMethodAccessorImpl.java:0) finished in 0.021 s
21/11/23 11:30:30 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:30 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:30 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:30 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:30 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:30 @INFO @DAGScheduler@ Got job 19 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:30 @INFO @DAGScheduler@ Final stage: ResultStage 27 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:30 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 26)
21/11/23 11:30:30 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting ResultStage 27 (MapPartitionsRDD[67] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_27 stored as values in memory (estimated size 11.0 KiB, free 433.7 MiB)
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_27_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.7 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Added broadcast_27_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @SparkContext@ Created broadcast 27 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[67] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Adding task set 27.0 with 1 tasks resource profile 0
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 0.0 in stage 27.0 (TID 287) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @Executor@ Running task 0.0 in stage 27.0 (TID 287)
21/11/23 11:30:30 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:30 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:30 @INFO @Executor@ Finished task 0.0 in stage 27.0 (TID 287). 2605 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 0.0 in stage 27.0 (TID 287) in 2 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Removed TaskSet 27.0, whose tasks have all completed, from pool 
21/11/23 11:30:30 @INFO @DAGScheduler@ ResultStage 27 (count at NativeMethodAccessorImpl.java:0) finished in 0.005 s
21/11/23 11:30:30 @INFO @DAGScheduler@ Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 27: Stage finished
21/11/23 11:30:30 @INFO @DAGScheduler@ Job 19 finished: count at NativeMethodAccessorImpl.java:0, took 0.006230 s
21/11/23 11:30:30 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_28 stored as values in memory (estimated size 193.4 KiB, free 433.5 MiB)
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.5 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Added broadcast_28_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.3 MiB)
21/11/23 11:30:30 @INFO @SparkContext@ Created broadcast 28 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:30 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:30 @INFO @InMemoryFileIndex@ It took 2 ms to list leaf files for 50 paths.
21/11/23 11:30:30 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:30 @INFO @DAGScheduler@ Registering RDD 71 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 8
21/11/23 11:30:30 @INFO @DAGScheduler@ Got map stage job 20 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:30 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 28 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:30 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:30 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting ShuffleMapStage 28 (MapPartitionsRDD[71] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_29 stored as values in memory (estimated size 16.0 KiB, free 433.5 MiB)
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_29_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.4 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Added broadcast_29_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:30 @INFO @SparkContext@ Created broadcast 29 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[71] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Adding task set 28.0 with 10 tasks resource profile 0
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 0.0 in stage 28.0 (TID 288) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 1.0 in stage 28.0 (TID 289) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 2.0 in stage 28.0 (TID 290) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 3.0 in stage 28.0 (TID 291) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 4.0 in stage 28.0 (TID 292) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 5.0 in stage 28.0 (TID 293) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 6.0 in stage 28.0 (TID 294) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 7.0 in stage 28.0 (TID 295) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 8.0 in stage 28.0 (TID 296) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 9.0 in stage 28.0 (TID 297) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @Executor@ Running task 1.0 in stage 28.0 (TID 289)
21/11/23 11:30:30 @INFO @Executor@ Running task 0.0 in stage 28.0 (TID 288)
21/11/23 11:30:30 @INFO @Executor@ Running task 5.0 in stage 28.0 (TID 293)
21/11/23 11:30:30 @INFO @Executor@ Running task 7.0 in stage 28.0 (TID 295)
21/11/23 11:30:30 @INFO @Executor@ Running task 9.0 in stage 28.0 (TID 297)
21/11/23 11:30:30 @INFO @Executor@ Running task 3.0 in stage 28.0 (TID 291)
21/11/23 11:30:30 @INFO @Executor@ Running task 2.0 in stage 28.0 (TID 290)
21/11/23 11:30:30 @INFO @Executor@ Running task 4.0 in stage 28.0 (TID 292)
21/11/23 11:30:30 @INFO @Executor@ Running task 8.0 in stage 28.0 (TID 296)
21/11/23 11:30:30 @INFO @Executor@ Running task 6.0 in stage 28.0 (TID 294)
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:30 @INFO @Executor@ Finished task 0.0 in stage 28.0 (TID 288). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:30 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 0.0 in stage 28.0 (TID 288) in 14 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 8.0 in stage 28.0 (TID 296). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @Executor@ Finished task 5.0 in stage 28.0 (TID 293). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 8.0 in stage 28.0 (TID 296) in 14 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 5.0 in stage 28.0 (TID 293) in 15 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 9.0 in stage 28.0 (TID 297). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 9.0 in stage 28.0 (TID 297) in 15 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 3.0 in stage 28.0 (TID 291). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 3.0 in stage 28.0 (TID 291) in 17 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 6.0 in stage 28.0 (TID 294). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 6.0 in stage 28.0 (TID 294) in 16 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 4.0 in stage 28.0 (TID 292). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 4.0 in stage 28.0 (TID 292) in 18 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 1.0 in stage 28.0 (TID 289). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 1.0 in stage 28.0 (TID 289) in 18 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 7.0 in stage 28.0 (TID 295). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 7.0 in stage 28.0 (TID 295) in 17 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:30 @INFO @Executor@ Finished task 2.0 in stage 28.0 (TID 290). 2124 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 2.0 in stage 28.0 (TID 290) in 19 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Removed TaskSet 28.0, whose tasks have all completed, from pool 
21/11/23 11:30:30 @INFO @DAGScheduler@ ShuffleMapStage 28 (count at NativeMethodAccessorImpl.java:0) finished in 0.021 s
21/11/23 11:30:30 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:30 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:30 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:30 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:30 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:30 @INFO @DAGScheduler@ Got job 21 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:30 @INFO @DAGScheduler@ Final stage: ResultStage 30 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:30 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 29)
21/11/23 11:30:30 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting ResultStage 30 (MapPartitionsRDD[74] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_30 stored as values in memory (estimated size 11.0 KiB, free 433.4 MiB)
21/11/23 11:30:30 @INFO @MemoryStore@ Block broadcast_30_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.4 MiB)
21/11/23 11:30:30 @INFO @BlockManagerInfo@ Added broadcast_30_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.2 MiB)
21/11/23 11:30:30 @INFO @SparkContext@ Created broadcast 30 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:30 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[74] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Adding task set 30.0 with 1 tasks resource profile 0
21/11/23 11:30:30 @INFO @TaskSetManager@ Starting task 0.0 in stage 30.0 (TID 298) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:30 @INFO @Executor@ Running task 0.0 in stage 30.0 (TID 298)
21/11/23 11:30:30 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:30 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:30 @INFO @Executor@ Finished task 0.0 in stage 30.0 (TID 298). 2605 bytes result sent to driver
21/11/23 11:30:30 @INFO @TaskSetManager@ Finished task 0.0 in stage 30.0 (TID 298) in 3 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Removed TaskSet 30.0, whose tasks have all completed, from pool 
21/11/23 11:30:30 @INFO @DAGScheduler@ ResultStage 30 (count at NativeMethodAccessorImpl.java:0) finished in 0.007 s
21/11/23 11:30:30 @INFO @DAGScheduler@ Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:30 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 30: Stage finished
21/11/23 11:30:30 @INFO @DAGScheduler@ Job 21 finished: count at NativeMethodAccessorImpl.java:0, took 0.009124 s
21/11/23 11:30:30 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:30 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_31 stored as values in memory (estimated size 193.4 KiB, free 433.2 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_31_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_31_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 31 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @InMemoryFileIndex@ It took 3 ms to list leaf files for 50 paths.
21/11/23 11:30:31 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:31 @INFO @DAGScheduler@ Registering RDD 78 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 9
21/11/23 11:30:31 @INFO @DAGScheduler@ Got map stage job 22 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ShuffleMapStage 31 (MapPartitionsRDD[78] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_32 stored as values in memory (estimated size 16.0 KiB, free 433.2 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_32_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 433.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_32_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 32 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[78] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 31.0 with 10 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 31.0 (TID 299) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 1.0 in stage 31.0 (TID 300) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 2.0 in stage 31.0 (TID 301) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 3.0 in stage 31.0 (TID 302) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 4.0 in stage 31.0 (TID 303) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 5.0 in stage 31.0 (TID 304) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 6.0 in stage 31.0 (TID 305) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 7.0 in stage 31.0 (TID 306) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 8.0 in stage 31.0 (TID 307) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 9.0 in stage 31.0 (TID 308) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 31.0 (TID 299)
21/11/23 11:30:31 @INFO @Executor@ Running task 3.0 in stage 31.0 (TID 302)
21/11/23 11:30:31 @INFO @Executor@ Running task 7.0 in stage 31.0 (TID 306)
21/11/23 11:30:31 @INFO @Executor@ Running task 2.0 in stage 31.0 (TID 301)
21/11/23 11:30:31 @INFO @Executor@ Running task 4.0 in stage 31.0 (TID 303)
21/11/23 11:30:31 @INFO @Executor@ Running task 8.0 in stage 31.0 (TID 307)
21/11/23 11:30:31 @INFO @Executor@ Running task 9.0 in stage 31.0 (TID 308)
21/11/23 11:30:31 @INFO @Executor@ Running task 6.0 in stage 31.0 (TID 305)
21/11/23 11:30:31 @INFO @Executor@ Running task 1.0 in stage 31.0 (TID 300)
21/11/23 11:30:31 @INFO @Executor@ Running task 5.0 in stage 31.0 (TID 304)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 6.0 in stage 31.0 (TID 305). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 9.0 in stage 31.0 (TID 308). 2081 bytes result sent to driver
21/11/23 11:30:31 @INFO @Executor@ Finished task 7.0 in stage 31.0 (TID 306). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 6.0 in stage 31.0 (TID 305) in 16 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 9.0 in stage 31.0 (TID 308) in 15 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 1.0 in stage 31.0 (TID 300). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 7.0 in stage 31.0 (TID 306) in 15 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 1.0 in stage 31.0 (TID 300) in 16 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 5.0 in stage 31.0 (TID 304). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 5.0 in stage 31.0 (TID 304) in 17 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 31.0 (TID 299). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 31.0 (TID 299) in 18 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 3.0 in stage 31.0 (TID 302). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 3.0 in stage 31.0 (TID 302) in 18 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 2.0 in stage 31.0 (TID 301). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 2.0 in stage 31.0 (TID 301) in 19 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 4.0 in stage 31.0 (TID 303). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 4.0 in stage 31.0 (TID 303) in 19 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 8.0 in stage 31.0 (TID 307). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 8.0 in stage 31.0 (TID 307) in 19 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 31.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ShuffleMapStage 31 (count at NativeMethodAccessorImpl.java:0) finished in 0.023 s
21/11/23 11:30:31 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:31 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:31 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @DAGScheduler@ Got job 23 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ResultStage 33 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 32)
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ResultStage 33 (MapPartitionsRDD[81] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_33 stored as values in memory (estimated size 11.0 KiB, free 433.2 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_33_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_33_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 33 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[81] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 33.0 with 1 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 33.0 (TID 309) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 33.0 (TID 309)
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 33.0 (TID 309). 2605 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 33.0 (TID 309) in 2 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 33.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ResultStage 33 (count at NativeMethodAccessorImpl.java:0) finished in 0.005 s
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 33: Stage finished
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 23 finished: count at NativeMethodAccessorImpl.java:0, took 0.005989 s
21/11/23 11:30:31 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_34 stored as values in memory (estimated size 193.4 KiB, free 433.0 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.0 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_34_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 34 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @InMemoryFileIndex@ It took 2 ms to list leaf files for 50 paths.
21/11/23 11:30:31 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:31 @INFO @DAGScheduler@ Registering RDD 85 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 10
21/11/23 11:30:31 @INFO @DAGScheduler@ Got map stage job 24 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 34 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ShuffleMapStage 34 (MapPartitionsRDD[85] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_35 stored as values in memory (estimated size 16.0 KiB, free 432.9 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_35_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 432.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_35_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 35 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[85] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 34.0 with 10 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 34.0 (TID 310) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 1.0 in stage 34.0 (TID 311) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 2.0 in stage 34.0 (TID 312) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 3.0 in stage 34.0 (TID 313) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 4.0 in stage 34.0 (TID 314) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 5.0 in stage 34.0 (TID 315) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 6.0 in stage 34.0 (TID 316) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 7.0 in stage 34.0 (TID 317) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 8.0 in stage 34.0 (TID 318) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 9.0 in stage 34.0 (TID 319) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 1.0 in stage 34.0 (TID 311)
21/11/23 11:30:31 @INFO @Executor@ Running task 5.0 in stage 34.0 (TID 315)
21/11/23 11:30:31 @INFO @Executor@ Running task 8.0 in stage 34.0 (TID 318)
21/11/23 11:30:31 @INFO @Executor@ Running task 3.0 in stage 34.0 (TID 313)
21/11/23 11:30:31 @INFO @Executor@ Running task 4.0 in stage 34.0 (TID 314)
21/11/23 11:30:31 @INFO @Executor@ Running task 2.0 in stage 34.0 (TID 312)
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 34.0 (TID 310)
21/11/23 11:30:31 @INFO @Executor@ Running task 7.0 in stage 34.0 (TID 317)
21/11/23 11:30:31 @INFO @Executor@ Running task 6.0 in stage 34.0 (TID 316)
21/11/23 11:30:31 @INFO @Executor@ Running task 9.0 in stage 34.0 (TID 319)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 6.0 in stage 34.0 (TID 316). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 8.0 in stage 34.0 (TID 318). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 6.0 in stage 34.0 (TID 316) in 14 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 8.0 in stage 34.0 (TID 318) in 15 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 34.0 (TID 310). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 34.0 (TID 310) in 17 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 3.0 in stage 34.0 (TID 313). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 3.0 in stage 34.0 (TID 313) in 17 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 7.0 in stage 34.0 (TID 317). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 7.0 in stage 34.0 (TID 317) in 16 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 1.0 in stage 34.0 (TID 311). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 1.0 in stage 34.0 (TID 311) in 18 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 2.0 in stage 34.0 (TID 312). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 2.0 in stage 34.0 (TID 312) in 18 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 5.0 in stage 34.0 (TID 315). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 5.0 in stage 34.0 (TID 315) in 17 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 9.0 in stage 34.0 (TID 319). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 9.0 in stage 34.0 (TID 319) in 18 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 4.0 in stage 34.0 (TID 314). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 4.0 in stage 34.0 (TID 314) in 18 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 34.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ShuffleMapStage 34 (count at NativeMethodAccessorImpl.java:0) finished in 0.021 s
21/11/23 11:30:31 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:31 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:31 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @DAGScheduler@ Got job 25 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ResultStage 36 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 35)
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ResultStage 36 (MapPartitionsRDD[88] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_36 stored as values in memory (estimated size 11.0 KiB, free 432.9 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_36_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 432.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_36_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 36 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[88] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 36.0 with 1 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 36.0 (TID 320) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 36.0 (TID 320)
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 36.0 (TID 320). 2648 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 36.0 (TID 320) in 3 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 36.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ResultStage 36 (count at NativeMethodAccessorImpl.java:0) finished in 0.005 s
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 36: Stage finished
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 25 finished: count at NativeMethodAccessorImpl.java:0, took 0.005942 s
21/11/23 11:30:31 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_37 stored as values in memory (estimated size 193.4 KiB, free 432.7 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_37_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 432.7 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_37_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.1 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 37 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @InMemoryFileIndex@ It took 2 ms to list leaf files for 50 paths.
21/11/23 11:30:31 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:31 @INFO @DAGScheduler@ Registering RDD 92 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 11
21/11/23 11:30:31 @INFO @DAGScheduler@ Got map stage job 26 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 37 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ShuffleMapStage 37 (MapPartitionsRDD[92] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_38 stored as values in memory (estimated size 16.0 KiB, free 432.7 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_38_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 432.7 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_38_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.1 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 38 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[92] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 37.0 with 10 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 37.0 (TID 321) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 1.0 in stage 37.0 (TID 322) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 2.0 in stage 37.0 (TID 323) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 3.0 in stage 37.0 (TID 324) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 4.0 in stage 37.0 (TID 325) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 5.0 in stage 37.0 (TID 326) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 6.0 in stage 37.0 (TID 327) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 7.0 in stage 37.0 (TID 328) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 8.0 in stage 37.0 (TID 329) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 9.0 in stage 37.0 (TID 330) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 1.0 in stage 37.0 (TID 322)
21/11/23 11:30:31 @INFO @Executor@ Running task 2.0 in stage 37.0 (TID 323)
21/11/23 11:30:31 @INFO @Executor@ Running task 7.0 in stage 37.0 (TID 328)
21/11/23 11:30:31 @INFO @Executor@ Running task 9.0 in stage 37.0 (TID 330)
21/11/23 11:30:31 @INFO @Executor@ Running task 8.0 in stage 37.0 (TID 329)
21/11/23 11:30:31 @INFO @Executor@ Running task 5.0 in stage 37.0 (TID 326)
21/11/23 11:30:31 @INFO @Executor@ Running task 3.0 in stage 37.0 (TID 324)
21/11/23 11:30:31 @INFO @Executor@ Running task 6.0 in stage 37.0 (TID 327)
21/11/23 11:30:31 @INFO @Executor@ Running task 4.0 in stage 37.0 (TID 325)
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 37.0 (TID 321)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 3.0 in stage 37.0 (TID 324). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 3.0 in stage 37.0 (TID 324) in 13 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 9.0 in stage 37.0 (TID 330). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 9.0 in stage 37.0 (TID 330) in 14 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 5.0 in stage 37.0 (TID 326). 2081 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 5.0 in stage 37.0 (TID 326) in 15 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 2.0 in stage 37.0 (TID 323). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 2.0 in stage 37.0 (TID 323) in 16 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 4.0 in stage 37.0 (TID 325). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 4.0 in stage 37.0 (TID 325) in 17 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 1.0 in stage 37.0 (TID 322). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 1.0 in stage 37.0 (TID 322) in 18 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 7.0 in stage 37.0 (TID 328). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 7.0 in stage 37.0 (TID 328) in 17 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 37.0 (TID 321). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 37.0 (TID 321) in 19 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 6.0 in stage 37.0 (TID 327). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 6.0 in stage 37.0 (TID 327) in 18 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 8.0 in stage 37.0 (TID 329). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 8.0 in stage 37.0 (TID 329) in 19 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 37.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ShuffleMapStage 37 (count at NativeMethodAccessorImpl.java:0) finished in 0.022 s
21/11/23 11:30:31 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:31 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:31 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @DAGScheduler@ Got job 27 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ResultStage 39 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 38)
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ResultStage 39 (MapPartitionsRDD[95] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_39 stored as values in memory (estimated size 11.0 KiB, free 432.7 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_39_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 432.7 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_39_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.1 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 39 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[95] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 39.0 with 1 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 39.0 (TID 331) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 39.0 (TID 331)
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 39.0 (TID 331). 2605 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 39.0 (TID 331) in 2 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 39.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ResultStage 39 (count at NativeMethodAccessorImpl.java:0) finished in 0.004 s
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 39: Stage finished
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 27 finished: count at NativeMethodAccessorImpl.java:0, took 0.005226 s
21/11/23 11:30:31 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_40 stored as values in memory (estimated size 193.4 KiB, free 432.5 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_40_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 432.4 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_40_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.1 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 40 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @InMemoryFileIndex@ It took 5 ms to list leaf files for 50 paths.
21/11/23 11:30:31 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:31 @INFO @DAGScheduler@ Registering RDD 99 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 12
21/11/23 11:30:31 @INFO @DAGScheduler@ Got map stage job 28 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 40 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ShuffleMapStage 40 (MapPartitionsRDD[99] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_41 stored as values in memory (estimated size 16.0 KiB, free 432.4 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_41_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 432.4 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_41_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.1 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 41 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[99] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 40.0 with 10 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 40.0 (TID 332) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 1.0 in stage 40.0 (TID 333) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 2.0 in stage 40.0 (TID 334) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 3.0 in stage 40.0 (TID 335) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 4.0 in stage 40.0 (TID 336) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 5.0 in stage 40.0 (TID 337) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 6.0 in stage 40.0 (TID 338) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 7.0 in stage 40.0 (TID 339) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 8.0 in stage 40.0 (TID 340) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 9.0 in stage 40.0 (TID 341) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 3.0 in stage 40.0 (TID 335)
21/11/23 11:30:31 @INFO @Executor@ Running task 4.0 in stage 40.0 (TID 336)
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 40.0 (TID 332)
21/11/23 11:30:31 @INFO @Executor@ Running task 1.0 in stage 40.0 (TID 333)
21/11/23 11:30:31 @INFO @Executor@ Running task 2.0 in stage 40.0 (TID 334)
21/11/23 11:30:31 @INFO @Executor@ Running task 5.0 in stage 40.0 (TID 337)
21/11/23 11:30:31 @INFO @Executor@ Running task 6.0 in stage 40.0 (TID 338)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:31 @INFO @Executor@ Running task 7.0 in stage 40.0 (TID 339)
21/11/23 11:30:31 @INFO @Executor@ Running task 8.0 in stage 40.0 (TID 340)
21/11/23 11:30:31 @INFO @Executor@ Running task 9.0 in stage 40.0 (TID 341)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 40.0 (TID 332). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 40.0 (TID 332) in 12 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 4.0 in stage 40.0 (TID 336). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 4.0 in stage 40.0 (TID 336) in 15 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 6.0 in stage 40.0 (TID 338). 2081 bytes result sent to driver
21/11/23 11:30:31 @INFO @Executor@ Finished task 1.0 in stage 40.0 (TID 333). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 6.0 in stage 40.0 (TID 338) in 17 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 1.0 in stage 40.0 (TID 333) in 18 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 5.0 in stage 40.0 (TID 337). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 7.0 in stage 40.0 (TID 339). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 5.0 in stage 40.0 (TID 337) in 19 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 7.0 in stage 40.0 (TID 339) in 19 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 2.0 in stage 40.0 (TID 334). 2081 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 2.0 in stage 40.0 (TID 334) in 23 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 3.0 in stage 40.0 (TID 335). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 3.0 in stage 40.0 (TID 335) in 23 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 8.0 in stage 40.0 (TID 340). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 8.0 in stage 40.0 (TID 340) in 24 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 9.0 in stage 40.0 (TID 341). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 9.0 in stage 40.0 (TID 341) in 25 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 40.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ShuffleMapStage 40 (count at NativeMethodAccessorImpl.java:0) finished in 0.029 s
21/11/23 11:30:31 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:31 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:31 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @DAGScheduler@ Got job 29 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ResultStage 42 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 41)
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ResultStage 42 (MapPartitionsRDD[102] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_42 stored as values in memory (estimated size 11.0 KiB, free 432.4 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_42_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 432.4 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_42_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.1 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 42 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[102] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 42.0 with 1 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 42.0 (TID 342) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 42.0 (TID 342)
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 42.0 (TID 342). 2605 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 42.0 (TID 342) in 2 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 42.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ResultStage 42 (count at NativeMethodAccessorImpl.java:0) finished in 0.006 s
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 42: Stage finished
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 29 finished: count at NativeMethodAccessorImpl.java:0, took 0.006666 s
21/11/23 11:30:31 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_43 stored as values in memory (estimated size 193.4 KiB, free 432.2 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_43_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 432.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_43_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.0 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 43 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @InMemoryFileIndex@ It took 2 ms to list leaf files for 50 paths.
21/11/23 11:30:31 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:31 @INFO @DAGScheduler@ Registering RDD 106 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 13
21/11/23 11:30:31 @INFO @DAGScheduler@ Got map stage job 30 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 43 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ShuffleMapStage 43 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_44 stored as values in memory (estimated size 16.0 KiB, free 432.2 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_44_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 432.1 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_44_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.0 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 44 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[106] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 43.0 with 10 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 43.0 (TID 343) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 1.0 in stage 43.0 (TID 344) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 2.0 in stage 43.0 (TID 345) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 3.0 in stage 43.0 (TID 346) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 4.0 in stage 43.0 (TID 347) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 5.0 in stage 43.0 (TID 348) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 6.0 in stage 43.0 (TID 349) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 7.0 in stage 43.0 (TID 350) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 8.0 in stage 43.0 (TID 351) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 9.0 in stage 43.0 (TID 352) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 2.0 in stage 43.0 (TID 345)
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 43.0 (TID 343)
21/11/23 11:30:31 @INFO @Executor@ Running task 6.0 in stage 43.0 (TID 349)
21/11/23 11:30:31 @INFO @Executor@ Running task 5.0 in stage 43.0 (TID 348)
21/11/23 11:30:31 @INFO @Executor@ Running task 8.0 in stage 43.0 (TID 351)
21/11/23 11:30:31 @INFO @Executor@ Running task 1.0 in stage 43.0 (TID 344)
21/11/23 11:30:31 @INFO @Executor@ Running task 4.0 in stage 43.0 (TID 347)
21/11/23 11:30:31 @INFO @Executor@ Running task 3.0 in stage 43.0 (TID 346)
21/11/23 11:30:31 @INFO @Executor@ Running task 9.0 in stage 43.0 (TID 352)
21/11/23 11:30:31 @INFO @Executor@ Running task 7.0 in stage 43.0 (TID 350)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 8.0 in stage 43.0 (TID 351). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 8.0 in stage 43.0 (TID 351) in 11 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 6.0 in stage 43.0 (TID 349). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 6.0 in stage 43.0 (TID 349) in 13 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 2.0 in stage 43.0 (TID 345). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 2.0 in stage 43.0 (TID 345) in 14 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 3.0 in stage 43.0 (TID 346). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 3.0 in stage 43.0 (TID 346) in 14 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 1.0 in stage 43.0 (TID 344). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 1.0 in stage 43.0 (TID 344) in 15 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 9.0 in stage 43.0 (TID 352). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 9.0 in stage 43.0 (TID 352) in 15 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 7.0 in stage 43.0 (TID 350). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 7.0 in stage 43.0 (TID 350) in 15 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 5.0 in stage 43.0 (TID 348). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 5.0 in stage 43.0 (TID 348) in 16 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 43.0 (TID 343). 2081 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 43.0 (TID 343) in 16 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 4.0 in stage 43.0 (TID 347). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 4.0 in stage 43.0 (TID 347) in 17 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 43.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ShuffleMapStage 43 (count at NativeMethodAccessorImpl.java:0) finished in 0.020 s
21/11/23 11:30:31 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:31 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:31 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @DAGScheduler@ Got job 31 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ResultStage 45 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 44)
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ResultStage 45 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_45 stored as values in memory (estimated size 11.0 KiB, free 432.1 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_45_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 432.1 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_45_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.0 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 45 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[109] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 45.0 with 1 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 45.0 (TID 353) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 45.0 (TID 353)
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 45.0 (TID 353). 2605 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 45.0 (TID 353) in 3 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 45.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ResultStage 45 (count at NativeMethodAccessorImpl.java:0) finished in 0.005 s
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 45: Stage finished
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 31 finished: count at NativeMethodAccessorImpl.java:0, took 0.005623 s
21/11/23 11:30:31 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Output Data Schema: struct<>
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_46 stored as values in memory (estimated size 193.4 KiB, free 431.9 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_46_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 431.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_46_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.0 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 46 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @InMemoryFileIndex@ It took 3 ms to list leaf files for 50 paths.
21/11/23 11:30:31 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:31 @INFO @DAGScheduler@ Registering RDD 113 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 14
21/11/23 11:30:31 @INFO @DAGScheduler@ Got map stage job 32 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 46 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ShuffleMapStage 46 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_47 stored as values in memory (estimated size 16.0 KiB, free 431.9 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_47_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 431.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_47_piece0 in memory on 192.168.0.30:64662 (size: 7.4 KiB, free: 434.0 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 47 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 46 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 46.0 with 10 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 46.0 (TID 354) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 1.0 in stage 46.0 (TID 355) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 2.0 in stage 46.0 (TID 356) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 3.0 in stage 46.0 (TID 357) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 4.0 in stage 46.0 (TID 358) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 5.0 in stage 46.0 (TID 359) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 6.0 in stage 46.0 (TID 360) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 7.0 in stage 46.0 (TID 361) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 8.0 in stage 46.0 (TID 362) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 9.0 in stage 46.0 (TID 363) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 2.0 in stage 46.0 (TID 356)
21/11/23 11:30:31 @INFO @Executor@ Running task 3.0 in stage 46.0 (TID 357)
21/11/23 11:30:31 @INFO @Executor@ Running task 6.0 in stage 46.0 (TID 360)
21/11/23 11:30:31 @INFO @Executor@ Running task 4.0 in stage 46.0 (TID 358)
21/11/23 11:30:31 @INFO @Executor@ Running task 5.0 in stage 46.0 (TID 359)
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 46.0 (TID 354)
21/11/23 11:30:31 @INFO @Executor@ Running task 1.0 in stage 46.0 (TID 355)
21/11/23 11:30:31 @INFO @Executor@ Running task 9.0 in stage 46.0 (TID 363)
21/11/23 11:30:31 @INFO @Executor@ Running task 7.0 in stage 46.0 (TID 361)
21/11/23 11:30:31 @INFO @Executor@ Running task 8.0 in stage 46.0 (TID 362)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 4.0 in stage 46.0 (TID 358). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 7.0 in stage 46.0 (TID 361). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 46.0 (TID 354). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @Executor@ Finished task 8.0 in stage 46.0 (TID 362). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 4.0 in stage 46.0 (TID 358) in 24 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 7.0 in stage 46.0 (TID 361) in 23 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 46.0 (TID 354) in 24 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 8.0 in stage 46.0 (TID 362) in 23 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 3.0 in stage 46.0 (TID 357). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 3.0 in stage 46.0 (TID 357) in 25 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:31 @INFO @Executor@ Finished task 9.0 in stage 46.0 (TID 363). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @Executor@ Finished task 6.0 in stage 46.0 (TID 360). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 9.0 in stage 46.0 (TID 363) in 26 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 6.0 in stage 46.0 (TID 360) in 26 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 1.0 in stage 46.0 (TID 355). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 1.0 in stage 46.0 (TID 355) in 28 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 2.0 in stage 46.0 (TID 356). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 2.0 in stage 46.0 (TID 356) in 28 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:31 @INFO @Executor@ Finished task 5.0 in stage 46.0 (TID 359). 2124 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 5.0 in stage 46.0 (TID 359) in 29 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 46.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ShuffleMapStage 46 (count at NativeMethodAccessorImpl.java:0) finished in 0.032 s
21/11/23 11:30:31 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:31 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:31 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:31 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @DAGScheduler@ Got job 33 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ResultStage 48 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 47)
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ResultStage 48 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_48 stored as values in memory (estimated size 11.0 KiB, free 431.9 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_48_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 431.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_48_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.0 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 48 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[116] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 48.0 with 1 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 48.0 (TID 364) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 48.0 (TID 364)
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (600.0 B) non-empty blocks including 10 (600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:31 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:31 @INFO @Executor@ Finished task 0.0 in stage 48.0 (TID 364). 2605 bytes result sent to driver
21/11/23 11:30:31 @INFO @TaskSetManager@ Finished task 0.0 in stage 48.0 (TID 364) in 3 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Removed TaskSet 48.0, whose tasks have all completed, from pool 
21/11/23 11:30:31 @INFO @DAGScheduler@ ResultStage 48 (count at NativeMethodAccessorImpl.java:0) finished in 0.005 s
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 48: Stage finished
21/11/23 11:30:31 @INFO @DAGScheduler@ Job 33 finished: count at NativeMethodAccessorImpl.java:0, took 0.005700 s
21/11/23 11:30:31 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:31 @INFO @FileSourceStrategy@ Output Data Schema: struct<code: string, gengo: string, wareki: string, seireki: string, chu: string ... 6 more fields>
21/11/23 11:30:31 @WARN @package@ Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
21/11/23 11:30:31 @INFO @CodeGenerator@ Code generated in 8.506542 ms
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_49 stored as values in memory (estimated size 194.4 KiB, free 431.7 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_49_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 431.6 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_49_piece0 in memory on 192.168.0.30:64662 (size: 34.4 KiB, free: 433.9 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 49 from showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:31 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:31 @INFO @InMemoryFileIndex@ It took 2 ms to list leaf files for 50 paths.
21/11/23 11:30:31 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:31 @INFO @DAGScheduler@ Registering RDD 121 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 15
21/11/23 11:30:31 @INFO @DAGScheduler@ Got map stage job 34 (showString at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:31 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 49 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:31 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting ShuffleMapStage 49 (MapPartitionsRDD[121] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_50 stored as values in memory (estimated size 108.8 KiB, free 431.5 MiB)
21/11/23 11:30:31 @INFO @MemoryStore@ Block broadcast_50_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 431.5 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Added broadcast_50_piece0 in memory on 192.168.0.30:64662 (size: 34.3 KiB, free: 433.9 MiB)
21/11/23 11:30:31 @INFO @SparkContext@ Created broadcast 50 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:31 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[121] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:31 @INFO @TaskSchedulerImpl@ Adding task set 49.0 with 10 tasks resource profile 0
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 0.0 in stage 49.0 (TID 365) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 1.0 in stage 49.0 (TID 366) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 2.0 in stage 49.0 (TID 367) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 3.0 in stage 49.0 (TID 368) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 4.0 in stage 49.0 (TID 369) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 5.0 in stage 49.0 (TID 370) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 6.0 in stage 49.0 (TID 371) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 7.0 in stage 49.0 (TID 372) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 8.0 in stage 49.0 (TID 373) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @TaskSetManager@ Starting task 9.0 in stage 49.0 (TID 374) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:31 @INFO @Executor@ Running task 0.0 in stage 49.0 (TID 365)
21/11/23 11:30:31 @INFO @Executor@ Running task 1.0 in stage 49.0 (TID 366)
21/11/23 11:30:31 @INFO @Executor@ Running task 4.0 in stage 49.0 (TID 369)
21/11/23 11:30:31 @INFO @Executor@ Running task 5.0 in stage 49.0 (TID 370)
21/11/23 11:30:31 @INFO @Executor@ Running task 7.0 in stage 49.0 (TID 372)
21/11/23 11:30:31 @INFO @Executor@ Running task 8.0 in stage 49.0 (TID 373)
21/11/23 11:30:31 @INFO @Executor@ Running task 9.0 in stage 49.0 (TID 374)
21/11/23 11:30:31 @INFO @Executor@ Running task 3.0 in stage 49.0 (TID 368)
21/11/23 11:30:31 @INFO @Executor@ Running task 2.0 in stage 49.0 (TID 367)
21/11/23 11:30:31 @INFO @Executor@ Running task 6.0 in stage 49.0 (TID 371)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_26_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 433.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_38_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 433.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_45_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 433.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_33_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 433.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_39_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 433.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_30_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 433.9 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_22_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.0 MiB)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_44_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.0 MiB)
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:31 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_43_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.0 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_40_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.0 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_31_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.1 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_34_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.1 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_46_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.1 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_28_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_27_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_41_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_47_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_42_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_48_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_32_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_37_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.2 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_29_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.3 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_35_piece0 on 192.168.0.30:64662 in memory (size: 7.4 KiB, free: 434.3 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_36_piece0 on 192.168.0.30:64662 in memory (size: 5.5 KiB, free: 434.3 MiB)
21/11/23 11:30:31 @INFO @BlockManagerInfo@ Removed broadcast_25_piece0 on 192.168.0.30:64662 in memory (size: 34.0 KiB, free: 434.3 MiB)
21/11/23 11:30:32 @INFO @CodeGenerator@ Code generated in 4.725417 ms
21/11/23 11:30:32 @INFO @CodeGenerator@ Code generated in 3.539 ms
21/11/23 11:30:32 @INFO @CodeGenerator@ Code generated in 5.383042 ms
21/11/23 11:30:32 @INFO @CodeGenerator@ Code generated in 70.308167 ms
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:32 @INFO @Executor@ Finished task 9.0 in stage 49.0 (TID 374). 2943 bytes result sent to driver
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 9.0 in stage 49.0 (TID 374) in 549 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:32 @INFO @Executor@ Finished task 5.0 in stage 49.0 (TID 370). 2943 bytes result sent to driver
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 5.0 in stage 49.0 (TID 370) in 562 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:32 @INFO @Executor@ Finished task 3.0 in stage 49.0 (TID 368). 2943 bytes result sent to driver
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 3.0 in stage 49.0 (TID 368) in 571 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:32 @INFO @Executor@ Finished task 2.0 in stage 49.0 (TID 367). 2943 bytes result sent to driver
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 2.0 in stage 49.0 (TID 367) in 574 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:32 @INFO @Executor@ Finished task 7.0 in stage 49.0 (TID 372). 2943 bytes result sent to driver
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 7.0 in stage 49.0 (TID 372) in 577 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:32 @INFO @Executor@ Finished task 0.0 in stage 49.0 (TID 365). 2943 bytes result sent to driver
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 0.0 in stage 49.0 (TID 365) in 581 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:32 @INFO @Executor@ Finished task 8.0 in stage 49.0 (TID 373). 2943 bytes result sent to driver
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 8.0 in stage 49.0 (TID 373) in 579 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:32 @INFO @Executor@ Finished task 6.0 in stage 49.0 (TID 371). 2943 bytes result sent to driver
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 6.0 in stage 49.0 (TID 371) in 582 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:32 @INFO @Executor@ Finished task 1.0 in stage 49.0 (TID 366). 2943 bytes result sent to driver
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 1.0 in stage 49.0 (TID 366) in 584 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:32 @INFO @Executor@ Finished task 4.0 in stage 49.0 (TID 369). 2943 bytes result sent to driver
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 4.0 in stage 49.0 (TID 369) in 585 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:32 @INFO @TaskSchedulerImpl@ Removed TaskSet 49.0, whose tasks have all completed, from pool 
21/11/23 11:30:32 @INFO @DAGScheduler@ ShuffleMapStage 49 (showString at NativeMethodAccessorImpl.java:0) finished in 0.590 s
21/11/23 11:30:32 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:32 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:32 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:32 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:32 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:32 @INFO @DAGScheduler@ Got job 35 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:32 @INFO @DAGScheduler@ Final stage: ResultStage 51 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:32 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 50)
21/11/23 11:30:32 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:32 @INFO @DAGScheduler@ Submitting ResultStage 51 (MapPartitionsRDD[124] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:32 @INFO @MemoryStore@ Block broadcast_51 stored as values in memory (estimated size 224.8 KiB, free 433.6 MiB)
21/11/23 11:30:32 @INFO @MemoryStore@ Block broadcast_51_piece0 stored as bytes in memory (estimated size 58.1 KiB, free 433.5 MiB)
21/11/23 11:30:32 @INFO @BlockManagerInfo@ Added broadcast_51_piece0 in memory on 192.168.0.30:64662 (size: 58.1 KiB, free: 434.2 MiB)
21/11/23 11:30:32 @INFO @SparkContext@ Created broadcast 51 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:32 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[124] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:32 @INFO @TaskSchedulerImpl@ Adding task set 51.0 with 1 tasks resource profile 0
21/11/23 11:30:32 @INFO @TaskSetManager@ Starting task 0.0 in stage 51.0 (TID 375) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:32 @INFO @Executor@ Running task 0.0 in stage 51.0 (TID 375)
21/11/23 11:30:32 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (5.6 KiB) non-empty blocks including 10 (5.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:32 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:32 @INFO @CodeGenerator@ Code generated in 3.839833 ms
21/11/23 11:30:32 @INFO @CodeGenerator@ Code generated in 8.6695 ms
21/11/23 11:30:32 @INFO @Executor@ Finished task 0.0 in stage 51.0 (TID 375). 4182 bytes result sent to driver
21/11/23 11:30:32 @INFO @TaskSetManager@ Finished task 0.0 in stage 51.0 (TID 375) in 98 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:32 @INFO @TaskSchedulerImpl@ Removed TaskSet 51.0, whose tasks have all completed, from pool 
21/11/23 11:30:32 @INFO @DAGScheduler@ ResultStage 51 (showString at NativeMethodAccessorImpl.java:0) finished in 0.104 s
21/11/23 11:30:32 @INFO @DAGScheduler@ Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:32 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 51: Stage finished
21/11/23 11:30:32 @INFO @DAGScheduler@ Job 35 finished: showString at NativeMethodAccessorImpl.java:0, took 0.104875 s
21/11/23 11:30:32 @INFO @CodeGenerator@ Code generated in 5.049459 ms
21/11/23 11:30:32 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:32 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:32 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:32 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:32 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:32 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:32 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:32 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:32 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:32 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:32 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:32 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:32 @INFO @HiveMetaStore@ 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
21/11/23 11:30:32 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
21/11/23 11:30:32 @INFO @HiveMetaStore@ 0: get_partitions_by_filter : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:32 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:32 @INFO @InMemoryFileIndex@ It took 0 ms to list leaf files for 1 paths.
21/11/23 11:30:32 @INFO @DataSourceStrategy@ Pruning directories with: isnotnull(kenmei#27633),(kenmei#27633 = 東京都)
21/11/23 11:30:32 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:32 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:32 @INFO @FileSourceStrategy@ Output Data Schema: struct<code: string, gengo: string, wareki: string, seireki: string, chu: string ... 6 more fields>
21/11/23 11:30:32 @INFO @MemoryStore@ Block broadcast_52 stored as values in memory (estimated size 194.4 KiB, free 433.3 MiB)
21/11/23 11:30:32 @INFO @MemoryStore@ Block broadcast_52_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.3 MiB)
21/11/23 11:30:32 @INFO @BlockManagerInfo@ Added broadcast_52_piece0 in memory on 192.168.0.30:64662 (size: 34.4 KiB, free: 434.2 MiB)
21/11/23 11:30:32 @INFO @SparkContext@ Created broadcast 52 from showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:32 @INFO @InMemoryFileIndex@ Selected 1 partitions out of 1, pruned 0.0% partitions.
21/11/23 11:30:32 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:32 @INFO @DAGScheduler@ Registering RDD 129 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 16
21/11/23 11:30:32 @INFO @DAGScheduler@ Got map stage job 36 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:32 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 52 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:32 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:32 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:32 @INFO @DAGScheduler@ Submitting ShuffleMapStage 52 (MapPartitionsRDD[129] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:32 @INFO @MemoryStore@ Block broadcast_53 stored as values in memory (estimated size 109.5 KiB, free 433.2 MiB)
21/11/23 11:30:32 @INFO @MemoryStore@ Block broadcast_53_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 433.2 MiB)
21/11/23 11:30:32 @INFO @BlockManagerInfo@ Added broadcast_53_piece0 in memory on 192.168.0.30:64662 (size: 34.7 KiB, free: 434.2 MiB)
21/11/23 11:30:32 @INFO @SparkContext@ Created broadcast 53 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:32 @INFO @DAGScheduler@ Submitting 1 missing tasks from ShuffleMapStage 52 (MapPartitionsRDD[129] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:32 @INFO @TaskSchedulerImpl@ Adding task set 52.0 with 1 tasks resource profile 0
21/11/23 11:30:32 @INFO @TaskSetManager@ Starting task 0.0 in stage 52.0 (TID 376) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 5093 bytes) taskResourceAssignments Map()
21/11/23 11:30:32 @INFO @Executor@ Running task 0.0 in stage 52.0 (TID 376)
21/11/23 11:30:32 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:33 @INFO @Executor@ Finished task 0.0 in stage 52.0 (TID 376). 2900 bytes result sent to driver
21/11/23 11:30:33 @INFO @TaskSetManager@ Finished task 0.0 in stage 52.0 (TID 376) in 70 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:33 @INFO @TaskSchedulerImpl@ Removed TaskSet 52.0, whose tasks have all completed, from pool 
21/11/23 11:30:33 @INFO @DAGScheduler@ ShuffleMapStage 52 (showString at NativeMethodAccessorImpl.java:0) finished in 0.074 s
21/11/23 11:30:33 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:33 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:33 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:33 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:33 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:33 @INFO @DAGScheduler@ Got job 37 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:33 @INFO @DAGScheduler@ Final stage: ResultStage 54 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:33 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 53)
21/11/23 11:30:33 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:33 @INFO @DAGScheduler@ Submitting ResultStage 54 (MapPartitionsRDD[132] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:33 @INFO @MemoryStore@ Block broadcast_54 stored as values in memory (estimated size 224.6 KiB, free 433.0 MiB)
21/11/23 11:30:33 @INFO @MemoryStore@ Block broadcast_54_piece0 stored as bytes in memory (estimated size 58.3 KiB, free 432.9 MiB)
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Added broadcast_54_piece0 in memory on 192.168.0.30:64662 (size: 58.3 KiB, free: 434.1 MiB)
21/11/23 11:30:33 @INFO @SparkContext@ Created broadcast 54 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:33 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[132] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:33 @INFO @TaskSchedulerImpl@ Adding task set 54.0 with 1 tasks resource profile 0
21/11/23 11:30:33 @INFO @TaskSetManager@ Starting task 0.0 in stage 54.0 (TID 377) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:33 @INFO @Executor@ Running task 0.0 in stage 54.0 (TID 377)
21/11/23 11:30:33 @INFO @ShuffleBlockFetcherIterator@ Getting 1 (251.0 B) non-empty blocks including 1 (251.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:33 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:33 @INFO @CodeGenerator@ Code generated in 5.441625 ms
21/11/23 11:30:33 @INFO @Executor@ Finished task 0.0 in stage 54.0 (TID 377). 4167 bytes result sent to driver
21/11/23 11:30:33 @INFO @TaskSetManager@ Finished task 0.0 in stage 54.0 (TID 377) in 76 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:33 @INFO @TaskSchedulerImpl@ Removed TaskSet 54.0, whose tasks have all completed, from pool 
21/11/23 11:30:33 @INFO @DAGScheduler@ ResultStage 54 (showString at NativeMethodAccessorImpl.java:0) finished in 0.080 s
21/11/23 11:30:33 @INFO @DAGScheduler@ Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:33 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 54: Stage finished
21/11/23 11:30:33 @INFO @DAGScheduler@ Job 37 finished: showString at NativeMethodAccessorImpl.java:0, took 0.080912 s
21/11/23 11:30:33 @INFO @CodeGenerator@ Code generated in 4.479375 ms
21/11/23 11:30:33 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:33 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:33 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:33 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:33 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:33 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:33 @INFO @HiveMetaStore@ 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
21/11/23 11:30:33 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
21/11/23 11:30:33 @INFO @HiveMetaStore@ 0: get_partitions_by_filter : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:33 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:33 @INFO @InMemoryFileIndex@ It took 0 ms to list leaf files for 1 paths.
21/11/23 11:30:33 @INFO @DataSourceStrategy@ Pruning directories with: isnotnull(kenmei#27633),(kenmei#27633 = 東京都)
21/11/23 11:30:33 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:33 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:33 @INFO @FileSourceStrategy@ Output Data Schema: struct<code: string, gengo: string, wareki: string, seireki: string, chu: string ... 6 more fields>
21/11/23 11:30:33 @INFO @MemoryStore@ Block broadcast_55 stored as values in memory (estimated size 194.4 KiB, free 432.7 MiB)
21/11/23 11:30:33 @INFO @MemoryStore@ Block broadcast_55_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 432.7 MiB)
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Added broadcast_55_piece0 in memory on 192.168.0.30:64662 (size: 34.4 KiB, free: 434.1 MiB)
21/11/23 11:30:33 @INFO @SparkContext@ Created broadcast 55 from collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13
21/11/23 11:30:33 @INFO @InMemoryFileIndex@ Selected 1 partitions out of 1, pruned 0.0% partitions.
21/11/23 11:30:33 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:33 @INFO @DAGScheduler@ Registering RDD 137 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13) as input to shuffle 17
21/11/23 11:30:33 @INFO @DAGScheduler@ Got map stage job 38 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13) with 1 output partitions
21/11/23 11:30:33 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 55 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13)
21/11/23 11:30:33 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:33 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:33 @INFO @DAGScheduler@ Submitting ShuffleMapStage 55 (MapPartitionsRDD[137] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13), which has no missing parents
21/11/23 11:30:33 @INFO @MemoryStore@ Block broadcast_56 stored as values in memory (estimated size 109.3 KiB, free 432.6 MiB)
21/11/23 11:30:33 @INFO @MemoryStore@ Block broadcast_56_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 432.5 MiB)
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Added broadcast_56_piece0 in memory on 192.168.0.30:64662 (size: 34.7 KiB, free: 434.1 MiB)
21/11/23 11:30:33 @INFO @SparkContext@ Created broadcast 56 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:33 @INFO @DAGScheduler@ Submitting 1 missing tasks from ShuffleMapStage 55 (MapPartitionsRDD[137] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:33 @INFO @TaskSchedulerImpl@ Adding task set 55.0 with 1 tasks resource profile 0
21/11/23 11:30:33 @INFO @TaskSetManager@ Starting task 0.0 in stage 55.0 (TID 378) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 5093 bytes) taskResourceAssignments Map()
21/11/23 11:30:33 @INFO @Executor@ Running task 0.0 in stage 55.0 (TID 378)
21/11/23 11:30:33 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:33 @INFO @Executor@ Finished task 0.0 in stage 55.0 (TID 378). 2900 bytes result sent to driver
21/11/23 11:30:33 @INFO @TaskSetManager@ Finished task 0.0 in stage 55.0 (TID 378) in 68 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:33 @INFO @TaskSchedulerImpl@ Removed TaskSet 55.0, whose tasks have all completed, from pool 
21/11/23 11:30:33 @INFO @DAGScheduler@ ShuffleMapStage 55 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13) finished in 0.072 s
21/11/23 11:30:33 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:33 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:33 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:33 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Removed broadcast_49_piece0 on 192.168.0.30:64662 in memory (size: 34.4 KiB, free: 434.1 MiB)
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Removed broadcast_50_piece0 on 192.168.0.30:64662 in memory (size: 34.3 KiB, free: 434.1 MiB)
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Removed broadcast_54_piece0 on 192.168.0.30:64662 in memory (size: 58.3 KiB, free: 434.2 MiB)
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Removed broadcast_52_piece0 on 192.168.0.30:64662 in memory (size: 34.4 KiB, free: 434.2 MiB)
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Removed broadcast_56_piece0 on 192.168.0.30:64662 in memory (size: 34.7 KiB, free: 434.2 MiB)
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Removed broadcast_51_piece0 on 192.168.0.30:64662 in memory (size: 58.1 KiB, free: 434.3 MiB)
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Removed broadcast_53_piece0 on 192.168.0.30:64662 in memory (size: 34.7 KiB, free: 434.3 MiB)
21/11/23 11:30:33 @INFO @SparkContext@ Starting job: collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13
21/11/23 11:30:33 @INFO @DAGScheduler@ Got job 39 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13) with 1 output partitions
21/11/23 11:30:33 @INFO @DAGScheduler@ Final stage: ResultStage 57 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13)
21/11/23 11:30:33 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 56)
21/11/23 11:30:33 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:33 @INFO @DAGScheduler@ Submitting ResultStage 57 (MapPartitionsRDD[140] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13), which has no missing parents
21/11/23 11:30:33 @INFO @MemoryStore@ Block broadcast_57 stored as values in memory (estimated size 223.9 KiB, free 433.7 MiB)
21/11/23 11:30:33 @INFO @MemoryStore@ Block broadcast_57_piece0 stored as bytes in memory (estimated size 57.9 KiB, free 433.7 MiB)
21/11/23 11:30:33 @INFO @BlockManagerInfo@ Added broadcast_57_piece0 in memory on 192.168.0.30:64662 (size: 57.9 KiB, free: 434.3 MiB)
21/11/23 11:30:33 @INFO @SparkContext@ Created broadcast 57 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:33 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[140] at collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:33 @INFO @TaskSchedulerImpl@ Adding task set 57.0 with 1 tasks resource profile 0
21/11/23 11:30:33 @INFO @TaskSetManager@ Starting task 0.0 in stage 57.0 (TID 379) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:33 @INFO @Executor@ Running task 0.0 in stage 57.0 (TID 379)
21/11/23 11:30:33 @INFO @ShuffleBlockFetcherIterator@ Getting 1 (251.0 B) non-empty blocks including 1 (251.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:33 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:33 @INFO @CodeGenerator@ Code generated in 5.00775 ms
21/11/23 11:30:33 @INFO @Executor@ Finished task 0.0 in stage 57.0 (TID 379). 4136 bytes result sent to driver
21/11/23 11:30:33 @INFO @TaskSetManager@ Finished task 0.0 in stage 57.0 (TID 379) in 78 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:33 @INFO @TaskSchedulerImpl@ Removed TaskSet 57.0, whose tasks have all completed, from pool 
21/11/23 11:30:33 @INFO @DAGScheduler@ ResultStage 57 (collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13) finished in 0.083 s
21/11/23 11:30:33 @INFO @DAGScheduler@ Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:33 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 57: Stage finished
21/11/23 11:30:33 @INFO @DAGScheduler@ Job 39 finished: collect at /var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_54888/1442538006.py:13, took 0.083862 s
21/11/23 11:30:37 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:37 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:30:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:30:37 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_code
21/11/23 11:30:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_code	
21/11/23 11:30:37 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:37 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:37 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:37 @INFO @CodeGenerator@ Code generated in 3.6325 ms
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_58 stored as values in memory (estimated size 192.6 KiB, free 433.5 MiB)
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_58_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 433.5 MiB)
21/11/23 11:30:37 @INFO @BlockManagerInfo@ Added broadcast_58_piece0 in memory on 192.168.0.30:64662 (size: 33.7 KiB, free: 434.2 MiB)
21/11/23 11:30:37 @INFO @SparkContext@ Created broadcast 58 from 
21/11/23 11:30:37 @INFO @FileInputFormat@ Total input files to process : 1
21/11/23 11:30:37 @INFO @DAGScheduler@ Registering RDD 146 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 18
21/11/23 11:30:37 @INFO @DAGScheduler@ Got map stage job 40 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:37 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 58 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:37 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:37 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:37 @INFO @DAGScheduler@ Submitting ShuffleMapStage 58 (MapPartitionsRDD[146] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_59 stored as values in memory (estimated size 15.4 KiB, free 433.4 MiB)
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_59_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.4 MiB)
21/11/23 11:30:37 @INFO @BlockManagerInfo@ Added broadcast_59_piece0 in memory on 192.168.0.30:64662 (size: 7.7 KiB, free: 434.2 MiB)
21/11/23 11:30:37 @INFO @SparkContext@ Created broadcast 59 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:37 @INFO @DAGScheduler@ Submitting 1 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[146] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:37 @INFO @TaskSchedulerImpl@ Adding task set 58.0 with 1 tasks resource profile 0
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 0.0 in stage 58.0 (TID 380) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4635 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @Executor@ Running task 0.0 in stage 58.0 (TID 380)
21/11/23 11:30:37 @INFO @HadoopRDD@ Input split: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code/part-00000-9fa5a8ad-90ba-42b3-aff4-1bfc768f81a1-c000:0+4074
21/11/23 11:30:37 @INFO @Executor@ Finished task 0.0 in stage 58.0 (TID 380). 1916 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 0.0 in stage 58.0 (TID 380) in 29 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:37 @INFO @TaskSchedulerImpl@ Removed TaskSet 58.0, whose tasks have all completed, from pool 
21/11/23 11:30:37 @INFO @DAGScheduler@ ShuffleMapStage 58 (count at NativeMethodAccessorImpl.java:0) finished in 0.033 s
21/11/23 11:30:37 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:37 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:37 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:37 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:37 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:37 @INFO @DAGScheduler@ Got job 41 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:37 @INFO @DAGScheduler@ Final stage: ResultStage 60 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:37 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 59)
21/11/23 11:30:37 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:37 @INFO @DAGScheduler@ Submitting ResultStage 60 (MapPartitionsRDD[149] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_60 stored as values in memory (estimated size 11.0 KiB, free 433.4 MiB)
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_60_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.4 MiB)
21/11/23 11:30:37 @INFO @BlockManagerInfo@ Added broadcast_60_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.2 MiB)
21/11/23 11:30:37 @INFO @SparkContext@ Created broadcast 60 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:37 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[149] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:37 @INFO @TaskSchedulerImpl@ Adding task set 60.0 with 1 tasks resource profile 0
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 0.0 in stage 60.0 (TID 381) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @Executor@ Running task 0.0 in stage 60.0 (TID 381)
21/11/23 11:30:37 @INFO @ShuffleBlockFetcherIterator@ Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:37 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:37 @INFO @Executor@ Finished task 0.0 in stage 60.0 (TID 381). 2605 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 0.0 in stage 60.0 (TID 381) in 3 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:37 @INFO @TaskSchedulerImpl@ Removed TaskSet 60.0, whose tasks have all completed, from pool 
21/11/23 11:30:37 @INFO @DAGScheduler@ ResultStage 60 (count at NativeMethodAccessorImpl.java:0) finished in 0.004 s
21/11/23 11:30:37 @INFO @DAGScheduler@ Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:37 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 60: Stage finished
21/11/23 11:30:37 @INFO @DAGScheduler@ Job 41 finished: count at NativeMethodAccessorImpl.java:0, took 0.005217 s
21/11/23 11:30:37 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:37 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:37 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:37 @INFO @FileSourceStrategy@ Output Data Schema: struct<code: string>
21/11/23 11:30:37 @INFO @HashAggregateExec@ spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
21/11/23 11:30:37 @INFO @CodeGenerator@ Code generated in 6.878375 ms
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_61 stored as values in memory (estimated size 193.5 KiB, free 433.2 MiB)
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_61_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 433.2 MiB)
21/11/23 11:30:37 @INFO @BlockManagerInfo@ Added broadcast_61_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.2 MiB)
21/11/23 11:30:37 @INFO @SparkContext@ Created broadcast 61 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:37 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:37 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:37 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:37 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:37 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:37 @INFO @InMemoryFileIndex@ It took 2 ms to list leaf files for 50 paths.
21/11/23 11:30:37 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:37 @INFO @DAGScheduler@ Registering RDD 153 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 19
21/11/23 11:30:37 @INFO @DAGScheduler@ Got map stage job 42 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:37 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 61 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:37 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:37 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:37 @INFO @DAGScheduler@ Submitting ShuffleMapStage 61 (MapPartitionsRDD[153] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_62 stored as values in memory (estimated size 23.5 KiB, free 433.2 MiB)
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_62_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 433.2 MiB)
21/11/23 11:30:37 @INFO @BlockManagerInfo@ Added broadcast_62_piece0 in memory on 192.168.0.30:64662 (size: 10.5 KiB, free: 434.2 MiB)
21/11/23 11:30:37 @INFO @SparkContext@ Created broadcast 62 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:37 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 61 (MapPartitionsRDD[153] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:37 @INFO @TaskSchedulerImpl@ Adding task set 61.0 with 10 tasks resource profile 0
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 0.0 in stage 61.0 (TID 382) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 1.0 in stage 61.0 (TID 383) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 2.0 in stage 61.0 (TID 384) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 3.0 in stage 61.0 (TID 385) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 4.0 in stage 61.0 (TID 386) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 5.0 in stage 61.0 (TID 387) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 6.0 in stage 61.0 (TID 388) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 7.0 in stage 61.0 (TID 389) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 8.0 in stage 61.0 (TID 390) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 9.0 in stage 61.0 (TID 391) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @Executor@ Running task 2.0 in stage 61.0 (TID 384)
21/11/23 11:30:37 @INFO @Executor@ Running task 0.0 in stage 61.0 (TID 382)
21/11/23 11:30:37 @INFO @Executor@ Running task 4.0 in stage 61.0 (TID 386)
21/11/23 11:30:37 @INFO @Executor@ Running task 6.0 in stage 61.0 (TID 388)
21/11/23 11:30:37 @INFO @Executor@ Running task 5.0 in stage 61.0 (TID 387)
21/11/23 11:30:37 @INFO @Executor@ Running task 9.0 in stage 61.0 (TID 391)
21/11/23 11:30:37 @INFO @Executor@ Running task 8.0 in stage 61.0 (TID 390)
21/11/23 11:30:37 @INFO @Executor@ Running task 1.0 in stage 61.0 (TID 383)
21/11/23 11:30:37 @INFO @Executor@ Running task 7.0 in stage 61.0 (TID 389)
21/11/23 11:30:37 @INFO @Executor@ Running task 3.0 in stage 61.0 (TID 385)
21/11/23 11:30:37 @INFO @CodeGenerator@ Code generated in 3.871584 ms
21/11/23 11:30:37 @INFO @CodeGenerator@ Code generated in 2.699917 ms
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:37 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:37 @INFO @Executor@ Finished task 0.0 in stage 61.0 (TID 382). 3102 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 0.0 in stage 61.0 (TID 382) in 54 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:37 @INFO @Executor@ Finished task 1.0 in stage 61.0 (TID 383). 3102 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 1.0 in stage 61.0 (TID 383) in 55 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:37 @INFO @Executor@ Finished task 8.0 in stage 61.0 (TID 390). 3102 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 8.0 in stage 61.0 (TID 390) in 55 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:37 @INFO @Executor@ Finished task 6.0 in stage 61.0 (TID 388). 3102 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 6.0 in stage 61.0 (TID 388) in 56 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:37 @INFO @Executor@ Finished task 2.0 in stage 61.0 (TID 384). 3102 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 2.0 in stage 61.0 (TID 384) in 57 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:37 @INFO @Executor@ Finished task 5.0 in stage 61.0 (TID 387). 3102 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 5.0 in stage 61.0 (TID 387) in 58 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:37 @INFO @Executor@ Finished task 3.0 in stage 61.0 (TID 385). 3102 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 3.0 in stage 61.0 (TID 385) in 59 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:37 @INFO @Executor@ Finished task 4.0 in stage 61.0 (TID 386). 3102 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 4.0 in stage 61.0 (TID 386) in 59 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:37 @INFO @Executor@ Finished task 7.0 in stage 61.0 (TID 389). 3102 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 7.0 in stage 61.0 (TID 389) in 58 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:37 @INFO @Executor@ Finished task 9.0 in stage 61.0 (TID 391). 3102 bytes result sent to driver
21/11/23 11:30:37 @INFO @TaskSetManager@ Finished task 9.0 in stage 61.0 (TID 391) in 59 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:37 @INFO @TaskSchedulerImpl@ Removed TaskSet 61.0, whose tasks have all completed, from pool 
21/11/23 11:30:37 @INFO @DAGScheduler@ ShuffleMapStage 61 (count at NativeMethodAccessorImpl.java:0) finished in 0.063 s
21/11/23 11:30:37 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:37 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:37 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:37 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:37 @INFO @ShufflePartitionsUtil@ For shuffle(19), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
21/11/23 11:30:37 @INFO @HashAggregateExec@ spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
21/11/23 11:30:37 @INFO @CodeGenerator@ Code generated in 4.014167 ms
21/11/23 11:30:37 @INFO @DAGScheduler@ Registering RDD 156 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 20
21/11/23 11:30:37 @INFO @DAGScheduler@ Got map stage job 43 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:37 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 63 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:37 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 62)
21/11/23 11:30:37 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:37 @INFO @DAGScheduler@ Submitting ShuffleMapStage 63 (MapPartitionsRDD[156] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_63 stored as values in memory (estimated size 34.2 KiB, free 433.1 MiB)
21/11/23 11:30:37 @INFO @MemoryStore@ Block broadcast_63_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 433.1 MiB)
21/11/23 11:30:37 @INFO @BlockManagerInfo@ Added broadcast_63_piece0 in memory on 192.168.0.30:64662 (size: 16.2 KiB, free: 434.2 MiB)
21/11/23 11:30:37 @INFO @SparkContext@ Created broadcast 63 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:37 @INFO @DAGScheduler@ Submitting 1 missing tasks from ShuffleMapStage 63 (MapPartitionsRDD[156] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:37 @INFO @TaskSchedulerImpl@ Adding task set 63.0 with 1 tasks resource profile 0
21/11/23 11:30:37 @INFO @TaskSetManager@ Starting task 0.0 in stage 63.0 (TID 392) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()
21/11/23 11:30:37 @INFO @Executor@ Running task 0.0 in stage 63.0 (TID 392)
21/11/23 11:30:38 @INFO @ShuffleBlockFetcherIterator@ Getting 10 (3.5 KiB) non-empty blocks including 10 (3.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:38 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:38 @INFO @Executor@ Finished task 0.0 in stage 63.0 (TID 392). 4687 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 0.0 in stage 63.0 (TID 392) in 10 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Removed TaskSet 63.0, whose tasks have all completed, from pool 
21/11/23 11:30:38 @INFO @DAGScheduler@ ShuffleMapStage 63 (count at NativeMethodAccessorImpl.java:0) finished in 0.012 s
21/11/23 11:30:38 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:38 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:38 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:38 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:38 @INFO @CodeGenerator@ Code generated in 2.948583 ms
21/11/23 11:30:38 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:38 @INFO @DAGScheduler@ Got job 44 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:38 @INFO @DAGScheduler@ Final stage: ResultStage 66 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:38 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 65)
21/11/23 11:30:38 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:38 @INFO @DAGScheduler@ Submitting ResultStage 66 (MapPartitionsRDD[159] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_64 stored as values in memory (estimated size 11.0 KiB, free 433.1 MiB)
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_64_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.1 MiB)
21/11/23 11:30:38 @INFO @BlockManagerInfo@ Added broadcast_64_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.2 MiB)
21/11/23 11:30:38 @INFO @SparkContext@ Created broadcast 64 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:38 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[159] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Adding task set 66.0 with 1 tasks resource profile 0
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 0.0 in stage 66.0 (TID 393) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @Executor@ Running task 0.0 in stage 66.0 (TID 393)
21/11/23 11:30:38 @INFO @ShuffleBlockFetcherIterator@ Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:38 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:38 @INFO @Executor@ Finished task 0.0 in stage 66.0 (TID 393). 2605 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 0.0 in stage 66.0 (TID 393) in 3 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Removed TaskSet 66.0, whose tasks have all completed, from pool 
21/11/23 11:30:38 @INFO @DAGScheduler@ ResultStage 66 (count at NativeMethodAccessorImpl.java:0) finished in 0.006 s
21/11/23 11:30:38 @INFO @DAGScheduler@ Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 66: Stage finished
21/11/23 11:30:38 @INFO @DAGScheduler@ Job 44 finished: count at NativeMethodAccessorImpl.java:0, took 0.006278 s
21/11/23 11:30:38 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:38 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:38 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:38 @INFO @FileSourceStrategy@ Output Data Schema: struct<code: string>
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_65 stored as values in memory (estimated size 192.6 KiB, free 432.9 MiB)
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_65_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 432.9 MiB)
21/11/23 11:30:38 @INFO @BlockManagerInfo@ Added broadcast_65_piece0 in memory on 192.168.0.30:64662 (size: 33.7 KiB, free: 434.1 MiB)
21/11/23 11:30:38 @INFO @SparkContext@ Created broadcast 65 from 
21/11/23 11:30:38 @INFO @FileInputFormat@ Total input files to process : 1
21/11/23 11:30:38 @INFO @SparkContext@ Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
21/11/23 11:30:38 @INFO @DAGScheduler@ Got job 45 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
21/11/23 11:30:38 @INFO @DAGScheduler@ Final stage: ResultStage 67 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
21/11/23 11:30:38 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:38 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:38 @INFO @DAGScheduler@ Submitting ResultStage 67 (MapPartitionsRDD[164] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_66 stored as values in memory (estimated size 10.7 KiB, free 432.9 MiB)
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_66_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 432.9 MiB)
21/11/23 11:30:38 @INFO @BlockManagerInfo@ Added broadcast_66_piece0 in memory on 192.168.0.30:64662 (size: 5.6 KiB, free: 434.1 MiB)
21/11/23 11:30:38 @INFO @SparkContext@ Created broadcast 66 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:38 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[164] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Adding task set 67.0 with 1 tasks resource profile 0
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 0.0 in stage 67.0 (TID 394) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4646 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @Executor@ Running task 0.0 in stage 67.0 (TID 394)
21/11/23 11:30:38 @INFO @HadoopRDD@ Input split: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_code/part-00000-9fa5a8ad-90ba-42b3-aff4-1bfc768f81a1-c000:0+4074
21/11/23 11:30:38 @INFO @Executor@ Finished task 0.0 in stage 67.0 (TID 394). 1752 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 0.0 in stage 67.0 (TID 394) in 12 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Removed TaskSet 67.0, whose tasks have all completed, from pool 
21/11/23 11:30:38 @INFO @DAGScheduler@ ResultStage 67 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.015 s
21/11/23 11:30:38 @INFO @DAGScheduler@ Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 67: Stage finished
21/11/23 11:30:38 @INFO @DAGScheduler@ Job 45 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.015571 s
21/11/23 11:30:38 @INFO @CodeGenerator@ Code generated in 2.798333 ms
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_67 stored as values in memory (estimated size 2.0 MiB, free 430.9 MiB)
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_67_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 430.9 MiB)
21/11/23 11:30:38 @INFO @BlockManagerInfo@ Added broadcast_67_piece0 in memory on 192.168.0.30:64662 (size: 2.9 KiB, free: 434.1 MiB)
21/11/23 11:30:38 @INFO @SparkContext@ Created broadcast 67 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
21/11/23 11:30:38 @INFO @DataSourceStrategy@ Pruning directories with: 
21/11/23 11:30:38 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:38 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:38 @INFO @FileSourceStrategy@ Output Data Schema: struct<code: string>
21/11/23 11:30:38 @INFO @HashAggregateExec@ spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
21/11/23 11:30:38 @INFO @CodeGenerator@ Code generated in 6.131625 ms
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_68 stored as values in memory (estimated size 193.5 KiB, free 430.7 MiB)
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_68_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 430.6 MiB)
21/11/23 11:30:38 @INFO @BlockManagerInfo@ Added broadcast_68_piece0 in memory on 192.168.0.30:64662 (size: 34.0 KiB, free: 434.1 MiB)
21/11/23 11:30:38 @INFO @SparkContext@ Created broadcast 68 from count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:38 @INFO @HiveMetaStore@ 0: get_database: data_management_crush_course
21/11/23 11:30:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: data_management_crush_course	
21/11/23 11:30:38 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:38 @INFO @HiveMetaStore@ 0: get_table : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:38 @INFO @HiveMetaStore@ 0: get_partitions : db=data_management_crush_course tbl=jinko_table
21/11/23 11:30:38 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_partitions : db=data_management_crush_course tbl=jinko_table	
21/11/23 11:30:38 @INFO @InMemoryFileIndex@ It took 2 ms to list leaf files for 50 paths.
21/11/23 11:30:38 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 20983284 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:38 @INFO @DAGScheduler@ Registering RDD 168 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 21
21/11/23 11:30:38 @INFO @DAGScheduler@ Got map stage job 46 (count at NativeMethodAccessorImpl.java:0) with 10 output partitions
21/11/23 11:30:38 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 68 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:38 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:38 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:38 @INFO @DAGScheduler@ Submitting ShuffleMapStage 68 (MapPartitionsRDD[168] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_69 stored as values in memory (estimated size 39.3 KiB, free 430.6 MiB)
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_69_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 430.6 MiB)
21/11/23 11:30:38 @INFO @BlockManagerInfo@ Added broadcast_69_piece0 in memory on 192.168.0.30:64662 (size: 18.4 KiB, free: 434.1 MiB)
21/11/23 11:30:38 @INFO @SparkContext@ Created broadcast 69 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:38 @INFO @DAGScheduler@ Submitting 10 missing tasks from ShuffleMapStage 68 (MapPartitionsRDD[168] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Adding task set 68.0 with 10 tasks resource profile 0
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 0.0 in stage 68.0 (TID 395) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 1.0 in stage 68.0 (TID 396) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 2.0 in stage 68.0 (TID 397) (192.168.0.30, executor driver, partition 2, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 3.0 in stage 68.0 (TID 398) (192.168.0.30, executor driver, partition 3, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 4.0 in stage 68.0 (TID 399) (192.168.0.30, executor driver, partition 4, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 5.0 in stage 68.0 (TID 400) (192.168.0.30, executor driver, partition 5, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 6.0 in stage 68.0 (TID 401) (192.168.0.30, executor driver, partition 6, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 7.0 in stage 68.0 (TID 402) (192.168.0.30, executor driver, partition 7, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 8.0 in stage 68.0 (TID 403) (192.168.0.30, executor driver, partition 8, PROCESS_LOCAL, 6175 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 9.0 in stage 68.0 (TID 404) (192.168.0.30, executor driver, partition 9, PROCESS_LOCAL, 6169 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @Executor@ Running task 2.0 in stage 68.0 (TID 397)
21/11/23 11:30:38 @INFO @Executor@ Running task 0.0 in stage 68.0 (TID 395)
21/11/23 11:30:38 @INFO @Executor@ Running task 9.0 in stage 68.0 (TID 404)
21/11/23 11:30:38 @INFO @Executor@ Running task 6.0 in stage 68.0 (TID 401)
21/11/23 11:30:38 @INFO @Executor@ Running task 3.0 in stage 68.0 (TID 398)
21/11/23 11:30:38 @INFO @Executor@ Running task 1.0 in stage 68.0 (TID 396)
21/11/23 11:30:38 @INFO @Executor@ Running task 4.0 in stage 68.0 (TID 399)
21/11/23 11:30:38 @INFO @Executor@ Running task 5.0 in stage 68.0 (TID 400)
21/11/23 11:30:38 @INFO @Executor@ Running task 7.0 in stage 68.0 (TID 402)
21/11/23 11:30:38 @INFO @Executor@ Running task 8.0 in stage 68.0 (TID 403)
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=新潟県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [新潟県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大阪府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2361, partition values: [大阪府]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=高知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [高知県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=和歌山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2340, partition values: [和歌山県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛媛県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [愛媛県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山形県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2344, partition values: [山形県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=熊本県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [熊本県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=秋田県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2352, partition values: [秋田県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=愛知県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [愛知県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=全国/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2407, partition values: [全国]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2405, partition values: [人口集中地区]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=沖縄県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2348, partition values: [沖縄県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=香川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2339, partition values: [香川県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [宮崎県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=北海道/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2360, partition values: [北海道]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=神奈川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2375, partition values: [神奈川県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=人口集中地区以外の地区/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2396, partition values: [人口集中地区以外の地区]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山口県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [山口県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=栃木県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [栃木県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=三重県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [三重県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=山梨県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [山梨県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=島根県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2338, partition values: [島根県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=奈良県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [奈良県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=富山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [富山県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=埼玉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2373, partition values: [埼玉県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岐阜県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2350, partition values: [岐阜県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=佐賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [佐賀県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=東京都/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2378, partition values: [東京都]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=滋賀県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [滋賀県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岩手県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [岩手県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2356, partition values: [福島県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=徳島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [徳島県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=広島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [広島県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鹿児島県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2343, partition values: [鹿児島県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=群馬県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2347, partition values: [群馬県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=宮城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [宮城県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=千葉県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2376, partition values: [千葉県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福井県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2334, partition values: [福井県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=福岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [福岡県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=静岡県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2358, partition values: [静岡県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長崎県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2349, partition values: [長崎県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=鳥取県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2330, partition values: [鳥取県]
21/11/23 11:30:38 @INFO @Executor@ Finished task 5.0 in stage 68.0 (TID 400). 3422 bytes result sent to driver
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=石川県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2336, partition values: [石川県]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=京都府/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [京都府]
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=青森県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2341, partition values: [青森県]
21/11/23 11:30:38 @INFO @Executor@ Finished task 8.0 in stage 68.0 (TID 403). 3422 bytes result sent to driver
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=茨城県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2353, partition values: [茨城県]
21/11/23 11:30:38 @INFO @Executor@ Finished task 4.0 in stage 68.0 (TID 399). 3422 bytes result sent to driver
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=兵庫県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2357, partition values: [兵庫県]
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 5.0 in stage 68.0 (TID 400) in 29 ms on 192.168.0.30 (executor driver) (1/10)
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=長野県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2362, partition values: [長野県]
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 8.0 in stage 68.0 (TID 403) in 29 ms on 192.168.0.30 (executor driver) (2/10)
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 4.0 in stage 68.0 (TID 399) in 30 ms on 192.168.0.30 (executor driver) (3/10)
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=大分県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [大分県]
21/11/23 11:30:38 @INFO @Executor@ Finished task 7.0 in stage 68.0 (TID 402). 3422 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 7.0 in stage 68.0 (TID 402) in 30 ms on 192.168.0.30 (executor driver) (4/10)
21/11/23 11:30:38 @INFO @Executor@ Finished task 9.0 in stage 68.0 (TID 404). 3422 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 9.0 in stage 68.0 (TID 404) in 31 ms on 192.168.0.30 (executor driver) (5/10)
21/11/23 11:30:38 @INFO @FileScanRDD@ Reading File path: file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/data_management_crush_course.db/jinko_table/kenmei=岡山県/part-00000-0ebed64a-9505-4db5-af0b-d7d809c19859.c000.snappy.parquet, range: 0-2342, partition values: [岡山県]
21/11/23 11:30:38 @INFO @Executor@ Finished task 2.0 in stage 68.0 (TID 397). 3422 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 2.0 in stage 68.0 (TID 397) in 31 ms on 192.168.0.30 (executor driver) (6/10)
21/11/23 11:30:38 @INFO @Executor@ Finished task 3.0 in stage 68.0 (TID 398). 3422 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 3.0 in stage 68.0 (TID 398) in 31 ms on 192.168.0.30 (executor driver) (7/10)
21/11/23 11:30:38 @INFO @Executor@ Finished task 6.0 in stage 68.0 (TID 401). 3422 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 6.0 in stage 68.0 (TID 401) in 33 ms on 192.168.0.30 (executor driver) (8/10)
21/11/23 11:30:38 @INFO @Executor@ Finished task 0.0 in stage 68.0 (TID 395). 3551 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 0.0 in stage 68.0 (TID 395) in 34 ms on 192.168.0.30 (executor driver) (9/10)
21/11/23 11:30:38 @INFO @Executor@ Finished task 1.0 in stage 68.0 (TID 396). 3551 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 1.0 in stage 68.0 (TID 396) in 36 ms on 192.168.0.30 (executor driver) (10/10)
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Removed TaskSet 68.0, whose tasks have all completed, from pool 
21/11/23 11:30:38 @INFO @DAGScheduler@ ShuffleMapStage 68 (count at NativeMethodAccessorImpl.java:0) finished in 0.039 s
21/11/23 11:30:38 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:38 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:38 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:38 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:38 @INFO @ShufflePartitionsUtil@ For shuffle(21), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
21/11/23 11:30:38 @INFO @HashAggregateExec@ spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
21/11/23 11:30:38 @INFO @DAGScheduler@ Registering RDD 171 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 22
21/11/23 11:30:38 @INFO @DAGScheduler@ Got map stage job 47 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:38 @INFO @DAGScheduler@ Final stage: ShuffleMapStage 70 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:38 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 69)
21/11/23 11:30:38 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:38 @INFO @DAGScheduler@ Submitting ShuffleMapStage 70 (MapPartitionsRDD[171] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_70 stored as values in memory (estimated size 42.1 KiB, free 430.5 MiB)
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_70_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 430.5 MiB)
21/11/23 11:30:38 @INFO @BlockManagerInfo@ Added broadcast_70_piece0 in memory on 192.168.0.30:64662 (size: 20.2 KiB, free: 434.1 MiB)
21/11/23 11:30:38 @INFO @SparkContext@ Created broadcast 70 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:38 @INFO @DAGScheduler@ Submitting 1 missing tasks from ShuffleMapStage 70 (MapPartitionsRDD[171] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Adding task set 70.0 with 1 tasks resource profile 0
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 0.0 in stage 70.0 (TID 405) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @Executor@ Running task 0.0 in stage 70.0 (TID 405)
21/11/23 11:30:38 @INFO @ShuffleBlockFetcherIterator@ Getting 2 (144.0 B) non-empty blocks including 2 (144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:38 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:38 @INFO @Executor@ Finished task 0.0 in stage 70.0 (TID 405). 5136 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 0.0 in stage 70.0 (TID 405) in 4 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Removed TaskSet 70.0, whose tasks have all completed, from pool 
21/11/23 11:30:38 @INFO @DAGScheduler@ ShuffleMapStage 70 (count at NativeMethodAccessorImpl.java:0) finished in 0.007 s
21/11/23 11:30:38 @INFO @DAGScheduler@ looking for newly runnable stages
21/11/23 11:30:38 @INFO @DAGScheduler@ running: Set()
21/11/23 11:30:38 @INFO @DAGScheduler@ waiting: Set()
21/11/23 11:30:38 @INFO @DAGScheduler@ failed: Set()
21/11/23 11:30:38 @INFO @SparkContext@ Starting job: count at NativeMethodAccessorImpl.java:0
21/11/23 11:30:38 @INFO @DAGScheduler@ Got job 48 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:38 @INFO @DAGScheduler@ Final stage: ResultStage 73 (count at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:38 @INFO @DAGScheduler@ Parents of final stage: List(ShuffleMapStage 72)
21/11/23 11:30:38 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:38 @INFO @DAGScheduler@ Submitting ResultStage 73 (MapPartitionsRDD[174] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_71 stored as values in memory (estimated size 11.0 KiB, free 430.5 MiB)
21/11/23 11:30:38 @INFO @MemoryStore@ Block broadcast_71_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 430.5 MiB)
21/11/23 11:30:38 @INFO @BlockManagerInfo@ Added broadcast_71_piece0 in memory on 192.168.0.30:64662 (size: 5.5 KiB, free: 434.0 MiB)
21/11/23 11:30:38 @INFO @SparkContext@ Created broadcast 71 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:38 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[174] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Adding task set 73.0 with 1 tasks resource profile 0
21/11/23 11:30:38 @INFO @TaskSetManager@ Starting task 0.0 in stage 73.0 (TID 406) (192.168.0.30, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21/11/23 11:30:38 @INFO @Executor@ Running task 0.0 in stage 73.0 (TID 406)
21/11/23 11:30:38 @INFO @ShuffleBlockFetcherIterator@ Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21/11/23 11:30:38 @INFO @ShuffleBlockFetcherIterator@ Started 0 remote fetches in 0 ms
21/11/23 11:30:38 @INFO @Executor@ Finished task 0.0 in stage 73.0 (TID 406). 2562 bytes result sent to driver
21/11/23 11:30:38 @INFO @TaskSetManager@ Finished task 0.0 in stage 73.0 (TID 406) in 1 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Removed TaskSet 73.0, whose tasks have all completed, from pool 
21/11/23 11:30:38 @INFO @DAGScheduler@ ResultStage 73 (count at NativeMethodAccessorImpl.java:0) finished in 0.003 s
21/11/23 11:30:38 @INFO @DAGScheduler@ Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:38 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 73: Stage finished
21/11/23 11:30:38 @INFO @DAGScheduler@ Job 48 finished: count at NativeMethodAccessorImpl.java:0, took 0.004197 s
21/11/23 11:30:40 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:30:40 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:30:40 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:40 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:40 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:40 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:40 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:40 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:40 @INFO @FileSourceStrategy@ Output Data Schema: struct<database_name: string, table_name: string, table_definition: string, sammary: string, record_num: string ... 6 more fields>
21/11/23 11:30:40 @INFO @CodeGenerator@ Code generated in 4.893709 ms
21/11/23 11:30:40 @INFO @MemoryStore@ Block broadcast_72 stored as values in memory (estimated size 194.4 KiB, free 430.3 MiB)
21/11/23 11:30:40 @INFO @MemoryStore@ Block broadcast_72_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 430.3 MiB)
21/11/23 11:30:40 @INFO @BlockManagerInfo@ Added broadcast_72_piece0 in memory on 192.168.0.30:64662 (size: 34.4 KiB, free: 434.0 MiB)
21/11/23 11:30:40 @INFO @SparkContext@ Created broadcast 72 from showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:40 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:40 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:40 @INFO @DAGScheduler@ Got job 49 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:40 @INFO @DAGScheduler@ Final stage: ResultStage 74 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:40 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:40 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:40 @INFO @DAGScheduler@ Submitting ResultStage 74 (MapPartitionsRDD[178] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:40 @INFO @MemoryStore@ Block broadcast_73 stored as values in memory (estimated size 18.0 KiB, free 430.3 MiB)
21/11/23 11:30:40 @INFO @MemoryStore@ Block broadcast_73_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 430.2 MiB)
21/11/23 11:30:40 @INFO @BlockManagerInfo@ Added broadcast_73_piece0 in memory on 192.168.0.30:64662 (size: 6.9 KiB, free: 434.0 MiB)
21/11/23 11:30:40 @INFO @SparkContext@ Created broadcast 73 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:40 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 74 (MapPartitionsRDD[178] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:40 @INFO @TaskSchedulerImpl@ Adding task set 74.0 with 1 tasks resource profile 0
21/11/23 11:30:40 @INFO @TaskSetManager@ Starting task 0.0 in stage 74.0 (TID 407) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 5004 bytes) taskResourceAssignments Map()
21/11/23 11:30:40 @INFO @Executor@ Running task 0.0 in stage 74.0 (TID 407)
21/11/23 11:30:40 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata/part-00009-e9c0b82c-259e-48fa-8ba0-ba5b20f74dd2-c000.snappy.parquet, range: 0-5047, partition values: [empty row]
21/11/23 11:30:40 @INFO @Executor@ Finished task 0.0 in stage 74.0 (TID 407). 2154 bytes result sent to driver
21/11/23 11:30:40 @INFO @TaskSetManager@ Finished task 0.0 in stage 74.0 (TID 407) in 6 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:40 @INFO @TaskSchedulerImpl@ Removed TaskSet 74.0, whose tasks have all completed, from pool 
21/11/23 11:30:40 @INFO @DAGScheduler@ ResultStage 74 (showString at NativeMethodAccessorImpl.java:0) finished in 0.010 s
21/11/23 11:30:40 @INFO @DAGScheduler@ Job 49 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:40 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 74: Stage finished
21/11/23 11:30:40 @INFO @DAGScheduler@ Job 49 finished: showString at NativeMethodAccessorImpl.java:0, took 0.011127 s
21/11/23 11:30:40 @INFO @SparkContext@ Starting job: showString at NativeMethodAccessorImpl.java:0
21/11/23 11:30:40 @INFO @DAGScheduler@ Got job 50 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:40 @INFO @DAGScheduler@ Final stage: ResultStage 75 (showString at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:40 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:40 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:40 @INFO @DAGScheduler@ Submitting ResultStage 75 (MapPartitionsRDD[178] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:40 @INFO @MemoryStore@ Block broadcast_74 stored as values in memory (estimated size 18.0 KiB, free 430.2 MiB)
21/11/23 11:30:40 @INFO @MemoryStore@ Block broadcast_74_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 430.2 MiB)
21/11/23 11:30:40 @INFO @BlockManagerInfo@ Added broadcast_74_piece0 in memory on 192.168.0.30:64662 (size: 6.9 KiB, free: 434.0 MiB)
21/11/23 11:30:40 @INFO @SparkContext@ Created broadcast 74 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:40 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[178] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))
21/11/23 11:30:40 @INFO @TaskSchedulerImpl@ Adding task set 75.0 with 1 tasks resource profile 0
21/11/23 11:30:40 @INFO @TaskSetManager@ Starting task 0.0 in stage 75.0 (TID 408) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 5004 bytes) taskResourceAssignments Map()
21/11/23 11:30:40 @INFO @Executor@ Running task 0.0 in stage 75.0 (TID 408)
21/11/23 11:30:40 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata/part-00000-e9c0b82c-259e-48fa-8ba0-ba5b20f74dd2-c000.snappy.parquet, range: 0-1012, partition values: [empty row]
21/11/23 11:30:40 @INFO @Executor@ Finished task 0.0 in stage 75.0 (TID 408). 1555 bytes result sent to driver
21/11/23 11:30:40 @INFO @TaskSetManager@ Finished task 0.0 in stage 75.0 (TID 408) in 3 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:40 @INFO @TaskSchedulerImpl@ Removed TaskSet 75.0, whose tasks have all completed, from pool 
21/11/23 11:30:40 @INFO @DAGScheduler@ ResultStage 75 (showString at NativeMethodAccessorImpl.java:0) finished in 0.005 s
21/11/23 11:30:40 @INFO @DAGScheduler@ Job 50 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:40 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 75: Stage finished
21/11/23 11:30:40 @INFO @DAGScheduler@ Job 50 finished: showString at NativeMethodAccessorImpl.java:0, took 0.005229 s
21/11/23 11:30:40 @INFO @CodeGenerator@ Code generated in 3.545666 ms
21/11/23 11:30:44 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:30:44 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:30:44 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:44 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:44 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:44 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:44 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:30:44 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:30:44 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:44 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:44 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:44 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:44 @INFO @InMemoryFileIndex@ It took 0 ms to list leaf files for 1 paths.
21/11/23 11:30:44 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:44 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:44 @INFO @FileSourceStrategy@ Output Data Schema: struct<database_name: string, table_name: string, table_definition: string, sammary: string, record_num: string ... 6 more fields>
21/11/23 11:30:44 @INFO @ParquetFileFormat@ Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:44 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:30:44 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:30:44 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:44 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:30:44 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:30:44 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:44 @INFO @CodeGenerator@ Code generated in 4.139166 ms
21/11/23 11:30:44 @INFO @MemoryStore@ Block broadcast_75 stored as values in memory (estimated size 194.4 KiB, free 430.0 MiB)
21/11/23 11:30:44 @INFO @MemoryStore@ Block broadcast_75_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 430.0 MiB)
21/11/23 11:30:44 @INFO @BlockManagerInfo@ Added broadcast_75_piece0 in memory on 192.168.0.30:64662 (size: 34.4 KiB, free: 434.0 MiB)
21/11/23 11:30:44 @INFO @SparkContext@ Created broadcast 75 from parquet at NativeMethodAccessorImpl.java:0
21/11/23 11:30:44 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:44 @INFO @SparkContext@ Starting job: parquet at NativeMethodAccessorImpl.java:0
21/11/23 11:30:44 @INFO @DAGScheduler@ Got job 51 (parquet at NativeMethodAccessorImpl.java:0) with 2 output partitions
21/11/23 11:30:44 @INFO @DAGScheduler@ Final stage: ResultStage 76 (parquet at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:44 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:44 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:44 @INFO @DAGScheduler@ Submitting ResultStage 76 (MapPartitionsRDD[181] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:44 @INFO @MemoryStore@ Block broadcast_76 stored as values in memory (estimated size 215.7 KiB, free 429.8 MiB)
21/11/23 11:30:44 @INFO @MemoryStore@ Block broadcast_76_piece0 stored as bytes in memory (estimated size 76.6 KiB, free 429.7 MiB)
21/11/23 11:30:44 @INFO @BlockManagerInfo@ Added broadcast_76_piece0 in memory on 192.168.0.30:64662 (size: 76.6 KiB, free: 433.9 MiB)
21/11/23 11:30:44 @INFO @SparkContext@ Created broadcast 76 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:44 @INFO @DAGScheduler@ Submitting 2 missing tasks from ResultStage 76 (MapPartitionsRDD[181] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
21/11/23 11:30:44 @INFO @TaskSchedulerImpl@ Adding task set 76.0 with 2 tasks resource profile 0
21/11/23 11:30:44 @INFO @TaskSetManager@ Starting task 0.0 in stage 76.0 (TID 409) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 5004 bytes) taskResourceAssignments Map()
21/11/23 11:30:44 @INFO @TaskSetManager@ Starting task 1.0 in stage 76.0 (TID 410) (192.168.0.30, executor driver, partition 1, PROCESS_LOCAL, 5004 bytes) taskResourceAssignments Map()
21/11/23 11:30:44 @INFO @Executor@ Running task 0.0 in stage 76.0 (TID 409)
21/11/23 11:30:44 @INFO @Executor@ Running task 1.0 in stage 76.0 (TID 410)
21/11/23 11:30:44 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:30:44 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:30:44 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:30:44 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:30:44 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:44 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:30:44 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:30:44 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:44 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:30:44 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:30:44 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:44 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:44 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata/part-00000-e9c0b82c-259e-48fa-8ba0-ba5b20f74dd2-c000.snappy.parquet, range: 0-1012, partition values: [empty row]
21/11/23 11:30:44 @INFO @SparkHadoopMapRedUtil@ No need to commit output of task because needsTaskCommit=false: attempt_202111231130441814037349894466015_0076_m_000001_410
21/11/23 11:30:44 @INFO @Executor@ Finished task 1.0 in stage 76.0 (TID 410). 2637 bytes result sent to driver
21/11/23 11:30:44 @INFO @TaskSetManager@ Finished task 1.0 in stage 76.0 (TID 410) in 22 ms on 192.168.0.30 (executor driver) (1/2)
21/11/23 11:30:44 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "database_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "table_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "table_definition",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sammary",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "record_num",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "selectivity",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "consistency_flag",
    "type" : "boolean",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "frequency_access",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary database_name (STRING);
  optional binary table_name (STRING);
  optional binary table_definition (STRING);
  optional binary sammary (STRING);
  optional binary record_num (STRING);
  optional binary selectivity (STRING);
  optional boolean consistency_flag;
  optional binary frequency_access (STRING);
}

       
21/11/23 11:30:44 @INFO @CodecPool@ Got brand-new compressor [.snappy]
21/11/23 11:30:45 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata/part-00009-e9c0b82c-259e-48fa-8ba0-ba5b20f74dd2-c000.snappy.parquet, range: 0-5047, partition values: [empty row]
21/11/23 11:30:45 @INFO @FileOutputCommitter@ Saved output of task 'attempt_20211123113044180125438152577925_0076_m_000000_409' to file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/tmp/_temporary/0/task_20211123113044180125438152577925_0076_m_000000
21/11/23 11:30:45 @INFO @SparkHadoopMapRedUtil@ attempt_20211123113044180125438152577925_0076_m_000000_409: Committed
21/11/23 11:30:45 @INFO @Executor@ Finished task 0.0 in stage 76.0 (TID 409). 2723 bytes result sent to driver
21/11/23 11:30:45 @INFO @TaskSetManager@ Finished task 0.0 in stage 76.0 (TID 409) in 110 ms on 192.168.0.30 (executor driver) (2/2)
21/11/23 11:30:45 @INFO @TaskSchedulerImpl@ Removed TaskSet 76.0, whose tasks have all completed, from pool 
21/11/23 11:30:45 @INFO @DAGScheduler@ ResultStage 76 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.127 s
21/11/23 11:30:45 @INFO @DAGScheduler@ Job 51 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:45 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 76: Stage finished
21/11/23 11:30:45 @INFO @DAGScheduler@ Job 51 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.127795 s
21/11/23 11:30:45 @INFO @FileFormatWriter@ Start to commit write Job 55241449-7299-4bd3-92ee-dac3c823e7d7.
21/11/23 11:30:45 @INFO @FileFormatWriter@ Write Job 55241449-7299-4bd3-92ee-dac3c823e7d7 committed. Elapsed time: 10 ms.
21/11/23 11:30:45 @INFO @FileFormatWriter@ Finished processing stats for write job 55241449-7299-4bd3-92ee-dac3c823e7d7.
21/11/23 11:30:45 @INFO @InMemoryFileIndex@ It took 0 ms to list leaf files for 1 paths.
21/11/23 11:30:45 @INFO @SparkContext@ Starting job: parquet at NativeMethodAccessorImpl.java:0
21/11/23 11:30:45 @INFO @DAGScheduler@ Got job 52 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/11/23 11:30:45 @INFO @DAGScheduler@ Final stage: ResultStage 77 (parquet at NativeMethodAccessorImpl.java:0)
21/11/23 11:30:45 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:45 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:45 @INFO @DAGScheduler@ Submitting ResultStage 77 (MapPartitionsRDD[183] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
21/11/23 11:30:45 @INFO @MemoryStore@ Block broadcast_77 stored as values in memory (estimated size 101.3 KiB, free 429.6 MiB)
21/11/23 11:30:45 @INFO @MemoryStore@ Block broadcast_77_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 429.6 MiB)
21/11/23 11:30:45 @INFO @BlockManagerInfo@ Added broadcast_77_piece0 in memory on 192.168.0.30:64662 (size: 36.5 KiB, free: 433.9 MiB)
21/11/23 11:30:45 @INFO @SparkContext@ Created broadcast 77 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:45 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[183] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:45 @INFO @TaskSchedulerImpl@ Adding task set 77.0 with 1 tasks resource profile 0
21/11/23 11:30:45 @INFO @TaskSetManager@ Starting task 0.0 in stage 77.0 (TID 411) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4697 bytes) taskResourceAssignments Map()
21/11/23 11:30:45 @INFO @Executor@ Running task 0.0 in stage 77.0 (TID 411)
21/11/23 11:30:45 @INFO @Executor@ Finished task 0.0 in stage 77.0 (TID 411). 1972 bytes result sent to driver
21/11/23 11:30:45 @INFO @TaskSetManager@ Finished task 0.0 in stage 77.0 (TID 411) in 12 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:45 @INFO @TaskSchedulerImpl@ Removed TaskSet 77.0, whose tasks have all completed, from pool 
21/11/23 11:30:45 @INFO @DAGScheduler@ ResultStage 77 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.041 s
21/11/23 11:30:45 @INFO @DAGScheduler@ Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:45 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 77: Stage finished
21/11/23 11:30:45 @INFO @DAGScheduler@ Job 52 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.042537 s
21/11/23 11:30:45 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:30:45 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:30:45 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:45 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:45 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:45 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:45 @INFO @InMemoryFileIndex@ It took 0 ms to list leaf files for 1 paths.
21/11/23 11:30:45 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:45 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:45 @INFO @FileSourceStrategy@ Output Data Schema: struct<database_name: string, table_name: string, table_definition: string, sammary: string, record_num: string ... 6 more fields>
21/11/23 11:30:45 @INFO @ParquetFileFormat@ Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:45 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:30:45 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:30:45 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:45 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:30:45 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:30:45 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:45 @INFO @CodeGenerator@ Code generated in 5.610917 ms
21/11/23 11:30:45 @INFO @MemoryStore@ Block broadcast_78 stored as values in memory (estimated size 194.1 KiB, free 429.4 MiB)
21/11/23 11:30:45 @INFO @MemoryStore@ Block broadcast_78_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 429.4 MiB)
21/11/23 11:30:45 @INFO @BlockManagerInfo@ Added broadcast_78_piece0 in memory on 192.168.0.30:64662 (size: 34.3 KiB, free: 433.8 MiB)
21/11/23 11:30:45 @INFO @SparkContext@ Created broadcast 78 from sql at <unknown>:0
21/11/23 11:30:45 @INFO @FileSourceScanExec@ Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
21/11/23 11:30:45 @INFO @SparkContext@ Starting job: sql at <unknown>:0
21/11/23 11:30:45 @INFO @DAGScheduler@ Got job 53 (sql at <unknown>:0) with 1 output partitions
21/11/23 11:30:45 @INFO @DAGScheduler@ Final stage: ResultStage 78 (sql at <unknown>:0)
21/11/23 11:30:45 @INFO @DAGScheduler@ Parents of final stage: List()
21/11/23 11:30:45 @INFO @DAGScheduler@ Missing parents: List()
21/11/23 11:30:45 @INFO @DAGScheduler@ Submitting ResultStage 78 (MapPartitionsRDD[186] at sql at <unknown>:0), which has no missing parents
21/11/23 11:30:45 @INFO @MemoryStore@ Block broadcast_79 stored as values in memory (estimated size 214.8 KiB, free 429.1 MiB)
21/11/23 11:30:45 @INFO @MemoryStore@ Block broadcast_79_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 429.1 MiB)
21/11/23 11:30:45 @INFO @BlockManagerInfo@ Added broadcast_79_piece0 in memory on 192.168.0.30:64662 (size: 76.3 KiB, free: 433.7 MiB)
21/11/23 11:30:45 @INFO @SparkContext@ Created broadcast 79 from broadcast at DAGScheduler.scala:1427
21/11/23 11:30:45 @INFO @DAGScheduler@ Submitting 1 missing tasks from ResultStage 78 (MapPartitionsRDD[186] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))
21/11/23 11:30:45 @INFO @TaskSchedulerImpl@ Adding task set 78.0 with 1 tasks resource profile 0
21/11/23 11:30:45 @INFO @TaskSetManager@ Starting task 0.0 in stage 78.0 (TID 412) (192.168.0.30, executor driver, partition 0, PROCESS_LOCAL, 4976 bytes) taskResourceAssignments Map()
21/11/23 11:30:45 @INFO @Executor@ Running task 0.0 in stage 78.0 (TID 412)
21/11/23 11:30:45 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:30:45 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:30:45 @INFO @SQLHadoopMapReduceCommitProtocol@ Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:45 @INFO @FileOutputCommitter@ File Output Committer Algorithm version is 1
21/11/23 11:30:45 @INFO @FileOutputCommitter@ FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
21/11/23 11:30:45 @INFO @SQLHadoopMapReduceCommitProtocol@ Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/11/23 11:30:45 @INFO @ParquetWriteSupport@ Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "database_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "table_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "table_definition",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sammary",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "record_num",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "selectivity",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "consistency_flag",
    "type" : "boolean",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "frequency_access",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary database_name (STRING);
  optional binary table_name (STRING);
  optional binary table_definition (STRING);
  optional binary sammary (STRING);
  optional binary record_num (STRING);
  optional binary selectivity (STRING);
  optional boolean consistency_flag;
  optional binary frequency_access (STRING);
}

       
21/11/23 11:30:45 @INFO @FileScanRDD@ Reading File path: file:///Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/tmp/part-00000-30a73401-5618-4112-bf6f-4c9495e4d536-c000.snappy.parquet, range: 0-5134, partition values: [empty row]
21/11/23 11:30:45 @INFO @FileOutputCommitter@ Saved output of task 'attempt_202111231130451131418069657132005_0078_m_000000_412' to file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata/_temporary/0/task_202111231130451131418069657132005_0078_m_000000
21/11/23 11:30:45 @INFO @SparkHadoopMapRedUtil@ attempt_202111231130451131418069657132005_0078_m_000000_412: Committed
21/11/23 11:30:45 @INFO @Executor@ Finished task 0.0 in stage 78.0 (TID 412). 2723 bytes result sent to driver
21/11/23 11:30:45 @INFO @TaskSetManager@ Finished task 0.0 in stage 78.0 (TID 412) in 69 ms on 192.168.0.30 (executor driver) (1/1)
21/11/23 11:30:45 @INFO @TaskSchedulerImpl@ Removed TaskSet 78.0, whose tasks have all completed, from pool 
21/11/23 11:30:45 @INFO @DAGScheduler@ ResultStage 78 (sql at <unknown>:0) finished in 0.085 s
21/11/23 11:30:45 @INFO @DAGScheduler@ Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
21/11/23 11:30:45 @INFO @TaskSchedulerImpl@ Killing all running tasks in stage 78: Stage finished
21/11/23 11:30:45 @INFO @DAGScheduler@ Job 53 finished: sql at <unknown>:0, took 0.103354 s
21/11/23 11:30:45 @INFO @FileFormatWriter@ Start to commit write Job ac63e250-85f5-49ad-aa66-7d79a9fa514c.
21/11/23 11:30:45 @INFO @FileFormatWriter@ Write Job ac63e250-85f5-49ad-aa66-7d79a9fa514c committed. Elapsed time: 19 ms.
21/11/23 11:30:45 @INFO @FileFormatWriter@ Finished processing stats for write job ac63e250-85f5-49ad-aa66-7d79a9fa514c.
21/11/23 11:30:45 @INFO @InMemoryFileIndex@ It took 0 ms to list leaf files for 1 paths.
21/11/23 11:30:45 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:30:45 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:30:45 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:45 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:45 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:45 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:45 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:45 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:45 @INFO @HiveMetaStore@ 0: alter_table: db=metadata_tmp tbl=sample_metadata newtbl=sample_metadata
21/11/23 11:30:45 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=alter_table: db=metadata_tmp tbl=sample_metadata newtbl=sample_metadata	
21/11/23 11:30:45 @INFO @log@ Updating table stats fast for sample_metadata
21/11/23 11:30:45 @INFO @log@ Updated size of table sample_metadata to 5134
21/11/23 11:30:47 @INFO @HiveMetaStore@ 0: get_database: metadata_tmp
21/11/23 11:30:47 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_database: metadata_tmp	
21/11/23 11:30:47 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:47 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:47 @INFO @HiveMetaStore@ 0: get_table : db=metadata_tmp tbl=sample_metadata
21/11/23 11:30:47 @INFO @audit@ ugi=saitouyuuki	ip=unknown-ip-addr	cmd=get_table : db=metadata_tmp tbl=sample_metadata	
21/11/23 11:30:47 @INFO @InMemoryFileIndex@ It took 0 ms to list leaf files for 1 paths.
21/11/23 11:30:47 @INFO @FileSourceStrategy@ Pushed Filters: 
21/11/23 11:30:47 @INFO @FileSourceStrategy@ Post-Scan Filters: 
21/11/23 11:30:47 @INFO @FileSourceStrategy@ Output Data Schema: struct<database_name: string, table_name: string, table_definition: string, sammary: string, record_num: string ... 6 more fields>

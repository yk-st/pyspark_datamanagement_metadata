{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. データマネジメントとは何者のなのか？\n",
    "2. メタデータとは？\n",
    "3. ビッグデータ基盤にメタデータが必要な理由\n",
    "4. メタデータを保存するメタデータストアとは\n",
    "5. ビッグデータ基盤におけるメタデータの提供形態\n",
    "6. 3種類存在するメタデータ\n",
    "7. 次のセクションへの導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データマネジメントとは何者なのか？\n",
    "データアーキテクチャ、マスタデータ、データウェアハウスなどの概念に対しての課題点や解決への糸口となるヒントを提供してくれる知識体型のことです。\n",
    "\n",
    "データマネジメントの基本は、データ利用のことを考えデータ利用の生産性向上を目的としたデータをどのように管理したら良いのか？\n",
    "という情報をまとめたものと考えると良いでしょう。\n",
    "\n",
    "データマネジメントの一環として本コースで紹介する「メタデータ」という概念が出てきます。\n",
    "\n",
    "※\n",
    "ご興味のある方は、外部リソースにデータマネジメント関連の本をリンクしておきます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータとは？\n",
    "メタデータとは、ビッグデータ基盤におけるメタデータは、データの設計書を表すものでデータの属性を表すデータのことです。  \n",
    "例えば、30という数字だけでは人間は、30という数字が何者なのか判断することができません。  \n",
    "そこで、km/hというメタデータを付けると速度制限か実際の速度なのか？どちらかに絞り込みができるようになってきます。  \n",
    "そしてlimitというメタデータを付けると人は速度制限と理解することが可能になります。  \n",
    "\n",
    "Mysqlの世界であれば　speed_limit_khみたいなカラムの定義になっているイメージを持ってもらえればと思います。\n",
    "\n",
    "この例は後に出てくる、ビジネスメタデータの一例です。\n",
    "メタデータには3種類あり、「ビジネスメタデータ」「テクニカルメタデータ」「オペレーショナルメタデータ」の3種類が存在しています。\n",
    "\n",
    "ビジネスメタデータとは、テーブル定義が最も有名なビジネスメタデータです。  \n",
    "テクニカルメタデータとは、テーブルのデータの状態であったり、データがどこに保存されているのか？といった技術的な情報を表すメタデータです  \n",
    "オペレーショナルメタデータとは、データに関する5w1hを示すメタデータで、データの生成するまでにどのようなことが起こっていたのかを表現するメタデータです。  \n",
    "\n",
    "となります。  \n",
    "それぞれのメタデータの取得や説明はセクション４、５、６で行っていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータ基盤にメタデータが必要な理由\n",
    "\n",
    "ビッグデータ基盤におけるメタデータの役割は、究極的には生産性の向上です。\n",
    "\n",
    "ビジネスメタデータはいわゆるテーブル定義に近いものですが、普段の開発でもER図などテーブルやデータに関する設計書を残すことが多いと思います(更新されているにしろされていないしろ)  \n",
    "ビッグデータ基盤も同じです。ただし、ビッグデータ基盤は複数のシステムのデータが一堂に会するシステムで利用する人は全社員といったことになるのがほとんどです。  \n",
    "そのため、「少人数の間だけで簡単な設計書を作って共有」というだけは分析という作業は進まないのです。  \n",
    "\n",
    "特にビッグデータを取り巻くシステムが全て統一されたルールで作成されていればいいのですが実際はそうではありません。  \n",
    "ちょっとした違いやちょっとした慣習が混じり合い意味不明になることがほとんどです。  \n",
    "\n",
    "そのためにもデータに関する情報、すなわちメタデータをしっかりと整理していかなければ誰も理解できないただのデータのゴミ置き場になってしまうということです（データの沼と呼ばれることがあります）。  \n",
    "ゴミを漁ってデータ生まれや性質をデータを利用する人が逐一調べていくのは非常に非効率なのと共に不可能なのでまとめた情報を保存する場所が必要になり。  \n",
    "その場所がメタデータストアであり、まとめた情報がメタデータというわけになります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータを保存するメタデータストアとは？\n",
    "メタデータストアとは、その名の通りメタデータを保存するデータストアのこと。  \n",
    "ビッグデータ基盤ではこのメタデータストアとデータストアは明確に分離されていることが通例。  \n",
    "例えば、メタデータストアはMysqlでデータストアはAmazon S3といった状況。  \n",
    "\n",
    "メタデータストアといっても特別なものではなく、古くから馴染みのあるMysqlや各種クラウドベンダーが提供しているメタデータを保存するためのマネージドサービスを利用すると良いと思います。  \n",
    "メタデータストアはプラットフォームによって異なる。  \n",
    "\n",
    "オンプレであればMysqlやMariaDBが利用される。  \n",
    "AWSやGCPといったクラウドであればGlue Data CaltalogやDataCatalogが利用される。  \n",
    "\n",
    "どれを選択しても本質部分は同じなので、  \n",
    "利用しているプラットフォームに合わせて選択すると良いと思います。  \n",
    "\n",
    "本コースはより本質の理解や個人のPCで実行可能にするためにMysqlをメタデータストアとして進めていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータ基盤におけるメタデータの提供形態\n",
    "メタデータに保存されたメタデータはどのようにユーザに提供されるのか？\n",
    "ビッグデータ基盤においてはメタデータの提供は２パターンで提供されることが多いです。\n",
    "\n",
    "- API(APIといっても超高速なレスポインスを求めるようなAPIではない)\n",
    "- GUI\n",
    "\n",
    "ユーザも、エンジニア、非エンジニアが混在しているためよりエンジニア向けのAPI(非エンジニアももちろん使える)と万人向けのGUIで提供するということが通例です。\n",
    "\n",
    "APIの場合はRESTのAPIでGET table とすればテーブルの定義がAPIとして返却されたり、共通的な指標を提供したり、GUIであればメタデータストアに保存されているデータをユーザに見やすいように整形するといった用途です。\n",
    "\n",
    "APIで気をつける点としては、データそのものを返却してしまおうとする行為です。    \n",
    "ビッグデータ基盤で管理しているデータは数TBを超えるデータを保持しています。  \n",
    "そのため、そのようなデータを返却しようとすると、そもそも返却できないことが大半でできたとしても、システムやネットワークにかなりの負荷をかけることになります。  \n",
    "\n",
    "今回は表示や取得のところまではコース内に含みませんが、Mysqlにデータを保存しておくことによってAPIとして返却することもGUIとして提供することも可能になります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3種類存在するメタデータ\n",
    "前述したように、メタデータには3種類のメタデータが存在しています。\n",
    "\n",
    "「ビジネスメタデータ」「テクニカルメタデータ」「オペレーショナルメタデータ」の３種類です。\n",
    "\n",
    "![図1.2 pyenvで設定したPythonのバージョンになっているかチェック](images/MetaData.png)\n",
    "  \n",
    "細かく分けると本コースでは紹介しきれないほどたくさんのメタデータの種類があります。  \n",
    "本コースでは幹となる３つのメタデータと、基本となるその中に存在するいくつかのカテゴリを紹介しています。  \n",
    "\n",
    "ビジネスメタデータ\n",
    "- テーブル定義\n",
    "\n",
    "テクニカルメタデータ\n",
    "- データプロファイリング\n",
    "\n",
    "オペレーショナルメタデータ\n",
    "- 5w1h\n",
    "- アクセスログ\n",
    "\n",
    "どれかひとつ欠けるだけでも途端に組織の生産性が落ちます。  \n",
    "データの利用が注目されている中一部の「強い人」に頼った構築やアウトプットの出し方では将来に所属しているデータ活用そのものが継続できなくなってしまう場合もあるのです。  \n",
    "「強くない普通の人」でも自分でデータを見つけて分析を行いアウトプットを出すビッグデータ基盤に求められるのはそんなシステム構成であり、それらを支えるのがメタデータです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次セクションへの導入\n",
    "次のセクションよりメタデータを解説及び、ところどころ算出していきます。\n",
    "そのための準備を行います。\n",
    "\n",
    "ここから、注意！！！！  \n",
    "Mysqlのデータベース(とテーブル)とSparkのデータベース(とテーブル)2つが出てきます。  \n",
    "それぞれ区別して記載してありますが、この2つは別物です。  \n",
    "Sparkのデータベース(とテーブル)のテーブル定義などはMysqlに格納されていますが、実データはロケーション\n",
    "「/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/parquet/」に格納され分離されています。\n",
    "\n",
    "## データの読み込みとテーブルの作成をします※\n",
    "1. jinko.csvを読み込んで表示する\n",
    "2. jinko.csvの内容を保存するSparkテーブルを作成する\n",
    "3. jinko.csvのデータを2で作成したSparkテーブルにInsertする\n",
    "\n",
    "※以下の講座で行った最初の準備の簡略版です。  \n",
    "「データサイエンスのための前処理入門PythonとSparkで学ぶビッグデータエンジニアリング(PySpark) 速習講座」  \n",
    "https://www.udemy.com/course/python-spark-pyspark/?referralCode=E67BF8B61F65866794EB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# spark.xxxxxと記載することで処理を分散させることが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"sokei\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\") \\\n",
    "    .csv(\"/Users/saitouyuuki/Desktop/src/pyspark_datamanagement_metadata/dataset/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)\n",
    "\n",
    "data_t=df.filter(\"code!='都道府県コード'\").filter(\"gengo=='平成'\")\n",
    "\n",
    "data_t.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義はMysqlに格納されています（次のセクションで確認を行います）\n",
    "spark.sql(\"\"\" \n",
    "create database if not exists data_management_crush_course;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルの作成をします。\n",
    "# 今回はCSVに一致した「jinko_table」Sparkテーブルを作成します\n",
    "# 定義はMysqlに格納されています（次のセクションで確認を行います）\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "CREATE TABLE IF NOT EXISTS data_management_crush_course.jinko_table (code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/saitouyuuki/Desktop/src/pyspark_datamanagement_metadata/dataset/data_management_crush_course.db/jinko_table';\n",
    "\"\"\")\n",
    "\n",
    "# locationとは実データが保存される領域です。\n",
    "# locationは一般に\n",
    "# DB.db/テーブル名\n",
    "# となることが多いです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# もう一つセクション５のためにテーブルを作成しておきます\n",
    "# 作業用のためCSV形式で作成を行います\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "CREATE TABLE IF NOT EXISTS data_management_crush_course.jinko_code (code String, kenmei String)\n",
    "row format delimited\n",
    "fields terminated by ','\n",
    "lines terminated by '\\n'\n",
    "LOCATION '/Users/saitouyuuki/Desktop/src/pyspark_datamanagement_metadata/dataset/data_management_crush_course.db/jinko_code';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成したSparkテーブルの一覧を確認することができます\n",
    "# 定義はMysqlに保存されているのでMysqlから内部的には引っ張り出してきています。\n",
    "spark.sql(\"show tables in   data_management_crush_course\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルの定義を確認することができます。\n",
    "spark.sql(\"show create table data_management_crush_course.jinko_table\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テーブルにデータを登録します\n",
    "# insertIntoというメソッドがありますが使いづらいのでテンポラリテーブルを作成してそこからselect insert します\n",
    "\n",
    "data_t.createOrReplaceTempView(\"jinko_table_tmp\")\n",
    "\n",
    "#「データサイエンスのための前処理入門PythonとSparkで学ぶビッグデータエンジニアリング(PySpark) 速習講座」  \n",
    "#https://www.udemy.com/course/python-spark-pyspark/?referralCode=E67BF8B61F65866794EB\n",
    "#より、パーティションとリパーティションをつけることを忘れずに\n",
    "spark.sql(\"\"\" \n",
    "Insert overwrite table data_management_crush_course.jinko_table partition(kenmei)\n",
    "Select \n",
    "/** REPARTITION(1) */\n",
    "code,gengo,wareki,seireki,chu,sokei,jinko_male,jinko_female,kenmei\n",
    "from jinko_table_tmp\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データがしっかり入っているか確認してみましょう\n",
    "spark.sql(\"\"\"\n",
    "select kenmei,count(*) from data_management_crush_course.jinko_table group by kenmei\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jinko_codeテーブルにデータを登録します\n",
    "# insertIntoというメソッドがありますが使いづらいのでテンポラリテーブルを作成してそこからselect insert します\n",
    "\n",
    "data_t.createOrReplaceTempView(\"jinko_table_tmp\")\n",
    "\n",
    "#「データサイエンスのための前処理入門PythonとSparkで学ぶビッグデータエンジニアリング(PySpark) 速習講座」  \n",
    "#https://www.udemy.com/course/python-spark-pyspark/?referralCode=E67BF8B61F65866794EB\n",
    "#より、パーティションとリパーティションをつけることを忘れずに\n",
    "spark.sql(\"\"\" \n",
    "Insert overwrite table data_management_crush_course.jinko_code\n",
    "Select \n",
    "/** REPARTITION(1) */ \n",
    "case \n",
    "when kenmei='神奈川県' then 'AA'\n",
    "else code\n",
    "END as code,\n",
    "kenmei\n",
    "from jinko_table_tmp\n",
    "where kenmei!='全国'\n",
    "  \"\"\")\n",
    "\n",
    "# のちの作業のために「全国」と「神奈川のコード」だけ変更します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jinko_codeテーブルの登録結果を見てみます。\n",
    "spark.sql(\"select * from data_management_crush_course.jinko_code\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後はSparkをクローズする\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本チャプターの目次\n",
    "1. データマネジメントとは何者のなのか？\n",
    "2. メタデータとは？\n",
    "3. ビッグデータ基盤にメタデータが必要な理由\n",
    "4. メタデータを保存するメタデータストアとは\n",
    "5. ビッグデータ基盤におけるメタデータの提供形態\n",
    "6. 3種類存在するメタデータ\n",
    "7. 次のセクションへの導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データマネジメントとは何者なのか？\n",
    "\n",
    "データマネジメントとは、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータとは？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータ基盤にメタデータが必要な理由"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータを保存するメタデータストアとは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータ基盤におけるメタデータの提供形態"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3種類存在するメタデータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次セクションへの導入\n",
    "次のセクションよりメタデータを解説及び、ところどころ算出していきます。\n",
    "そのための準備を行います。\n",
    "\n",
    "## データの読み込みとテーブルの作成をします※\n",
    "1. jinko.csvを読み込んで表示する\n",
    "2. jinko.csvの内容を保存するテーブルを作成する\n",
    "3. jinko.csvのデータを2で作成したテーブルにInsertする\n",
    "\n",
    "※以下の講座で行った最初の準備の簡略版です。\n",
    "「データサイエンスのための前処理入門PythonとSparkで学ぶビッグデータエンジニアリング(PySpark) 速習講座」\n",
    "https://www.udemy.com/course/python-spark-pyspark/?referralCode=E67BF8B61F65866794EB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/Cellar/apache-spark/3.2.0/libexec/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/19 10:44:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/11/19 10:44:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/11/19 10:44:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# spark.xxxxxと記載することで処理を分散させることが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "|          code|    kenmei|gengo|    wareki|   seireki| chu|       sokei|jinko_male|jinko_female|\n",
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "|都道府県コード|都道府県名| 元号|和暦（年）|西暦（年）|  注|人口（総数）|人口（男）|  人口（女）|\n",
      "|            00|      全国| 大正|         9|      1920|null|    55963053|  28044185|    27918868|\n",
      "|            01|    北海道| 大正|         9|      1920|null|     2359183|   1244322|     1114861|\n",
      "|            02|    青森県| 大正|         9|      1920|null|      756454|    381293|      375161|\n",
      "|            03|    岩手県| 大正|         9|      1920|null|      845540|    421069|      424471|\n",
      "|            04|    宮城県| 大正|         9|      1920|null|      961768|    485309|      476459|\n",
      "|            05|    秋田県| 大正|         9|      1920|null|      898537|    453682|      444855|\n",
      "|            06|    山形県| 大正|         9|      1920|null|      968925|    478328|      490597|\n",
      "|            07|    福島県| 大正|         9|      1920|null|     1362750|    673525|      689225|\n",
      "|            08|    茨城県| 大正|         9|      1920|null|     1350400|    662128|      688272|\n",
      "|            09|    栃木県| 大正|         9|      1920|null|     1046479|    514255|      532224|\n",
      "|            10|    群馬県| 大正|         9|      1920|null|     1052610|    514106|      538504|\n",
      "|            11|    埼玉県| 大正|         9|      1920|null|     1319533|    641161|      678372|\n",
      "|            12|    千葉県| 大正|         9|      1920|null|     1336155|    656968|      679187|\n",
      "|            13|    東京都| 大正|         9|      1920|null|     3699428|   1952989|     1746439|\n",
      "|            14|  神奈川県| 大正|         9|      1920|null|     1323390|    689751|      633639|\n",
      "|            15|    新潟県| 大正|         9|      1920|null|     1776474|    871532|      904942|\n",
      "|            16|    富山県| 大正|         9|      1920|null|      724276|    354775|      369501|\n",
      "|            17|    石川県| 大正|         9|      1920|null|      747360|    364375|      382985|\n",
      "|            18|    福井県| 大正|         9|      1920|null|      599155|    293181|      305974|\n",
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"sokei\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\") \\\n",
    "    .csv(\"/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/19 10:45:11 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/19 10:45:11 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "21/11/19 10:45:12 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "21/11/19 10:45:12 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "create database if not exists data_management_cruch_course;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/19 10:45:18 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "21/11/19 10:45:18 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "21/11/19 10:45:18 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "21/11/19 10:45:18 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テーブルの作成をします。\n",
    "# 今回はCSVに一致した「jinko_table」テーブルを作成します\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS data_management_cruch_course.jinko_table (code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/parquet/';\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------+-----------+\n",
      "|namespace                   |tableName  |isTemporary|\n",
      "+----------------------------+-----------+-----------+\n",
      "|data_management_cruch_course|jinko_table|false      |\n",
      "+----------------------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 作成したテーブルの一覧を確認することができます\n",
    "spark.sql(\"show tables in   data_management_cruch_course\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE `data_management_cruch_course`.`jinko_table` (\\n  `code` STRING,\\n  `gengo` STRING,\\n  `wareki` STRING,\\n  `seireki` STRING,\\n  `chu` STRING,\\n  `sokei` STRING,\\n  `jinko_male` STRING,\\n  `jinko_femail` STRING,\\n  `kenmei` STRING)\\nUSING parquet\\nPARTITIONED BY (kenmei)\\nLOCATION 'file:/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/parquet'\\nTBLPROPERTIES (\\n  'transient_lastDdlTime' = '1637223738')\\n|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# テーブルの定義を確認することができます。\n",
    "spark.sql(\"show create table data_management_cruch_course.jinko_table\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

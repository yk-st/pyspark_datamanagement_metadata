{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本チャプターの目次\n",
    "1. ビジネスメタデータの重要な役割\n",
    "2. ビジネスメタデータの活用方法\n",
    "3. 代表的なビジネスメタデータは？\n",
    "4. 実際にビジネスメタデータを覗いてみよう\n",
    "5. メタデータ保存用のテーブルを作成してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビジネスメタデータの重要な役割\n",
    "\n",
    "データマネジメントとは、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビジネスメタデータの活用方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代表的なビジネスメタデータは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際にビジネスメタデータを覗いてみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータ保存用のテーブルを作成してみよう\n",
    "\n",
    "最終的にメタデータをMysqlに保存する目的で本コースでは以下の順番で対応していきます。\n",
    "\n",
    "1. テクニカルメタデータをSparkのテーブルに一度保存\n",
    "2. オペレーショナルメタデータをSparkのテーブルに一度保存\n",
    "3. 1,2とビジネスメタデータと合わせてMysqlのテーブルに保存\n",
    "\n",
    "そのための準備を行います。\n",
    "\n",
    "メタデータ保存用のメタデータの保存用のテーブルとMysqlのテーブルをここで作成します。\n",
    "\n",
    "Sparkテーブル(metadata_tmpデータベース)\n",
    "- database_name -> データベース名\n",
    "- table_name -> テーブル名\n",
    "- table_definition ->テーブル定義\n",
    "- sammary ->　テーブル説明\n",
    "- row_num ->　レコード件数\n",
    "- selectivity ->　セレクティビティ\n",
    "- consistency_flag ->　コンシステンシー\n",
    "- frequency_access ->　アクセス数\n",
    "\n",
    "Mysqlテーブル(metadataデータベース)\n",
    "- database_name -> Sparkデータベース名\n",
    "- table_name -> Sparkテーブル名\n",
    "- table_definition -> Sparkテーブル定義\n",
    "- sammary ->　Sparkテーブル説明\n",
    "- row_num ->　レコード件数\n",
    "- selectivity ->　セレクティビティ\n",
    "- consistency_flag ->　コンシステンシー\n",
    "- frequency_access ->　アクセス数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータ保存用のMysqlテーブル\n",
    "\n",
    "```\n",
    "CREATE DATABASE if not exists metadata;\n",
    "USE metadata;\n",
    "CREATE TABLE if not exists metadatas(\n",
    "    database_name VARCHAR(255) , \n",
    "    table_name VARCHAR(255) , \n",
    "    table_definition VARCHAR(255) , \n",
    "    sammary VARCHAR(255) , \n",
    "    row_num VARCHAR(255) , \n",
    "    selectivity VARCHAR(255) , \n",
    "    consistency_flag VARCHAR(255) , \n",
    "    frequency_access VARCHAR(255) ,\n",
    "    PRIMARY KEY (database_name,table_name));\n",
    "```\n",
    "\n",
    "コーンソールに移動してMysqlコマンドを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メタデータ保存用のSparkテーブル\n",
    "spark.sql(\"create database if not exists metadata_tmp\")\n",
    "spark.sql(\"\"\" \n",
    "CREATE TABLE IF NOT EXISTS metadata_tmp.sample_metadata (database_name String, table_name String,table_definition String,sammary String,row_num String,selectivity String,consistency_flag String,frequency_access String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/parquet/';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　テーブルができたか確認します\n",
    "spark.sql(\"show tables in metadata_tmp\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# spark.xxxxxと記載することで処理を分散させることが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

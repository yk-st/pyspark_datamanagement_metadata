{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本チャプターの目次\n",
    "1. ビジネスメタデータの重要な役割\n",
    "2. ビジネスメタデータの活用方法\n",
    "3. 代表的なビジネスメタデータは？\n",
    "4. 実際にビジネスメタデータを覗いてみよう\n",
    "5. メタデータ保存用のテーブルを作成してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビジネスメタデータとは？\n",
    "ビジネスメタデータの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代表的なビジネスメタデータは？\n",
    "代表的なビジネスメタデータはテーブル定義です。  \n",
    "テーブル定義といっても、テーブル名やデータベース名、カラム名と言ったオーソドックスなところから、  \n",
    "各カラムの説明といったビジネス要件などの説明を含みます。  \n",
    "\n",
    "ビジネス要件（ドメイン知識などともよばれる）は社内の慣例や業界の慣例、システムの特別な仕様など、見ただけではわからないような仕様のことを指しています。  \n",
    "特にビジネス要件は機械的には抽出することができず人の手に頼っているのが実情です。  \n",
    "\n",
    "そのため、ビジネスメタデータは単純にテーブル定義だけを示せば良いわけではなく単一システムだけにとどまらない、テーブル同士やシステム同士の紐付きを言語化し登録する必要もあるというわけになります（リネージュと呼んだりします）。  \n",
    "\n",
    "そのため現場のエンジニアだけで作業が完了する問いことはまずなく、データ提供〜データ分析に参画する全てのユーザがこのビジネスメタデータに参画する必要があります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビジネスメタデータの重要性や活用方法\n",
    "ビジネスメタデータは、従来から存在しているテーブル定義に対してより付加価値をつけていくものと言えるわけなのですが、\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際にビジネスメタデータを覗いてみよう\n",
    "ここからは実際にビジネスメタデータ(テーブル定義)を覗いてみましょう。\n",
    "\n",
    "復習になりますが、メタデータはメタデータストア(今回だとMysqlに保存していきます。)\n",
    "特にテーブル定義はいづれの環境（オンプレでも、クラウドでも）自動的に保存されることが多いので、最初に確認するにはちょうどいい題材です。\n",
    "\n",
    "セクション２で行った環境構築設定などありますので、接続情報など忘れてしまった方は、以下のノートブックを見直してみてください。\n",
    "```\n",
    "必要ツールの設定を行います。\n",
    "以下を参照いただきご自身の環境作成をおこないます\n",
    "https://github.com/yk-st/pyspark_settings/blob/main/setting.ipynb\n",
    "```\n",
    "\n",
    "## Mysqlに接続してみよう\n",
    "1. mysql -uroot -proot でデータベースに接続\n",
    "2. use metastore　でmetastoreデータベースに接続\n",
    "3. show tablesでテーブル一覧を表示\n",
    "4. select * from TBL でメタデータストア内に保存されているテーブルを取得\n",
    "\n",
    "jinko_table\n",
    "sample_metadataテーブル共にセクション３で作成したSparkテーブルです。\n",
    "\n",
    "5. select * from DBSでデータベース情報を確認してみましょう\n",
    "\n",
    "DB_IDが4で取得した値と一致していることが確認できるはずです\n",
    "\n",
    "6. 肝心の定義はどこにあるでしょうか？\n",
    "SD_IDをもとにテーブル定義を引っ張ってきています。\n",
    "少し説明しにくいのでSQLを見て下さい。\n",
    "以下のSQLを実行するとsmaple_metadataテーブルのカラム名をメタデータストアから取得するSQLです。\n",
    "\n",
    "```\n",
    "select co.COLUMN_NAME from TBLS t \n",
    "   inner join SDS as s on t.SD_ID=s.SD_ID \n",
    "   inner join CDS c on s.CD_ID=c.CD_ID \n",
    "   inner join COLUMNS_V2 co on c.CD_ID = co.CD_ID \n",
    "where TBL_NAME='sample_metadata';\n",
    "```\n",
    "\n",
    "Mysqlで構築する場合特有の話になってしまうのであまり踏み込みませんが、全てのSPARKテーブルに関する情報は全て上記のようにJOINをすることによって取得することが可能です。  \n",
    "じつ業務で活用する場合は、上記に加えてカラムの説明を保存するテーブルを作成(今回の場合はsample_metadataテーブル)して結合することで統合的にGUIやAPIで表示返却する  \n",
    "ということが行われます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータ保存用のテーブルを作成してみよう\n",
    "\n",
    "最終的にメタデータをMysqlに保存する目的で本コースでは以下の順番で対応していきます。\n",
    "\n",
    "1. テクニカルメタデータをSparkのテーブルに一度保存\n",
    "2. オペレーショナルメタデータをSparkのテーブルに一度保存\n",
    "3. 1,2とビジネスメタデータと合わせてMysqlのテーブルに保存\n",
    "\n",
    "そのための準備を行います。\n",
    "\n",
    "メタデータ保存用のメタデータの保存用のテーブルとMysqlのテーブルをここで作成します。\n",
    "\n",
    "Sparkテーブル(metadata_tmpデータベース)\n",
    "- database_name -> データベース名\n",
    "- table_name -> テーブル名\n",
    "- table_definition ->テーブル定義\n",
    "- sammary ->　テーブル説明\n",
    "- row_num ->　レコード件数\n",
    "- selectivity ->　セレクティビティ\n",
    "- consistency_flag ->　コンシステンシー\n",
    "- frequency_access ->　アクセス数\n",
    "\n",
    "Mysqlテーブル(metadataデータベース)\n",
    "- database_name -> Sparkデータベース名\n",
    "- table_name -> Sparkテーブル名\n",
    "- table_definition -> Sparkテーブル定義\n",
    "- sammary ->　Sparkテーブル説明\n",
    "- row_num ->　レコード件数\n",
    "- selectivity ->　セレクティビティ\n",
    "- consistency_flag ->　コンシステンシー\n",
    "- frequency_access ->　アクセス数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータ保存用のMysqlテーブル\n",
    "\n",
    "```\n",
    "CREATE DATABASE if not exists metadata;\n",
    "USE metadata;\n",
    "CREATE TABLE if not exists metadatas(\n",
    "    database_name VARCHAR(255) , \n",
    "    table_name VARCHAR(255) , \n",
    "    table_definition VARCHAR(255) , \n",
    "    sammary VARCHAR(255) , \n",
    "    row_num VARCHAR(255) , \n",
    "    selectivity VARCHAR(255) , \n",
    "    consistency_flag VARCHAR(255) , \n",
    "    frequency_access VARCHAR(255) ,\n",
    "    PRIMARY KEY (database_name,table_name));\n",
    "```\n",
    "\n",
    "コーンソールに移動してMysqlコマンドを実行します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータ保存用のSparkテーブル\n",
    "ここからはメタデータ保存用のSparkテーブルを作成していきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# spark.xxxxxと記載することで処理を分散させることが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メタデータ保存用のSparkテーブル\n",
    "spark.sql(\"drop database metadata_tmp cascade\")\n",
    "spark.sql(\"create database if not exists metadata_tmp\")\n",
    "spark.sql(\"\"\" \n",
    "CREATE TABLE IF NOT EXISTS metadata_tmp.sample_metadata (database_name String, table_name String,table_definition String,sammary String,record_num String,selectivity String,consistency_flag boolean,frequency_access String)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/metadata_tmp.db/sample_metadata';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　テーブルができたか確認します\n",
    "spark.sql(\"show tables in metadata_tmp\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビジネスメタデータをSparkテーブル(sample_metadata)に一度保存してみましょう\n",
    "ここからはビジネスメタデータ(data_management_cruch_course.jinko_table)を一度Sparkテーブルに保存していきます。\n",
    "\n",
    "ここで確認ですがビジネスメタデータで基本となるのは\n",
    "1. テーブル定義\n",
    "2. ビジネス知識（ドメイン知識）\n",
    "\n",
    "の２つです。  \n",
    "今回はこれらを格納するテーブルだと、「table_definition」と「Summary」です。  \n",
    "最終的にはMysqlに入れますが、講座の都合上sample_metadataテーブルに都度格納をしながら行います。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# 横着をして Spark経由でテーブル定義を取得します\n",
    "table_def=spark.sql(\"\"\" show create table data_management_crush_course.jinko_table \"\"\").collect()[0].asDict()['createtab_stmt']\n",
    "\n",
    "# カラム名、型、Null OKか否かで設定していきます\n",
    "struct = StructType([\n",
    "    StructField(\"database_name\", StringType(), True),\n",
    "    StructField(\"table_name\", StringType(), True),\n",
    "    StructField(\"table_definition\", StringType(), True),\n",
    "    StructField(\"sammary\", StringType(), True),\n",
    "    StructField(\"record_num\", StringType(), True),\n",
    "    StructField(\"selectivity\", StringType(), True),\n",
    "    StructField(\"consistency_flag\", BooleanType(), True),\n",
    "    StructField(\"frequency_access\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame([(None,None,None,None,None,None,None,None)], struct)\n",
    "#今回は一個づつ値を設定していくので少しトリッキーにデータフレームの値を更新していきマス。\n",
    "\n",
    "# メタデータ取得対象のデータを更新する\n",
    "df2 = df.withColumn(\"database_name\", when(df.database_name.isNull() ,\"data_management_crush_course\").otherwise(df.database_name))\n",
    "df2 = df2.withColumn(\"table_name\", when(df.table_name.isNull() ,\"jinko_table\").otherwise(df.table_name))\n",
    "df2 = df2.withColumn(\"table_definition\", when(df.table_definition.isNull() ,table_def).otherwise(df.table_definition))\n",
    "df2 = df2.withColumn(\"sammary\", when(df.sammary.isNull() ,\"一旦テーブルの説明は空にしておきます。\").otherwise(df.sammary))\n",
    "\n",
    "df2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得したデータをmetadata_tmp.sample_metadataに格納していきます\n",
    "df2.createOrReplaceTempView(\"sample\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "Insert overwrite  table metadata_tmp.sample_metadata \n",
    "select  * from sample\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------------------------+----------+-----------+----------------+----------------+\n",
      "|       database_name| table_name|    table_definition|                               sammary|record_num|selectivity|consistency_flag|frequency_access|\n",
      "+--------------------+-----------+--------------------+--------------------------------------+----------+-----------+----------------+----------------+\n",
      "|data_management_c...|jinko_table|CREATE TABLE `dat...|一旦テーブルの説明は空にしておきます。|       300|       6.00|           false|            null|\n",
      "+--------------------+-----------+--------------------+--------------------------------------+----------+-----------+----------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mon Nov 22 22:12:25 JST 2021 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "Mon Nov 22 22:12:25 JST 2021 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "Mon Nov 22 22:12:25 JST 2021 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "Mon Nov 22 22:12:25 JST 2021 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 59922)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/saitouyuuki/.pyenv/versions/3.9.1/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/saitouyuuki/.pyenv/versions/3.9.1/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/saitouyuuki/.pyenv/versions/3.9.1/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/saitouyuuki/.pyenv/versions/3.9.1/lib/python3.9/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 結果の確認をしてみます\n",
    "\n",
    "spark.sql(\"select * from metadata_tmp.sample_metadata \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopは忘れずに\n",
    "# 忘れてしまうと、いつの間にか接続が溜まっていってしまいます\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本チャプターの目次\n",
    "1. テクニカルメタデータについて\n",
    "2. テクニカルメタデータの一種データプロファイリング\n",
    "3. PySparkでデータプロファイリングをしてみよう1 \n",
    "4. PySparkでデータプロファイリングをしてみようその２\n",
    "5. PySparkでデータプロファイリングをしてみようその3\n",
    "6. データプロファイリングの結果をテーブルに格納してみよう\n",
    "7. データ品質"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テクニカルメタデータについて\n",
    "テクニカルメタデータとは、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テクニカルメタデータの一種データプロファイリング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# spark.xxxxxと記載することで処理を分散させることが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メタデータ取得対象のデータを取得する\n",
    "# 前のチャプターで「データベース名」「テーブル名」「テーブル定義」「サマリー」を登録したテーブルを取得します\n",
    "\n",
    "df=spark.sql(\"select * from  metadata_tmp.sample_metadata\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkでデータプロファイリングをしてみようその１\n",
    "\n",
    "データプロファイリングその１では、以下の3つを算出してみます。  \n",
    "3つのうち件数をrow_num(件数)をメタデータストアに保存していきます。\n",
    "\n",
    "- 件数\n",
    "- 平均\n",
    "- 合計値\n",
    "\n",
    "## 件数/平均/合計の使い道\n",
    "例えば件数はよく、異常な状態を見付けることに使えます。  \n",
    "例えば、前日までに１０件だったものが急に1000件になったらアラートを鳴らすということにも使えたりします。  \n",
    "データ基盤は時には何万というテーブルが存在しているため、一つ一つ目で見るのは不可能に近いのが現実です。  \n",
    "そこで件数のような簡単なチェックを入れておくことでも、テーブル数が増え続けても問題ないデータ基盤を作ることが可能です。  \n",
    "\n",
    "一方で、取得しやすいがゆえに何も考えずに取得することは無意味になりますので、しっかりと自身の環境に適用できるかどうかを判断することは大切になります。  \n",
    "例えば、年齢の合計値はあまり意味を成さないかもしれませんし時と場合によっては、合計を知ることで何か得るものがあるかもしれません。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 件数の取得\n",
    "spark.sql(\"REFRESH TABLE data_management_cruch_course.jinko_table \")\n",
    "row_num=spark.sql(\"select count(*) as row_num from data_management_cruch_course.jinko_table \")\n",
    "row_num.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均と合計値\n",
    "# 今回は平均として、女性と男性の合計値の平均をとっていこうと思います。\n",
    "spark.sql(\"REFRESH TABLE data_management_cruch_course.jinko_table \")\n",
    "sum=spark.sql(\"select cast(sum(jinko_male)+sum(jinko_femail) as decimal(19, 0)) as sum from data_management_cruch_course.jinko_table \")\n",
    "# decimalにcastしないと数値が指数表記になってみづらくなるのでdecimalにしています。\n",
    "sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均と合計値\n",
    "# 今回は平均として、女性と男性の合計値の平均をとっていこうと思います。\n",
    "spark.sql(\"REFRESH TABLE data_management_cruch_course.jinko_table \")\n",
    "avg=spark.sql(\"select avg(sum) as avg from (select cast(sum(jinko_male)+sum(jinko_femail) as decimal(19, 0)) as sum from data_management_cruch_course.jinko_table) \")\n",
    "avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkでデータプロファイリングをしてみようその２\n",
    "データプロファリングその２では、以下の２つを算出していきます。\n",
    "\n",
    "- カーディナリティ\n",
    "- セレクティビティ\n",
    "\n",
    "# カーディナリティの使い道\n",
    "カーディナリティとはどれだけ値がバラけているかを示す指標です。  \n",
    "ビッグデータ基盤におけるカーディナリティはスモールファイル問題やデータスキューネスを発見するための手段としてビッグデータ基盤では使われます。  \n",
    "データスキューネスとは、データに偏りがある状態を指します。  \n",
    "\n",
    "ビッグデータ基盤における偏りやスモールファイルは非常に問題になることがあります。  \n",
    "例えば、クエリのエラーを多発させたり、データの転送を遅くさせたりするする原因にもなります。  \n",
    "\n",
    "スモールファイル問題について詳しく知りたい方は是非以下のコースも受講してみてください。  \n",
    "https://www.udemy.com/course/python-spark-pyspark/?referralCode=E67BF8B61F65866794EB   \n",
    "\n",
    "# セレクティビティの使い道\n",
    "セレクティビティとは、検索した時に結果が何件返却されるか？というものです  \n",
    "\n",
    "セレクティビティは、クエリのしやすさに直結してきます。  \n",
    "データを探索する際に、重複したデータが出てきたらどうでしょうか(そもそも重複に気付けない可能性がありますが)？  \n",
    "重複を除く処理をSQLに記載しなければなりませんし、さらには  \n",
    "最終的な分析結果を間違えることもあります。たとえば、結果が2倍になってしまうかも。  \n",
    "数億というレコードの中で重複を分析の結果だけ見て判断するというのはできないものです。  \n",
    "そうなれば、再度モデルの作成仕直しや開発のやり直しをしなければならなくなります。  \n",
    "\n",
    "ビッグデータ基盤にはPK（プライマリーキー）のような仕組みを持っているものが少なく、データの重複が多々発生してしまいます。  \n",
    "\n",
    "そこでセレクティビティのチェックを行うことによって、重複しているデータを見つけ出し対処を行うことができるのです※。  \n",
    "\n",
    "※どのような対処を取るべきなのか？についてはデータ品質管理という方法があります。準備が出来次第別のコースで作成予定です。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "# カーディナリティとセレクティビティはほぼ同時に出力することができます\n",
    "# 今回はkenmeiについてかーディなりティを測ってみます\n",
    "card=spark.sql(\"select * from data_management_cruch_course.jinko_table\")\n",
    "\n",
    "cols_cardinality = card.select([(approx_count_distinct(c)/card.count()).cast(DecimalType(38,2)).alias(\"cardinaryty_\"+c) for c in card.columns])\n",
    "\n",
    "cols_cardinality.show() \n",
    "\n",
    "# approx(正確さを捨て速度と引き換えに概算値を出してくれるもの)\n",
    "# 数値が大きければ大きいほど値がバラけています(カーディナリティが高い)。\n",
    "# 数値が小さければ小さいほど値が同じで値はバラけていません(カーディナリティが低い)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+------------------+-------------------+---------------+-----------------+----------------------+------------------------+------------------+\n",
      "|cardinaryty_code|cardinaryty_gengo|cardinaryty_wareki|cardinaryty_seireki|cardinaryty_chu|cardinaryty_sokei|cardinaryty_jinko_male|cardinaryty_jinko_femail|cardinaryty_kenmei|\n",
      "+----------------+-----------------+------------------+-------------------+---------------+-----------------+----------------------+------------------------+------------------+\n",
      "|            1.00|             1.00|              6.00|               6.00|           0.00|             6.00|                  6.00|                    6.00|              1.00|\n",
      "+----------------+-----------------+------------------+-------------------+---------------+-----------------+----------------------+------------------------+------------------+\n",
      "\n",
      "6.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#　セレクティビティはキーを指定した場合（今回の場合はkenmei）レコードの返却はどれくらいであるかを想定するものです\n",
    "card=spark.sql(\"select * from data_management_cruch_course.jinko_table where kenmei='東京都'\")\n",
    "\n",
    "cols_selctivity = card.select([(approx_count_distinct(c)).cast(DecimalType(38,2)).alias(\"cardinaryty_\"+c) for c in card.columns])\n",
    "\n",
    "cols_selctivity.show()\n",
    "# 表示された結果がおおよその返却結果（件数）になります。\n",
    "# 今回の結果だと6件ほど返されるテーブルになりそうですね\n",
    "\n",
    "\n",
    "# 今回はパーティションを指定した時を前提にどれくらいの件数が返却されるのかを\n",
    "cols_selctivity.createOrReplaceTempView(\"tmp\")\n",
    "list_selectivity=spark.sql(\"\"\"\n",
    "select \n",
    "CONCAT(cardinaryty_code, ',', cardinaryty_gengo, ',',cardinaryty_wareki ,',', cardinaryty_seireki,',', cardinaryty_chu,',',cardinaryty_sokei, ',', cardinaryty_jinko_male,',',cardinaryty_jinko_femail,',',cardinaryty_kenmei)\n",
    " as concat from tmp\n",
    "\"\"\"\n",
    ").collect()[0].asDict()['concat'].split(',')\n",
    "\n",
    "#調べた結果として最大値を機械的に取得して格納しておきます（サイトにメタデータテーブルに格納します。）\n",
    "cols_selctivity_max=max(list_selectivity)\n",
    "print(cols_selctivity_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果から考察してみる\n",
    "この結果を用いてどう考えるか？というところです。  \n",
    "現在は「kenmei」をパーティションにしています。\n",
    "\n",
    "パーティションは多すぎてもいけませんし、少なすぎてもいけません。\n",
    "この場合「cardinaryty_jinko_male」などでパーティションを切ってしまうとパーティションが多くなってしまいますし、一方で「cardinaryty_gengo」でパーティションを切るとデータが一つのパーティションに入り込んでしまいます。\n",
    "\n",
    "そのように考えると「cardinaryty_kenmei」は多くもなく少なくもないちょうどいいカーディナリティであることがわかります。\n",
    "\n",
    "セレクティビティに関してはパーティション唯一に絞ることはできないものの、県ごとに西暦が6年分記録されているようですね。  \n",
    "特にプライマリーキー(ビッグデータ基盤ではUUIDであることが多い)などをセレクティビティチェックした時にレコードが複数帰ってくる場合はどこかの経路で何か間違えているということになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkでデータプロファイリングをしてみようその3\n",
    "データプロファイリング３では、コンシステンシーについて考えてみたいと思います。\n",
    "コンシステンシーとは一貫性があるかないかという指標です。\n",
    "\n",
    "例えばkenmei_codeと今回のjinko_tableを使ってみていきましょう(作成はチャプター３で行っています)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データプロファイリングの結果をMysqlテーブルに格納してみよう\n",
    "\n",
    "頑張ればSparkを使って書き込むこともできますが、きれいなPGにはならないのでSparkの機能は使わずにPythonから実行するようにします\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果を格納しよう\n",
    "決して綺麗な入れ方ではありませんがここでも結果を格納してみましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ品質"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

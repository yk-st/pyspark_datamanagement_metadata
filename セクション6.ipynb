{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本チャプターの目次\n",
    "1. オペレーショナルメタデータは5w1h\n",
    "2. オペレーショナルメタデータの使い道は？\n",
    "3. アクセスログを使って5w1hを管理してみよう\n",
    "4. オペレーショナルメタデータの結果をメタデータストアに格納しよう\n",
    "5. メタデータの形は最終的にどうなったのか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# オペレーショナルメタデータは5w1h\n",
    "オペレーショナルメタデータとは、データに関する5w1hを取得することです。  \n",
    "どれくらいアクセスがあるのか？誰が主に使っているのか？\n",
    "そんな情報を取得し、メタデータストアに格納します。\n",
    "\n",
    "## 最もオーソドックスな方法はアクセスログを利用すること\n",
    "そして、データに関する5w1hを取得するために利用される最もオーソドックスな形がアクセスログを用いた5w1hの取得になります。  \n",
    "セクション２にて、環境構築をした際にlog4j.propertiesの設定を行なっているのですが、そこで今回利用する/var/log/spark/spark_operation.logを出力する設定を入れています。  \n",
    "\n",
    "今回は、リポジトリ内に「loggiles/spark_operation.log.1」というフォルダを使って解析を進めますが、「/var/log/spark/」配下を直接みて実行いただいても問題ありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# オペレーショナルメタデータの使い道は？\n",
    "オペレーショナルメタデータの役割の一つとして「データの沼化」を防ぐ効果があります。  \n",
    "データの沼化とは、使われないデータでデータレイク（やデータウェアハウス）が埋め尽くされてしまうような状況を指します。  \n",
    "\n",
    "そこでしっかりと、データに関するアクセスの状況を把握することによって、例えばアクセス数0が続くのであればデータを削除したりすることが検討できますし、  \n",
    "元々誰がアクセスしていたかを記録にとっていれば、そのユーザに対して聞き込みをすすることもできることになります  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アクセスログを使って5w1hを管理してみよう\n",
    "それでは、早速ここからアクセスログを使ってオペレーショナルメタデータを取得していきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# spark.xxxxxと記載することで処理を分散させることが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回ターゲットとするのは、以下のようなログの内容です。\n",
    "# @audit@の部分で監査ログであることがわかるのと、ugiでアクセスユーザ、ipで場所、cmdでどこのデータにアクセスしたのか、時間でいつアクセスしたのか？がわかります\n",
    "# howにあたる部分は今回はSpark限定で、whyにあたる部分は他の情報を使うことによって明確になります(主には聞くことが多い)\n",
    "# 21/11/23 11:27:53 @INFO @audit@ ugi=saitouyuuki\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
    "from pyspark.sql.functions import split, col\n",
    "df=spark.read.text(\"/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/logfiles/spark_operation.log.1\")\n",
    "\n",
    "df_log=df.withColumn(\"tmp\", split(df.value,\"@\")).select(\n",
    "    col(\"tmp\").getItem(0).alias('time'),\n",
    "    col(\"tmp\").getItem(1).alias('log_level'),\n",
    "    col(\"tmp\").getItem(2).alias('category'),\n",
    "    col(\"tmp\").getItem(3).alias('log'))\n",
    "\n",
    "df_log.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# いくつか不要な情報を除いていきましょう\n",
    "df_log.filter(\"category = 'audit'\").show(truncate=False)\n",
    "df_log_f=df_log.filter(\"category = 'audit'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ugi=saitouyuuki\\tip=unknown-ip-addr\\tcmd=get_database: data_management_crush_course\\tの部分を\n",
    "# \\tでさらに分割すればさらにこまかい分析が出来そうですね。\n",
    "\n",
    "df_log_split=df_log_f.withColumn(\"tmp\", split(df_log_f.log,\"\\\\t\")).select(\n",
    "    df_log_f.time,\n",
    "    df_log_f.log_level,\n",
    "    df_log_f.category,\n",
    "    col(\"tmp\").getItem(0).alias('ugi'),\n",
    "    col(\"tmp\").getItem(1).alias('ip'),\n",
    "    col(\"tmp\").getItem(2).alias('cmd'))\n",
    "\n",
    "df_log_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "# 21/11/23 11:27:55 |    INFO |   audit| ugi=saitouyuuki|ip=unknown-ip-addr|cmd=get_table : d\n",
    "# あと一歩最後の:をsuplitしたいですね。\n",
    "\n",
    "df_log_split_check=df_log_split.withColumn(\"tmp\", split(df_log_split.cmd,\" : \")).select(\n",
    "    df_log_split.time,\n",
    "    df_log_split.log_level,\n",
    "    df_log_split.category,\n",
    "    df_log_split.ugi,\n",
    "    df_log_split.ip,\n",
    "    trim(col(\"tmp\").getItem(0)).alias('cmd_sub'),\n",
    "    col(\"tmp\").getItem(1).alias('cmd_sub2')\n",
    "    )\n",
    "\n",
    "df_log_split_check.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回はこの中から、特定のテーブルへのアクセス数を数えてみようと思います\n",
    "# 特定のテーブルへのアクセスを抜き出すためにはcmd-get_tableが良さそうです。\n",
    "# また、時間的に重複しているようなものもあるのでそちらは除外しましょう\n",
    "\n",
    "df_log_split_check=df_log_split_check.filter(\"cmd_sub='cmd=get_table'\").distinct()\n",
    "df_log_split_check.createOrReplaceTempView(\"original\")\n",
    "\n",
    "result=spark.sql(\"\"\"\n",
    "select '2021/11/12 10:00:00' as time,ugi , ip , cmd_sub2, cast(count(*) as String) as access\n",
    "from \n",
    "original\n",
    "group by ugi , ip , cmd_sub2\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# これで誰がどこから、どこに、何回アクセスしてるかがわかりました。\n",
    "# アクセスの理由についてはアクセスしてきた人（不正理由でなければ）聞くことができます\n",
    "# 今回は、jinko_tableのメタデーを取得していますので、「db=data_management_crush_course tbl=jinko_table」のアクセス数を取得します。\n",
    "\n",
    "result_string=''\n",
    "# あまり関係ないですが、集計した後のデータであれば以下のようにループを回すことが可能です\n",
    "for x in result.filter(\"cmd_sub2='db=data_management_crush_course tbl=jinko_table'\").collect():\n",
    "    result_string=','.join(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# オペレーショナルメタデータの結果をメタデータストアに格納しよう\n",
    "これで全ての作業が完了しました。\n",
    "\n",
    "前のレクチャーで取得したアクセスログの結果をSparkテーブルへ登録していきましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "metadata_df=spark.sql(\"select * from metadata_tmp.sample_metadata \")\n",
    "\n",
    "# メタデータ取得対象のデータを更新する\n",
    "metadata_df = metadata_df.withColumn(\"frequency_access\", when(metadata_df.frequency_access.isNull() ,result_string).otherwise(metadata_df.frequency_access))\n",
    "metadata_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得したデータをmetadata_tmp.sample_metadataに格納していきます\n",
    "#読み込んだテーブルに対して直接データを入れることができないので、一度ファイルを吐き出します\n",
    "\n",
    "spark.sql(\"REFRESH TABLE metadata_tmp.sample_metadata\")\n",
    "metadata_df.write.mode('overwrite').parquet(\"/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/tmp/\")\n",
    "insert_df=spark.read.parquet(\"/Users/saitouyuuki/Desktop/src/pyspark_dataprofiling_dataquality/dataset/tmp/\")\n",
    "\n",
    "#取得したデータをmetadata_tmp.sample_metadataに格納していきます\n",
    "insert_df.createOrReplaceTempView(\"sample\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "Insert overwrite  table metadata_tmp.sample_metadata \n",
    "select  * from sample\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の確認をしてみます\n",
    "spark.sql(\"select * from metadata_tmp.sample_metadata \").show(truncate=False)\n",
    "\n",
    "# 無事入っているようですね\n",
    "# 次のセクションでは、Sparkテーブルに保存した結果をMysqlテーブルに格納してみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータの形は最終的にどうなったのか？\n",
    "まずは結果を確認する前に、Sparkテーブルの結果をMysqlに格納しましょう。　　\n",
    "Mysqlに格納する理由は他のアプリケーション（APIやGUI）からの連携性を高めるためでした。\n",
    "\n",
    "SparkテーブルとMysqlテーブルの大きな違いを上げるのであれば、実行速度の違いが挙げられます。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JDBC経由で結果をMysqlに入れることが可能です\n",
    "# JDBCの接続情報は\n",
    "\n",
    "metadata_df=spark.sql(\"select * from metadata_tmp.sample_metadata \")\n",
    "\n",
    "metadata_df.write.format('jdbc').options(\n",
    "      url='jdbc:mysql://localhost/metadata?enabledTLSProtocols=TLSv1.2',\n",
    "      driver='com.mysql.jdbc.Driver',\n",
    "      dbtable='metadatas',\n",
    "      user='root',\n",
    "      password='root').mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mysqlのコマンドです。\n",
    "\n",
    "Mysqlに接続後、以下のコマンドを順番に発行してみてください。\n",
    "\n",
    "```\n",
    "use metadata;\n",
    "select * from metadatas;\n",
    "```\n",
    "\n",
    "これらの値を、APIであったりGUIで表示することによってテーブル定義の管理であったり、データ基盤以外のシステムが稼働状況を把握できるようになってくるというわけです。\n",
    "\n",
    "## サマリーのカラムについて\n",
    "現状はサマリーのカラムは仮の文字を入れていました。  \n",
    "この項目は、ビジネスユーザに更新してもらうことを想定しているカラムです(主にGUIなどから)  \n",
    "テーブルの構造はわかっても、カラムの更新値がどのようなタイミングでその値に更新されるのか？は知っているようで知らない情報だったりします  \n",
    "そのような暗黙知を形式知に変えていくためのカラムがサマリーカラムの役割です（実際はもっと複雑ですがまずはビジネスメタデータの役割などを理解してもらえると嬉しいです）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
